{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from icecream import ic\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from itertools import product\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "sales_train = pd.read_csv('./data/sales_train.csv')\n",
    "test = pd.read_csv('./data/test.csv')   # (214200, 3)\n",
    "\n",
    "# 计算每个商品每个月的销售量，假如某个商品在某个月没有数据，则填充0（即这个月的销售量为0）\n",
    "sales_by_item_id = sales_train.pivot_table(index=['item_id'], values=['item_cnt_day'], columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\n",
    "sales_by_item_id.columns = sales_by_item_id.columns.droplevel().map(str)   # 去掉第一层索引\n",
    "sales_by_item_id.columns.values[0] = 'item_id'\n",
    "sales_by_item_id = sales_by_item_id.rename_axis(None, axis=1)\n",
    "\n",
    "# 获取最近6个月销售量为0的数据\n",
    "# six_zero = sales_by_item_id[(sales_by_item_id['28'] == 0) & (sales_by_item_id['29'] == 0) & (sales_by_item_id['30'] == 0) & (sales_by_item_id['31'] == 0) & (sales_by_item_id['32'] == 0) & (sales_by_item_id['33'] == 0)]\n",
    "# six_zero_item_id = list(six_zero['item_id'].values)   # item_id列表\n",
    "# test.loc[test.item_id.isin(six_zero_item_id), 'item_cnt_month'] = 0  # 将test数据中（最近六个月销量为0）的数据月销量设为0，有7812个\n",
    "\n",
    "# 计算每个商店每个月的销量\n",
    "sales_by_shop_id = sales_train.pivot_table(index=['shop_id'], values=['item_cnt_day'], aggfunc=np.sum, fill_value=0, columns='date_block_num').reset_index()\n",
    "sales_by_shop_id.columns = sales_by_shop_id.columns.droplevel().map(str)    # 将两层column转化为一层column,保留下层column\n",
    "sales_by_shop_id.columns.values[0] = 'shop_id'\n",
    "sales_by_shop_id = sales_by_shop_id.rename_axis(None, axis=1)   # 将列方向的轴重命名为none\n",
    "\n",
    "# zero = sales_train[sales_train.date_block_num==0]\n",
    "# ic(zero.shop_id.unique(), len(zero.item_id.unique()), len(zero.shop_id.unique()), len(zero.shop_id.unique()) * len(zero.item_id.unique()))\n",
    "# ic(sales_train.shop_id.unique(), len(sales_train.item_id.unique()), len(sales_train.shop_id.unique()), len(sales_train.shop_id.unique()) * len(sales_train.item_id.unique()))\n",
    "\n",
    "\"\"\"组合date_block_num,shop_id,item_id(部分) 总量：10913850\"\"\"\n",
    "matrix = []\n",
    "cols = ['date_block_num','shop_id','item_id']\n",
    "for i in range(34):\n",
    "    sales = sales_train[sales_train.date_block_num==i]\n",
    "    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n",
    "matrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n",
    "matrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\n",
    "matrix['shop_id'] = matrix['shop_id'].astype(np.int8)\n",
    "matrix.sort_values(cols, inplace=True)  # 排序\n",
    "sales_train['revenue'] = sales_train['item_price'] * sales_train['item_cnt_day']    # 某一天的销售额\n",
    "\n",
    "# 分组\n",
    "groupby = sales_train.groupby(['shop_id','item_id','date_block_num']).agg({'item_cnt_day': 'sum'}).reset_index()\n",
    "groupby = groupby.rename(columns={'item_cnt_day': 'item_cnt_month'})\n",
    "matrix = matrix.merge(groupby, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "matrix['item_cnt_month'] = matrix['item_cnt_month'].fillna(0).clip(0, 20)\n",
    "matrix['item_cnt_month'] = matrix['item_cnt_month'].astype(np.float16)\n",
    "\n",
    "# test数据\n",
    "test['date_block_num'] = 34\n",
    "test['date_block_num'] = test['date_block_num'].astype(np.int8)\n",
    "test['shop_id'] = test['shop_id'].astype(np.int8)\n",
    "test['item_id'] = test['item_id'].astype(np.int16)\n",
    "\n",
    "# 合并matrix,test\n",
    "matrix = pd.concat([matrix, test[cols]], ignore_index=True, axis=0)\n",
    "matrix['item_cnt_month'].fillna(0, inplace=True)\n",
    "\n",
    "# 商品信息\n",
    "items = pd.read_csv('./data/items.csv')\n",
    "items = items[['item_id', 'item_category_id']]\n",
    "matrix = pd.merge(left=matrix, right=items, on='item_id', how='left')  # 合并\n",
    "\n",
    "# 商品类别\n",
    "le = LabelEncoder()\n",
    "categories = pd.read_csv('./data/item_categories.csv')\n",
    "categories['split'] = categories['item_category_name'].str.split('-')\n",
    "categories['type'] = categories['split'].map(lambda x:x[0].strip())\n",
    "categories['subtype'] = categories['split'].map(lambda x:x[1].strip() if len(x)>1 else x[0].strip())\n",
    "categories = categories[['item_category_id','type','subtype']]\n",
    "categories['cat_type_code'] = le.fit_transform(categories['type'])\n",
    "categories['cat_subtype_code'] = le.fit_transform(categories['subtype'])\n",
    "matrix = pd.merge(left=matrix, right=categories[['item_category_id','cat_type_code','cat_subtype_code']], on='item_category_id', how='left')    # 合并\n",
    "\n",
    "# 商店信息\n",
    "shops = pd.read_csv('./data/shops.csv')\n",
    "shops['split']=shops.shop_name.str.split(' ')\n",
    "shops['shop_city'] = shops['split'].map(lambda x:x[0])\n",
    "shops['shop_city_code'] = le.fit_transform(shops['shop_city'])\n",
    "\n",
    "def st(name):\n",
    "    if 'ТЦ' in name or 'ТРЦ' in name:\n",
    "        shopt = 'ТЦ'\n",
    "    elif 'ТК' in name:\n",
    "        shopt = 'ТК'\n",
    "    elif 'ТРК' in name:\n",
    "        shopt = 'ТРК'\n",
    "    elif 'МТРЦ' in name:\n",
    "        shopt = 'МТРЦ'\n",
    "    else:\n",
    "        shopt = 'UNKNOWN'\n",
    "    return shopt\n",
    "shops['shop_type'] = shops['shop_name'].apply(st)\n",
    "\n",
    "shops.loc[shops.shop_id == 21, 'shop_type'] = 'МТРЦ'   # 修正\n",
    "shops['shop_type_code'] = le.fit_transform(shops['shop_type'])\n",
    "matrix = pd.merge(left=matrix, right=shops[['shop_id','shop_city_code','shop_type_code']], on='shop_id', how='left')    # 合并\n",
    "matrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\n",
    "matrix['cat_type_code'] = matrix['cat_type_code'].astype(np.int8)\n",
    "matrix['cat_subtype_code'] = matrix['cat_subtype_code'].astype(np.int8)\n",
    "matrix['shop_city_code'] = matrix['shop_city_code'].astype(np.int8)\n",
    "matrix['shop_type_code'] = matrix['shop_type_code'].astype(np.int8)\n",
    "\n",
    "\n",
    "\"\"\"历史信息\"\"\"\n",
    "\n",
    "def lag_features(df, lags, col):\n",
    "    tmp = df[['date_block_num','shop_id','item_id',col]]\n",
    "    for i in lags:\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id',col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] = shifted['date_block_num'] + i\n",
    "        df = pd.merge(left=df, right=shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    return df\n",
    "\n",
    "matrix = lag_features(matrix, [1,2,3,6,12], 'item_cnt_month')\n",
    "\n",
    "# 月销量（所有商品）\n",
    "group = matrix.groupby('date_block_num').agg({'item_cnt_month': 'mean'}).reset_index()\n",
    "group.columns = ['date_block_num', 'date_avg_item_cnt']\n",
    "matrix = pd.merge(left=matrix, right=group, on='date_block_num', how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,6,12], 'date_avg_item_cnt')\n",
    "matrix.drop('date_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（每一件商品）\n",
    "group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_item_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num','item_id'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\n",
    "matrix.drop('date_item_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（每个商店 ）\n",
    "group = matrix.groupby(['date_block_num','shop_id']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['date_shop_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num','shop_id'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\n",
    "matrix.drop('date_shop_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（每个类别）\n",
    "group = matrix.groupby(['date_block_num','item_category_id']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['date_cat_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix=pd.merge(left=matrix, right=group, on=['date_block_num','item_category_id'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,6,12], 'date_cat_avg_item_cnt')\n",
    "matrix.drop('date_cat_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（商品类别-商店）\n",
    "group = matrix.groupby(['date_block_num','item_category_id','shop_id']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['date_cat_shop_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num','item_category_id','shop_id'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,6,12], 'date_cat_shop_avg_item_cnt')\n",
    "matrix.drop('date_cat_shop_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（商品大类）\n",
    "group = matrix.groupby(['date_block_num','cat_type_code']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['date_type_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num','cat_type_code'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,6,12], 'date_type_avg_item_cnt')\n",
    "matrix.drop('date_type_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（商品-商品大类） ++++++++++++ 和 月销量（商品）是重复的，因为每一个商品，类别是确定的，大类也是确定的\n",
    "group = matrix.groupby(['date_block_num', 'item_id', 'cat_type_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_item_type_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num', 'item_id', 'cat_type_code'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,6,12], 'date_item_type_avg_item_cnt')\n",
    "matrix.drop('date_item_type_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（商店城市）\n",
    "group = matrix.groupby(['date_block_num','shop_city_code']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['date_city_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num','shop_city_code'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,6,12], 'date_city_avg_item_cnt')\n",
    "matrix.drop('date_city_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（商品-商店城市）\n",
    "group = matrix.groupby(['date_block_num', 'item_id', 'shop_city_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_item_city_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix=pd.merge(left=matrix, right=group, on=['date_block_num', 'item_id', 'shop_city_code'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,6,12], 'date_item_city_avg_item_cnt')\n",
    "matrix.drop('date_item_city_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 趋势特征\n",
    "group = sales_train.groupby('item_id').agg({'item_price': 'mean'})\n",
    "group.columns = ['item_avg_item_price']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on='item_id', how='left')\n",
    "\n",
    "group = sales_train.groupby(['date_block_num','item_id']).agg({'item_price': 'mean'})\n",
    "group.columns = ['date_item_avg_item_price']\n",
    "group = group.reset_index()\n",
    "matrix=pd.merge(left=matrix, right=group, on=['date_block_num','item_id'], how='left')\n",
    "\n",
    "matrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n",
    "matrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "# 计算matrix中商品的历史价格\n",
    "lags = [1,2,3,4,5,6,12]\n",
    "matrix = lag_features(matrix, lags, 'date_item_avg_item_price')\n",
    "for i in lags:\n",
    "    matrix['delta_price_lag_'+str(i)] = (matrix['date_item_avg_item_price_lag_' + str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n",
    "\n",
    "def select_trend(row):\n",
    "    for i in lags:\n",
    "        if pd.notnull(row['delta_price_lag_'+str(i)]):  # 如果不是NaN\n",
    "            return row['delta_price_lag_'+str(i)]\n",
    "    return 0   #  如果delta_price_lag_都为空，那么将趋势设为0，0代表没有趋势\n",
    "\n",
    "matrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\n",
    "matrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\n",
    "\n",
    "features_to_drop = ['item_avg_item_price','date_item_avg_item_price']\n",
    "for i in lags:\n",
    "    features_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n",
    "    features_to_drop += ['delta_price_lag_'+str(i)]\n",
    "matrix.drop(features_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# 每个月的天数\n",
    "matrix['month'] = matrix['date_block_num'] % 12\n",
    "days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n",
    "matrix['days'] = matrix['month'].map(days)\n",
    "matrix['days'] = matrix['days'].astype(np.int8)\n",
    "\n",
    "# 开始销量\n",
    "matrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n",
    "matrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\n",
    "\n",
    "# 因为有12个月的延迟特征（1，2，3，6，12）（1，2，3，4，5，6，12），所以需要删除前12月的数据\n",
    "matrix = matrix[matrix['date_block_num'] > 11]\n",
    "\n",
    "# 找到有NaN值的列，然后把那些列中的NaN值填充0\n",
    "columns = matrix.columns\n",
    "column_null = []\n",
    "for i in columns:\n",
    "    if len(matrix[matrix[i].isnull()]) > 0:\n",
    "        column_null.append(i)\n",
    "\n",
    "for i in column_null:\n",
    "    matrix[i].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = matrix.copy()\n",
    "matrix2 = matrix.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析数值型数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import boxcox1p, boxcox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\94690\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\fromnumeric.py:90: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "columns = []\n",
    "for col in matrix.columns:\n",
    "    if col != 'item_cnt_month':\n",
    "        if stats.skew(matrix[col]) > 0.75:\n",
    "            columns.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_subtype_code >>  0.8529682866274095\n",
      "item_cnt_month_lag_1 >>  9.40625\n",
      "item_cnt_month_lag_2 >>  9.375\n",
      "item_cnt_month_lag_3 >>  9.3828125\n",
      "item_cnt_month_lag_6 >>  9.4921875\n",
      "item_cnt_month_lag_12 >>  10.203125\n",
      "date_item_avg_item_cnt_lag_1 >>  9.8125\n",
      "date_item_avg_item_cnt_lag_2 >>  9.8125\n",
      "date_item_avg_item_cnt_lag_3 >>  9.7890625\n",
      "date_item_avg_item_cnt_lag_6 >>  9.7890625\n",
      "date_item_avg_item_cnt_lag_12 >>  10.1875\n",
      "date_shop_avg_item_cnt_lag_1 >>  2.05859375\n",
      "date_shop_avg_item_cnt_lag_2 >>  2.044921875\n",
      "date_shop_avg_item_cnt_lag_3 >>  2.046875\n",
      "date_shop_avg_item_cnt_lag_6 >>  2.048828125\n",
      "date_shop_avg_item_cnt_lag_12 >>  2.201171875\n",
      "date_cat_avg_item_cnt_lag_1 >>  18.171875\n",
      "date_cat_avg_item_cnt_lag_2 >>  18.296875\n",
      "date_cat_avg_item_cnt_lag_3 >>  18.390625\n",
      "date_cat_avg_item_cnt_lag_6 >>  18.734375\n",
      "date_cat_avg_item_cnt_lag_12 >>  20.1875\n",
      "date_cat_shop_avg_item_cnt_lag_1 >>  14.3046875\n",
      "date_cat_shop_avg_item_cnt_lag_2 >>  14.4140625\n",
      "date_cat_shop_avg_item_cnt_lag_3 >>  14.640625\n",
      "date_cat_shop_avg_item_cnt_lag_6 >>  15.140625\n",
      "date_cat_shop_avg_item_cnt_lag_12 >>  16.65625\n",
      "date_type_avg_item_cnt_lag_1 >>  14.5625\n",
      "date_type_avg_item_cnt_lag_2 >>  13.6328125\n",
      "date_type_avg_item_cnt_lag_3 >>  13.2734375\n",
      "date_type_avg_item_cnt_lag_6 >>  12.3984375\n",
      "date_type_avg_item_cnt_lag_12 >>  7.9296875\n",
      "date_item_type_avg_item_cnt_lag_1 >>  9.8125\n",
      "date_item_type_avg_item_cnt_lag_2 >>  9.8125\n",
      "date_item_type_avg_item_cnt_lag_3 >>  9.7890625\n",
      "date_item_type_avg_item_cnt_lag_6 >>  9.7890625\n",
      "date_item_type_avg_item_cnt_lag_12 >>  10.1875\n",
      "date_city_avg_item_cnt_lag_1 >>  0.7900390625\n",
      "date_city_avg_item_cnt_lag_2 >>  0.78955078125\n",
      "date_city_avg_item_cnt_lag_3 >>  0.802734375\n",
      "date_city_avg_item_cnt_lag_6 >>  0.85595703125\n",
      "date_city_avg_item_cnt_lag_12 >>  1.205078125\n",
      "date_item_city_avg_item_cnt_lag_1 >>  9.578125\n",
      "date_item_city_avg_item_cnt_lag_2 >>  9.546875\n",
      "date_item_city_avg_item_cnt_lag_3 >>  9.5625\n",
      "date_item_city_avg_item_cnt_lag_6 >>  9.6640625\n",
      "date_item_city_avg_item_cnt_lag_12 >>  10.359375\n"
     ]
    }
   ],
   "source": [
    "for i in columns:\n",
    "    print(i, '>> ', stats.skew(matrix[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in columns:\n",
    "    matrix[i]=boxcox1p(matrix[i], 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_subtype_code >>  0.21308243656317116\n",
      "item_cnt_month_lag_1 >>  3.831051826477051\n",
      "item_cnt_month_lag_2 >>  3.8560307025909424\n",
      "item_cnt_month_lag_3 >>  3.8862102031707764\n",
      "item_cnt_month_lag_6 >>  4.013579368591309\n",
      "item_cnt_month_lag_12 >>  4.534460067749023\n",
      "date_item_avg_item_cnt_lag_1 >>  3.9658572673797607\n",
      "date_item_avg_item_cnt_lag_2 >>  3.969264030456543\n",
      "date_item_avg_item_cnt_lag_3 >>  3.9657185077667236\n",
      "date_item_avg_item_cnt_lag_6 >>  4.00478458404541\n",
      "date_item_avg_item_cnt_lag_12 >>  4.375425338745117\n",
      "date_shop_avg_item_cnt_lag_1 >>  1.4135289192199707\n",
      "date_shop_avg_item_cnt_lag_2 >>  1.397884726524353\n",
      "date_shop_avg_item_cnt_lag_3 >>  1.4012951850891113\n",
      "date_shop_avg_item_cnt_lag_6 >>  1.4246762990951538\n",
      "date_shop_avg_item_cnt_lag_12 >>  1.6625432968139648\n",
      "date_cat_avg_item_cnt_lag_1 >>  3.311107873916626\n",
      "date_cat_avg_item_cnt_lag_2 >>  3.30843448638916\n",
      "date_cat_avg_item_cnt_lag_3 >>  3.2901079654693604\n",
      "date_cat_avg_item_cnt_lag_6 >>  3.340208053588867\n",
      "date_cat_avg_item_cnt_lag_12 >>  3.672301769256592\n",
      "date_cat_shop_avg_item_cnt_lag_1 >>  3.166423797607422\n",
      "date_cat_shop_avg_item_cnt_lag_2 >>  3.207017421722412\n",
      "date_cat_shop_avg_item_cnt_lag_3 >>  3.2477810382843018\n",
      "date_cat_shop_avg_item_cnt_lag_6 >>  3.424182415008545\n",
      "date_cat_shop_avg_item_cnt_lag_12 >>  3.948664903640747\n",
      "date_type_avg_item_cnt_lag_1 >>  1.8743860721588135\n",
      "date_type_avg_item_cnt_lag_2 >>  1.7464473247528076\n",
      "date_type_avg_item_cnt_lag_3 >>  1.723729133605957\n",
      "date_type_avg_item_cnt_lag_6 >>  1.7935436964035034\n",
      "date_type_avg_item_cnt_lag_12 >>  2.1071484088897705\n",
      "date_item_type_avg_item_cnt_lag_1 >>  3.9658572673797607\n",
      "date_item_type_avg_item_cnt_lag_2 >>  3.969264030456543\n",
      "date_item_type_avg_item_cnt_lag_3 >>  3.9657185077667236\n",
      "date_item_type_avg_item_cnt_lag_6 >>  4.00478458404541\n",
      "date_item_type_avg_item_cnt_lag_12 >>  4.375425338745117\n",
      "date_city_avg_item_cnt_lag_1 >>  0.483308881521225\n",
      "date_city_avg_item_cnt_lag_2 >>  0.48944491147994995\n",
      "date_city_avg_item_cnt_lag_3 >>  0.512820303440094\n",
      "date_city_avg_item_cnt_lag_6 >>  0.6042562127113342\n",
      "date_city_avg_item_cnt_lag_12 >>  0.9672856330871582\n",
      "date_item_city_avg_item_cnt_lag_1 >>  3.7590837478637695\n",
      "date_item_city_avg_item_cnt_lag_2 >>  3.7826695442199707\n",
      "date_item_city_avg_item_cnt_lag_3 >>  3.8109257221221924\n",
      "date_item_city_avg_item_cnt_lag_6 >>  3.933577299118042\n",
      "date_item_city_avg_item_cnt_lag_12 >>  4.433375358581543\n"
     ]
    }
   ],
   "source": [
    "for i in columns:\n",
    "    print(i, '>> ', stats.skew(matrix[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python37\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: early_stop_rounds\n",
      "[LightGBM] [Warning] Unknown parameter: early_stop_rounds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10825\n",
      "[LightGBM] [Info] Number of data points in the train set: 6425094, number of used features: 63\n",
      "[LightGBM] [Warning] Unknown parameter: early_stop_rounds\n",
      "[LightGBM] [Info] Start training from score 0.287729\n",
      "[1]\ttraining's rmse: 1.18109\tvalid_1's rmse: 0.287752\n",
      "[2]\ttraining's rmse: 1.17604\tvalid_1's rmse: 0.287929\n",
      "[3]\ttraining's rmse: 1.17215\tvalid_1's rmse: 0.288316\n",
      "[4]\ttraining's rmse: 1.16849\tvalid_1's rmse: 0.288682\n",
      "[5]\ttraining's rmse: 1.16357\tvalid_1's rmse: 0.289449\n",
      "[6]\ttraining's rmse: 1.16007\tvalid_1's rmse: 0.289938\n",
      "[7]\ttraining's rmse: 1.15593\tvalid_1's rmse: 0.290899\n",
      "[8]\ttraining's rmse: 1.15133\tvalid_1's rmse: 0.291681\n",
      "[9]\ttraining's rmse: 1.14711\tvalid_1's rmse: 0.29253\n",
      "[10]\ttraining's rmse: 1.14264\tvalid_1's rmse: 0.293474\n",
      "[11]\ttraining's rmse: 1.13806\tvalid_1's rmse: 0.295022\n",
      "[12]\ttraining's rmse: 1.13383\tvalid_1's rmse: 0.296261\n",
      "[13]\ttraining's rmse: 1.12973\tvalid_1's rmse: 0.297669\n",
      "[14]\ttraining's rmse: 1.12604\tvalid_1's rmse: 0.298838\n",
      "[15]\ttraining's rmse: 1.12238\tvalid_1's rmse: 0.300276\n",
      "[16]\ttraining's rmse: 1.11848\tvalid_1's rmse: 0.301898\n",
      "[17]\ttraining's rmse: 1.11484\tvalid_1's rmse: 0.303432\n",
      "[18]\ttraining's rmse: 1.11109\tvalid_1's rmse: 0.305145\n",
      "[19]\ttraining's rmse: 1.10717\tvalid_1's rmse: 0.307226\n",
      "[20]\ttraining's rmse: 1.10376\tvalid_1's rmse: 0.30923\n",
      "[21]\ttraining's rmse: 1.10013\tvalid_1's rmse: 0.311091\n",
      "[22]\ttraining's rmse: 1.09656\tvalid_1's rmse: 0.31316\n",
      "[23]\ttraining's rmse: 1.09324\tvalid_1's rmse: 0.315202\n",
      "[24]\ttraining's rmse: 1.08955\tvalid_1's rmse: 0.317545\n",
      "[25]\ttraining's rmse: 1.08586\tvalid_1's rmse: 0.319749\n",
      "[26]\ttraining's rmse: 1.08231\tvalid_1's rmse: 0.321811\n",
      "[27]\ttraining's rmse: 1.07932\tvalid_1's rmse: 0.323897\n",
      "[28]\ttraining's rmse: 1.07564\tvalid_1's rmse: 0.326813\n",
      "[29]\ttraining's rmse: 1.07249\tvalid_1's rmse: 0.329122\n",
      "[30]\ttraining's rmse: 1.06922\tvalid_1's rmse: 0.331319\n",
      "[31]\ttraining's rmse: 1.06578\tvalid_1's rmse: 0.333957\n",
      "[32]\ttraining's rmse: 1.06289\tvalid_1's rmse: 0.336167\n",
      "[33]\ttraining's rmse: 1.06069\tvalid_1's rmse: 0.337912\n",
      "[34]\ttraining's rmse: 1.0574\tvalid_1's rmse: 0.340746\n",
      "[35]\ttraining's rmse: 1.05455\tvalid_1's rmse: 0.342776\n",
      "[36]\ttraining's rmse: 1.05209\tvalid_1's rmse: 0.345972\n",
      "[37]\ttraining's rmse: 1.04932\tvalid_1's rmse: 0.348589\n",
      "[38]\ttraining's rmse: 1.04717\tvalid_1's rmse: 0.350338\n",
      "[39]\ttraining's rmse: 1.04413\tvalid_1's rmse: 0.352385\n",
      "[40]\ttraining's rmse: 1.04127\tvalid_1's rmse: 0.355256\n",
      "[41]\ttraining's rmse: 1.03822\tvalid_1's rmse: 0.357917\n",
      "[42]\ttraining's rmse: 1.03553\tvalid_1's rmse: 0.360092\n",
      "[43]\ttraining's rmse: 1.03289\tvalid_1's rmse: 0.362454\n",
      "[44]\ttraining's rmse: 1.0304\tvalid_1's rmse: 0.364861\n",
      "[45]\ttraining's rmse: 1.0284\tvalid_1's rmse: 0.36676\n",
      "[46]\ttraining's rmse: 1.02547\tvalid_1's rmse: 0.369277\n",
      "[47]\ttraining's rmse: 1.02276\tvalid_1's rmse: 0.37158\n",
      "[48]\ttraining's rmse: 1.01973\tvalid_1's rmse: 0.374682\n",
      "[49]\ttraining's rmse: 1.0172\tvalid_1's rmse: 0.377249\n",
      "[50]\ttraining's rmse: 1.01433\tvalid_1's rmse: 0.380356\n",
      "[51]\ttraining's rmse: 1.01176\tvalid_1's rmse: 0.382934\n",
      "[52]\ttraining's rmse: 1.00924\tvalid_1's rmse: 0.386415\n",
      "[53]\ttraining's rmse: 1.00677\tvalid_1's rmse: 0.388938\n",
      "[54]\ttraining's rmse: 1.00412\tvalid_1's rmse: 0.391681\n",
      "[55]\ttraining's rmse: 1.00174\tvalid_1's rmse: 0.394628\n",
      "[56]\ttraining's rmse: 0.999671\tvalid_1's rmse: 0.397163\n",
      "[57]\ttraining's rmse: 0.997227\tvalid_1's rmse: 0.400046\n",
      "[58]\ttraining's rmse: 0.994822\tvalid_1's rmse: 0.403025\n",
      "[59]\ttraining's rmse: 0.99229\tvalid_1's rmse: 0.406184\n",
      "[60]\ttraining's rmse: 0.989456\tvalid_1's rmse: 0.40953\n",
      "[61]\ttraining's rmse: 0.987477\tvalid_1's rmse: 0.412167\n",
      "[62]\ttraining's rmse: 0.985471\tvalid_1's rmse: 0.414392\n",
      "[63]\ttraining's rmse: 0.983242\tvalid_1's rmse: 0.417072\n",
      "[64]\ttraining's rmse: 0.981541\tvalid_1's rmse: 0.419253\n",
      "[65]\ttraining's rmse: 0.979734\tvalid_1's rmse: 0.421479\n",
      "[66]\ttraining's rmse: 0.977787\tvalid_1's rmse: 0.423907\n",
      "[67]\ttraining's rmse: 0.975755\tvalid_1's rmse: 0.426519\n",
      "[68]\ttraining's rmse: 0.97434\tvalid_1's rmse: 0.428842\n",
      "[69]\ttraining's rmse: 0.971971\tvalid_1's rmse: 0.43188\n",
      "[70]\ttraining's rmse: 0.969877\tvalid_1's rmse: 0.43436\n",
      "[71]\ttraining's rmse: 0.967759\tvalid_1's rmse: 0.437406\n",
      "[72]\ttraining's rmse: 0.965553\tvalid_1's rmse: 0.440199\n",
      "[73]\ttraining's rmse: 0.963704\tvalid_1's rmse: 0.442764\n",
      "[74]\ttraining's rmse: 0.961853\tvalid_1's rmse: 0.445344\n",
      "[75]\ttraining's rmse: 0.959772\tvalid_1's rmse: 0.448517\n",
      "[76]\ttraining's rmse: 0.957988\tvalid_1's rmse: 0.45099\n",
      "[77]\ttraining's rmse: 0.955949\tvalid_1's rmse: 0.454308\n",
      "[78]\ttraining's rmse: 0.954068\tvalid_1's rmse: 0.456874\n",
      "[79]\ttraining's rmse: 0.952421\tvalid_1's rmse: 0.459232\n",
      "[80]\ttraining's rmse: 0.951146\tvalid_1's rmse: 0.461439\n",
      "[81]\ttraining's rmse: 0.949409\tvalid_1's rmse: 0.464046\n",
      "[82]\ttraining's rmse: 0.947843\tvalid_1's rmse: 0.466388\n",
      "[83]\ttraining's rmse: 0.946158\tvalid_1's rmse: 0.469076\n",
      "[84]\ttraining's rmse: 0.944722\tvalid_1's rmse: 0.471626\n",
      "[85]\ttraining's rmse: 0.943085\tvalid_1's rmse: 0.474021\n",
      "[86]\ttraining's rmse: 0.941399\tvalid_1's rmse: 0.476434\n",
      "[87]\ttraining's rmse: 0.940309\tvalid_1's rmse: 0.478015\n",
      "[88]\ttraining's rmse: 0.93849\tvalid_1's rmse: 0.480945\n",
      "[89]\ttraining's rmse: 0.936816\tvalid_1's rmse: 0.484122\n",
      "[90]\ttraining's rmse: 0.935299\tvalid_1's rmse: 0.486173\n",
      "[91]\ttraining's rmse: 0.933708\tvalid_1's rmse: 0.488592\n",
      "[92]\ttraining's rmse: 0.932173\tvalid_1's rmse: 0.49085\n",
      "[93]\ttraining's rmse: 0.930704\tvalid_1's rmse: 0.49333\n",
      "[94]\ttraining's rmse: 0.929453\tvalid_1's rmse: 0.495141\n",
      "[95]\ttraining's rmse: 0.927859\tvalid_1's rmse: 0.49749\n",
      "[96]\ttraining's rmse: 0.926298\tvalid_1's rmse: 0.500298\n",
      "[97]\ttraining's rmse: 0.925099\tvalid_1's rmse: 0.502146\n",
      "[98]\ttraining's rmse: 0.923478\tvalid_1's rmse: 0.504703\n",
      "[99]\ttraining's rmse: 0.922575\tvalid_1's rmse: 0.506718\n",
      "[100]\ttraining's rmse: 0.92126\tvalid_1's rmse: 0.508985\n",
      "[101]\ttraining's rmse: 0.919972\tvalid_1's rmse: 0.511206\n",
      "[102]\ttraining's rmse: 0.918543\tvalid_1's rmse: 0.513598\n",
      "[103]\ttraining's rmse: 0.917354\tvalid_1's rmse: 0.51579\n",
      "[104]\ttraining's rmse: 0.915739\tvalid_1's rmse: 0.518473\n",
      "[105]\ttraining's rmse: 0.914099\tvalid_1's rmse: 0.521397\n",
      "[106]\ttraining's rmse: 0.912572\tvalid_1's rmse: 0.523965\n",
      "[107]\ttraining's rmse: 0.911476\tvalid_1's rmse: 0.525825\n",
      "[108]\ttraining's rmse: 0.910012\tvalid_1's rmse: 0.527778\n",
      "[109]\ttraining's rmse: 0.909162\tvalid_1's rmse: 0.529742\n",
      "[110]\ttraining's rmse: 0.907925\tvalid_1's rmse: 0.532358\n",
      "[111]\ttraining's rmse: 0.906809\tvalid_1's rmse: 0.534438\n",
      "[112]\ttraining's rmse: 0.905529\tvalid_1's rmse: 0.536721\n",
      "[113]\ttraining's rmse: 0.904414\tvalid_1's rmse: 0.53866\n",
      "[114]\ttraining's rmse: 0.903542\tvalid_1's rmse: 0.540601\n",
      "[115]\ttraining's rmse: 0.902338\tvalid_1's rmse: 0.542703\n",
      "[116]\ttraining's rmse: 0.901124\tvalid_1's rmse: 0.544837\n",
      "[117]\ttraining's rmse: 0.900132\tvalid_1's rmse: 0.546753\n",
      "[118]\ttraining's rmse: 0.898618\tvalid_1's rmse: 0.549559\n",
      "[119]\ttraining's rmse: 0.897732\tvalid_1's rmse: 0.551924\n",
      "[120]\ttraining's rmse: 0.896771\tvalid_1's rmse: 0.553519\n",
      "[121]\ttraining's rmse: 0.895854\tvalid_1's rmse: 0.555367\n",
      "[122]\ttraining's rmse: 0.894741\tvalid_1's rmse: 0.55778\n",
      "[123]\ttraining's rmse: 0.893748\tvalid_1's rmse: 0.559703\n",
      "[124]\ttraining's rmse: 0.892781\tvalid_1's rmse: 0.561649\n",
      "[125]\ttraining's rmse: 0.89184\tvalid_1's rmse: 0.563713\n",
      "[126]\ttraining's rmse: 0.890769\tvalid_1's rmse: 0.565624\n",
      "[127]\ttraining's rmse: 0.88971\tvalid_1's rmse: 0.567922\n",
      "[128]\ttraining's rmse: 0.888525\tvalid_1's rmse: 0.569853\n",
      "[129]\ttraining's rmse: 0.887679\tvalid_1's rmse: 0.571878\n",
      "[130]\ttraining's rmse: 0.886766\tvalid_1's rmse: 0.57328\n",
      "[131]\ttraining's rmse: 0.885633\tvalid_1's rmse: 0.575299\n",
      "[132]\ttraining's rmse: 0.884711\tvalid_1's rmse: 0.577033\n",
      "[133]\ttraining's rmse: 0.883828\tvalid_1's rmse: 0.578816\n",
      "[134]\ttraining's rmse: 0.88287\tvalid_1's rmse: 0.580877\n",
      "[135]\ttraining's rmse: 0.881997\tvalid_1's rmse: 0.58275\n",
      "[136]\ttraining's rmse: 0.881121\tvalid_1's rmse: 0.584695\n",
      "[137]\ttraining's rmse: 0.880273\tvalid_1's rmse: 0.586441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138]\ttraining's rmse: 0.879367\tvalid_1's rmse: 0.588629\n",
      "[139]\ttraining's rmse: 0.878578\tvalid_1's rmse: 0.590415\n",
      "[140]\ttraining's rmse: 0.87784\tvalid_1's rmse: 0.591483\n",
      "[141]\ttraining's rmse: 0.877038\tvalid_1's rmse: 0.592887\n",
      "[142]\ttraining's rmse: 0.87638\tvalid_1's rmse: 0.594614\n",
      "[143]\ttraining's rmse: 0.875345\tvalid_1's rmse: 0.597311\n",
      "[144]\ttraining's rmse: 0.874607\tvalid_1's rmse: 0.59891\n",
      "[145]\ttraining's rmse: 0.873767\tvalid_1's rmse: 0.600967\n",
      "[146]\ttraining's rmse: 0.873019\tvalid_1's rmse: 0.602578\n",
      "[147]\ttraining's rmse: 0.872341\tvalid_1's rmse: 0.604063\n",
      "[148]\ttraining's rmse: 0.871711\tvalid_1's rmse: 0.605635\n",
      "[149]\ttraining's rmse: 0.870962\tvalid_1's rmse: 0.60753\n",
      "[150]\ttraining's rmse: 0.870217\tvalid_1's rmse: 0.60936\n",
      "[151]\ttraining's rmse: 0.869672\tvalid_1's rmse: 0.610578\n",
      "[152]\ttraining's rmse: 0.868949\tvalid_1's rmse: 0.612296\n",
      "[153]\ttraining's rmse: 0.868309\tvalid_1's rmse: 0.61387\n",
      "[154]\ttraining's rmse: 0.867909\tvalid_1's rmse: 0.614759\n",
      "[155]\ttraining's rmse: 0.867169\tvalid_1's rmse: 0.61621\n",
      "[156]\ttraining's rmse: 0.866531\tvalid_1's rmse: 0.617876\n",
      "[157]\ttraining's rmse: 0.866061\tvalid_1's rmse: 0.619119\n",
      "[158]\ttraining's rmse: 0.865413\tvalid_1's rmse: 0.620343\n",
      "[159]\ttraining's rmse: 0.864793\tvalid_1's rmse: 0.621684\n",
      "[160]\ttraining's rmse: 0.864053\tvalid_1's rmse: 0.623589\n",
      "[161]\ttraining's rmse: 0.863432\tvalid_1's rmse: 0.625071\n",
      "[162]\ttraining's rmse: 0.862494\tvalid_1's rmse: 0.626636\n",
      "[163]\ttraining's rmse: 0.86171\tvalid_1's rmse: 0.628492\n",
      "[164]\ttraining's rmse: 0.861046\tvalid_1's rmse: 0.630297\n",
      "[165]\ttraining's rmse: 0.86033\tvalid_1's rmse: 0.631764\n",
      "[166]\ttraining's rmse: 0.859684\tvalid_1's rmse: 0.63333\n",
      "[167]\ttraining's rmse: 0.859099\tvalid_1's rmse: 0.634931\n",
      "[168]\ttraining's rmse: 0.858489\tvalid_1's rmse: 0.636658\n",
      "[169]\ttraining's rmse: 0.857888\tvalid_1's rmse: 0.63857\n",
      "[170]\ttraining's rmse: 0.857342\tvalid_1's rmse: 0.639873\n",
      "[171]\ttraining's rmse: 0.85666\tvalid_1's rmse: 0.641544\n",
      "[172]\ttraining's rmse: 0.855997\tvalid_1's rmse: 0.643187\n",
      "[173]\ttraining's rmse: 0.855664\tvalid_1's rmse: 0.644233\n",
      "[174]\ttraining's rmse: 0.855162\tvalid_1's rmse: 0.645452\n",
      "[175]\ttraining's rmse: 0.854651\tvalid_1's rmse: 0.646794\n",
      "[176]\ttraining's rmse: 0.854011\tvalid_1's rmse: 0.64886\n",
      "[177]\ttraining's rmse: 0.853468\tvalid_1's rmse: 0.649841\n",
      "[178]\ttraining's rmse: 0.853003\tvalid_1's rmse: 0.65076\n",
      "[179]\ttraining's rmse: 0.852517\tvalid_1's rmse: 0.651763\n",
      "[180]\ttraining's rmse: 0.852082\tvalid_1's rmse: 0.652985\n",
      "[181]\ttraining's rmse: 0.851429\tvalid_1's rmse: 0.654592\n",
      "[182]\ttraining's rmse: 0.851005\tvalid_1's rmse: 0.655572\n",
      "[183]\ttraining's rmse: 0.850516\tvalid_1's rmse: 0.656482\n",
      "[184]\ttraining's rmse: 0.849988\tvalid_1's rmse: 0.657917\n",
      "[185]\ttraining's rmse: 0.849549\tvalid_1's rmse: 0.659031\n",
      "[186]\ttraining's rmse: 0.84906\tvalid_1's rmse: 0.660617\n",
      "[187]\ttraining's rmse: 0.848192\tvalid_1's rmse: 0.662128\n",
      "[188]\ttraining's rmse: 0.847667\tvalid_1's rmse: 0.663479\n",
      "[189]\ttraining's rmse: 0.847093\tvalid_1's rmse: 0.664829\n",
      "[190]\ttraining's rmse: 0.846657\tvalid_1's rmse: 0.665978\n",
      "[191]\ttraining's rmse: 0.846192\tvalid_1's rmse: 0.666647\n",
      "[192]\ttraining's rmse: 0.845761\tvalid_1's rmse: 0.667578\n",
      "[193]\ttraining's rmse: 0.845292\tvalid_1's rmse: 0.669004\n",
      "[194]\ttraining's rmse: 0.844799\tvalid_1's rmse: 0.670326\n",
      "[195]\ttraining's rmse: 0.844356\tvalid_1's rmse: 0.672012\n",
      "[196]\ttraining's rmse: 0.843866\tvalid_1's rmse: 0.673617\n",
      "[197]\ttraining's rmse: 0.843368\tvalid_1's rmse: 0.67439\n",
      "[198]\ttraining's rmse: 0.842998\tvalid_1's rmse: 0.675461\n",
      "[199]\ttraining's rmse: 0.842707\tvalid_1's rmse: 0.676311\n",
      "[200]\ttraining's rmse: 0.842227\tvalid_1's rmse: 0.677956\n",
      "[201]\ttraining's rmse: 0.841704\tvalid_1's rmse: 0.679751\n",
      "[202]\ttraining's rmse: 0.841232\tvalid_1's rmse: 0.681429\n",
      "[203]\ttraining's rmse: 0.840922\tvalid_1's rmse: 0.68218\n",
      "[204]\ttraining's rmse: 0.840067\tvalid_1's rmse: 0.682925\n",
      "[205]\ttraining's rmse: 0.83961\tvalid_1's rmse: 0.68414\n",
      "[206]\ttraining's rmse: 0.83913\tvalid_1's rmse: 0.685258\n",
      "[207]\ttraining's rmse: 0.838668\tvalid_1's rmse: 0.686489\n",
      "[208]\ttraining's rmse: 0.838258\tvalid_1's rmse: 0.687721\n",
      "[209]\ttraining's rmse: 0.837776\tvalid_1's rmse: 0.68847\n",
      "[210]\ttraining's rmse: 0.83736\tvalid_1's rmse: 0.689777\n",
      "[211]\ttraining's rmse: 0.836966\tvalid_1's rmse: 0.691294\n",
      "[212]\ttraining's rmse: 0.836577\tvalid_1's rmse: 0.692238\n",
      "[213]\ttraining's rmse: 0.835916\tvalid_1's rmse: 0.693417\n",
      "[214]\ttraining's rmse: 0.83561\tvalid_1's rmse: 0.694021\n",
      "[215]\ttraining's rmse: 0.835243\tvalid_1's rmse: 0.695141\n",
      "[216]\ttraining's rmse: 0.834959\tvalid_1's rmse: 0.696047\n",
      "[217]\ttraining's rmse: 0.834519\tvalid_1's rmse: 0.696834\n",
      "[218]\ttraining's rmse: 0.834229\tvalid_1's rmse: 0.697247\n",
      "[219]\ttraining's rmse: 0.833848\tvalid_1's rmse: 0.698173\n",
      "[220]\ttraining's rmse: 0.833477\tvalid_1's rmse: 0.699118\n",
      "[221]\ttraining's rmse: 0.83318\tvalid_1's rmse: 0.700007\n",
      "[222]\ttraining's rmse: 0.832833\tvalid_1's rmse: 0.701003\n",
      "[223]\ttraining's rmse: 0.832538\tvalid_1's rmse: 0.702121\n",
      "[224]\ttraining's rmse: 0.832256\tvalid_1's rmse: 0.702819\n",
      "[225]\ttraining's rmse: 0.831923\tvalid_1's rmse: 0.704014\n",
      "[226]\ttraining's rmse: 0.831624\tvalid_1's rmse: 0.705109\n",
      "[227]\ttraining's rmse: 0.831131\tvalid_1's rmse: 0.706328\n",
      "[228]\ttraining's rmse: 0.83076\tvalid_1's rmse: 0.707464\n",
      "[229]\ttraining's rmse: 0.830538\tvalid_1's rmse: 0.707865\n",
      "[230]\ttraining's rmse: 0.830217\tvalid_1's rmse: 0.708431\n",
      "[231]\ttraining's rmse: 0.829832\tvalid_1's rmse: 0.709294\n",
      "[232]\ttraining's rmse: 0.829536\tvalid_1's rmse: 0.710072\n",
      "[233]\ttraining's rmse: 0.829172\tvalid_1's rmse: 0.711073\n",
      "[234]\ttraining's rmse: 0.828792\tvalid_1's rmse: 0.712036\n",
      "[235]\ttraining's rmse: 0.828267\tvalid_1's rmse: 0.712624\n",
      "[236]\ttraining's rmse: 0.827956\tvalid_1's rmse: 0.713994\n",
      "[237]\ttraining's rmse: 0.827643\tvalid_1's rmse: 0.714839\n",
      "[238]\ttraining's rmse: 0.827353\tvalid_1's rmse: 0.715622\n",
      "[239]\ttraining's rmse: 0.827053\tvalid_1's rmse: 0.716927\n",
      "[240]\ttraining's rmse: 0.826789\tvalid_1's rmse: 0.717436\n",
      "[241]\ttraining's rmse: 0.826283\tvalid_1's rmse: 0.718348\n",
      "[242]\ttraining's rmse: 0.825911\tvalid_1's rmse: 0.719231\n",
      "[243]\ttraining's rmse: 0.82559\tvalid_1's rmse: 0.720296\n",
      "[244]\ttraining's rmse: 0.824911\tvalid_1's rmse: 0.721247\n",
      "[245]\ttraining's rmse: 0.82444\tvalid_1's rmse: 0.721892\n",
      "[246]\ttraining's rmse: 0.824166\tvalid_1's rmse: 0.722609\n",
      "[247]\ttraining's rmse: 0.823899\tvalid_1's rmse: 0.722948\n",
      "[248]\ttraining's rmse: 0.823596\tvalid_1's rmse: 0.723537\n",
      "[249]\ttraining's rmse: 0.82334\tvalid_1's rmse: 0.724141\n",
      "[250]\ttraining's rmse: 0.82298\tvalid_1's rmse: 0.725696\n",
      "[251]\ttraining's rmse: 0.82274\tvalid_1's rmse: 0.72642\n",
      "[252]\ttraining's rmse: 0.822448\tvalid_1's rmse: 0.726898\n",
      "[253]\ttraining's rmse: 0.822218\tvalid_1's rmse: 0.727647\n",
      "[254]\ttraining's rmse: 0.82193\tvalid_1's rmse: 0.727916\n",
      "[255]\ttraining's rmse: 0.821595\tvalid_1's rmse: 0.728362\n",
      "[256]\ttraining's rmse: 0.821219\tvalid_1's rmse: 0.729241\n",
      "[257]\ttraining's rmse: 0.820957\tvalid_1's rmse: 0.729887\n",
      "[258]\ttraining's rmse: 0.820733\tvalid_1's rmse: 0.730588\n",
      "[259]\ttraining's rmse: 0.820481\tvalid_1's rmse: 0.731201\n",
      "[260]\ttraining's rmse: 0.820165\tvalid_1's rmse: 0.732017\n",
      "[261]\ttraining's rmse: 0.81997\tvalid_1's rmse: 0.732487\n",
      "[262]\ttraining's rmse: 0.819526\tvalid_1's rmse: 0.733458\n",
      "[263]\ttraining's rmse: 0.81927\tvalid_1's rmse: 0.733938\n",
      "[264]\ttraining's rmse: 0.818988\tvalid_1's rmse: 0.734661\n",
      "[265]\ttraining's rmse: 0.818745\tvalid_1's rmse: 0.735041\n",
      "[266]\ttraining's rmse: 0.81833\tvalid_1's rmse: 0.735746\n",
      "[267]\ttraining's rmse: 0.818143\tvalid_1's rmse: 0.736386\n",
      "[268]\ttraining's rmse: 0.817785\tvalid_1's rmse: 0.73683\n",
      "[269]\ttraining's rmse: 0.81746\tvalid_1's rmse: 0.737697\n",
      "[270]\ttraining's rmse: 0.817269\tvalid_1's rmse: 0.738079\n",
      "[271]\ttraining's rmse: 0.817006\tvalid_1's rmse: 0.739402\n",
      "[272]\ttraining's rmse: 0.816665\tvalid_1's rmse: 0.740216\n",
      "[273]\ttraining's rmse: 0.816294\tvalid_1's rmse: 0.740994\n",
      "[274]\ttraining's rmse: 0.816037\tvalid_1's rmse: 0.741778\n",
      "[275]\ttraining's rmse: 0.815839\tvalid_1's rmse: 0.742487\n",
      "[276]\ttraining's rmse: 0.815635\tvalid_1's rmse: 0.742951\n",
      "[277]\ttraining's rmse: 0.815379\tvalid_1's rmse: 0.74384\n",
      "[278]\ttraining's rmse: 0.815048\tvalid_1's rmse: 0.744441\n",
      "[279]\ttraining's rmse: 0.81484\tvalid_1's rmse: 0.744901\n",
      "[280]\ttraining's rmse: 0.814647\tvalid_1's rmse: 0.745653\n",
      "[281]\ttraining's rmse: 0.814454\tvalid_1's rmse: 0.746265\n",
      "[282]\ttraining's rmse: 0.814238\tvalid_1's rmse: 0.746646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[283]\ttraining's rmse: 0.814056\tvalid_1's rmse: 0.746946\n",
      "[284]\ttraining's rmse: 0.813881\tvalid_1's rmse: 0.747428\n",
      "[285]\ttraining's rmse: 0.813703\tvalid_1's rmse: 0.747831\n",
      "[286]\ttraining's rmse: 0.813343\tvalid_1's rmse: 0.748452\n",
      "[287]\ttraining's rmse: 0.813113\tvalid_1's rmse: 0.749438\n",
      "[288]\ttraining's rmse: 0.812769\tvalid_1's rmse: 0.750206\n",
      "[289]\ttraining's rmse: 0.81253\tvalid_1's rmse: 0.75121\n",
      "[290]\ttraining's rmse: 0.812326\tvalid_1's rmse: 0.751598\n",
      "[291]\ttraining's rmse: 0.812159\tvalid_1's rmse: 0.752221\n",
      "[292]\ttraining's rmse: 0.811981\tvalid_1's rmse: 0.752686\n",
      "[293]\ttraining's rmse: 0.81169\tvalid_1's rmse: 0.753376\n",
      "[294]\ttraining's rmse: 0.81141\tvalid_1's rmse: 0.754366\n",
      "[295]\ttraining's rmse: 0.811253\tvalid_1's rmse: 0.754728\n",
      "[296]\ttraining's rmse: 0.810911\tvalid_1's rmse: 0.755126\n",
      "[297]\ttraining's rmse: 0.810753\tvalid_1's rmse: 0.755702\n",
      "[298]\ttraining's rmse: 0.810458\tvalid_1's rmse: 0.756676\n",
      "[299]\ttraining's rmse: 0.810144\tvalid_1's rmse: 0.757426\n",
      "[300]\ttraining's rmse: 0.809825\tvalid_1's rmse: 0.758221\n",
      "[301]\ttraining's rmse: 0.809529\tvalid_1's rmse: 0.759121\n",
      "[302]\ttraining's rmse: 0.809262\tvalid_1's rmse: 0.759657\n",
      "[303]\ttraining's rmse: 0.80908\tvalid_1's rmse: 0.760138\n",
      "[304]\ttraining's rmse: 0.808868\tvalid_1's rmse: 0.760732\n",
      "[305]\ttraining's rmse: 0.808657\tvalid_1's rmse: 0.761364\n",
      "[306]\ttraining's rmse: 0.808405\tvalid_1's rmse: 0.762285\n",
      "[307]\ttraining's rmse: 0.808026\tvalid_1's rmse: 0.763289\n",
      "[308]\ttraining's rmse: 0.807835\tvalid_1's rmse: 0.764154\n",
      "[309]\ttraining's rmse: 0.807659\tvalid_1's rmse: 0.764269\n",
      "[310]\ttraining's rmse: 0.80749\tvalid_1's rmse: 0.764706\n",
      "[311]\ttraining's rmse: 0.807292\tvalid_1's rmse: 0.76516\n",
      "[312]\ttraining's rmse: 0.807137\tvalid_1's rmse: 0.765733\n",
      "[313]\ttraining's rmse: 0.806645\tvalid_1's rmse: 0.76638\n",
      "[314]\ttraining's rmse: 0.806491\tvalid_1's rmse: 0.76681\n",
      "[315]\ttraining's rmse: 0.806308\tvalid_1's rmse: 0.767367\n",
      "[316]\ttraining's rmse: 0.806103\tvalid_1's rmse: 0.767689\n",
      "[317]\ttraining's rmse: 0.805923\tvalid_1's rmse: 0.768225\n",
      "[318]\ttraining's rmse: 0.805773\tvalid_1's rmse: 0.768516\n",
      "[319]\ttraining's rmse: 0.805584\tvalid_1's rmse: 0.768672\n",
      "[320]\ttraining's rmse: 0.805428\tvalid_1's rmse: 0.768952\n",
      "[321]\ttraining's rmse: 0.805227\tvalid_1's rmse: 0.769221\n",
      "[322]\ttraining's rmse: 0.805068\tvalid_1's rmse: 0.769747\n",
      "[323]\ttraining's rmse: 0.804955\tvalid_1's rmse: 0.770223\n",
      "[324]\ttraining's rmse: 0.804807\tvalid_1's rmse: 0.770694\n",
      "[325]\ttraining's rmse: 0.804674\tvalid_1's rmse: 0.770863\n",
      "[326]\ttraining's rmse: 0.804561\tvalid_1's rmse: 0.771158\n",
      "[327]\ttraining's rmse: 0.804412\tvalid_1's rmse: 0.771636\n",
      "[328]\ttraining's rmse: 0.804276\tvalid_1's rmse: 0.77191\n",
      "[329]\ttraining's rmse: 0.804036\tvalid_1's rmse: 0.772192\n",
      "[330]\ttraining's rmse: 0.80381\tvalid_1's rmse: 0.77279\n",
      "[331]\ttraining's rmse: 0.803643\tvalid_1's rmse: 0.773046\n",
      "[332]\ttraining's rmse: 0.803402\tvalid_1's rmse: 0.773324\n",
      "[333]\ttraining's rmse: 0.803176\tvalid_1's rmse: 0.773602\n",
      "[334]\ttraining's rmse: 0.802945\tvalid_1's rmse: 0.774047\n",
      "[335]\ttraining's rmse: 0.802805\tvalid_1's rmse: 0.774422\n",
      "[336]\ttraining's rmse: 0.802638\tvalid_1's rmse: 0.77474\n",
      "[337]\ttraining's rmse: 0.80249\tvalid_1's rmse: 0.775191\n",
      "[338]\ttraining's rmse: 0.802364\tvalid_1's rmse: 0.775804\n",
      "[339]\ttraining's rmse: 0.802157\tvalid_1's rmse: 0.776212\n",
      "[340]\ttraining's rmse: 0.802017\tvalid_1's rmse: 0.776532\n",
      "[341]\ttraining's rmse: 0.801866\tvalid_1's rmse: 0.777015\n",
      "[342]\ttraining's rmse: 0.801658\tvalid_1's rmse: 0.7778\n",
      "[343]\ttraining's rmse: 0.801525\tvalid_1's rmse: 0.778083\n",
      "[344]\ttraining's rmse: 0.801361\tvalid_1's rmse: 0.778326\n",
      "[345]\ttraining's rmse: 0.801229\tvalid_1's rmse: 0.778677\n",
      "[346]\ttraining's rmse: 0.801111\tvalid_1's rmse: 0.778759\n",
      "[347]\ttraining's rmse: 0.800941\tvalid_1's rmse: 0.779092\n",
      "[348]\ttraining's rmse: 0.800833\tvalid_1's rmse: 0.779541\n",
      "[349]\ttraining's rmse: 0.800676\tvalid_1's rmse: 0.779962\n",
      "[350]\ttraining's rmse: 0.800519\tvalid_1's rmse: 0.780056\n",
      "[351]\ttraining's rmse: 0.800381\tvalid_1's rmse: 0.780285\n",
      "[352]\ttraining's rmse: 0.800161\tvalid_1's rmse: 0.781013\n",
      "[353]\ttraining's rmse: 0.800048\tvalid_1's rmse: 0.781397\n",
      "[354]\ttraining's rmse: 0.799849\tvalid_1's rmse: 0.782479\n",
      "[355]\ttraining's rmse: 0.799687\tvalid_1's rmse: 0.783076\n",
      "[356]\ttraining's rmse: 0.799583\tvalid_1's rmse: 0.783234\n",
      "[357]\ttraining's rmse: 0.799446\tvalid_1's rmse: 0.783668\n",
      "[358]\ttraining's rmse: 0.799309\tvalid_1's rmse: 0.78377\n",
      "[359]\ttraining's rmse: 0.799019\tvalid_1's rmse: 0.784583\n",
      "[360]\ttraining's rmse: 0.7988\tvalid_1's rmse: 0.784834\n",
      "[361]\ttraining's rmse: 0.798394\tvalid_1's rmse: 0.785376\n",
      "[362]\ttraining's rmse: 0.798255\tvalid_1's rmse: 0.785736\n",
      "[363]\ttraining's rmse: 0.798096\tvalid_1's rmse: 0.786509\n",
      "[364]\ttraining's rmse: 0.797937\tvalid_1's rmse: 0.786743\n",
      "[365]\ttraining's rmse: 0.797759\tvalid_1's rmse: 0.786936\n",
      "[366]\ttraining's rmse: 0.797607\tvalid_1's rmse: 0.787477\n",
      "[367]\ttraining's rmse: 0.79744\tvalid_1's rmse: 0.787637\n",
      "[368]\ttraining's rmse: 0.797167\tvalid_1's rmse: 0.787676\n",
      "[369]\ttraining's rmse: 0.797036\tvalid_1's rmse: 0.788189\n",
      "[370]\ttraining's rmse: 0.796789\tvalid_1's rmse: 0.788618\n",
      "[371]\ttraining's rmse: 0.796654\tvalid_1's rmse: 0.789001\n",
      "[372]\ttraining's rmse: 0.79656\tvalid_1's rmse: 0.789213\n",
      "[373]\ttraining's rmse: 0.796392\tvalid_1's rmse: 0.789568\n",
      "[374]\ttraining's rmse: 0.796255\tvalid_1's rmse: 0.789627\n",
      "[375]\ttraining's rmse: 0.795854\tvalid_1's rmse: 0.790354\n",
      "[376]\ttraining's rmse: 0.795696\tvalid_1's rmse: 0.790639\n",
      "[377]\ttraining's rmse: 0.795507\tvalid_1's rmse: 0.791024\n",
      "[378]\ttraining's rmse: 0.795365\tvalid_1's rmse: 0.791635\n",
      "[379]\ttraining's rmse: 0.79526\tvalid_1's rmse: 0.792097\n",
      "[380]\ttraining's rmse: 0.795159\tvalid_1's rmse: 0.792366\n",
      "[381]\ttraining's rmse: 0.794956\tvalid_1's rmse: 0.793157\n",
      "[382]\ttraining's rmse: 0.794722\tvalid_1's rmse: 0.793631\n",
      "[383]\ttraining's rmse: 0.794609\tvalid_1's rmse: 0.793675\n",
      "[384]\ttraining's rmse: 0.794463\tvalid_1's rmse: 0.793813\n",
      "[385]\ttraining's rmse: 0.79432\tvalid_1's rmse: 0.793967\n",
      "[386]\ttraining's rmse: 0.794215\tvalid_1's rmse: 0.794306\n",
      "[387]\ttraining's rmse: 0.794021\tvalid_1's rmse: 0.794828\n",
      "[388]\ttraining's rmse: 0.793933\tvalid_1's rmse: 0.794897\n",
      "[389]\ttraining's rmse: 0.793781\tvalid_1's rmse: 0.795578\n",
      "[390]\ttraining's rmse: 0.793662\tvalid_1's rmse: 0.795938\n",
      "[391]\ttraining's rmse: 0.793531\tvalid_1's rmse: 0.796382\n",
      "[392]\ttraining's rmse: 0.79338\tvalid_1's rmse: 0.796677\n",
      "[393]\ttraining's rmse: 0.79326\tvalid_1's rmse: 0.797056\n",
      "[394]\ttraining's rmse: 0.793056\tvalid_1's rmse: 0.797565\n",
      "[395]\ttraining's rmse: 0.79291\tvalid_1's rmse: 0.797898\n",
      "[396]\ttraining's rmse: 0.792741\tvalid_1's rmse: 0.798201\n",
      "[397]\ttraining's rmse: 0.792594\tvalid_1's rmse: 0.798778\n",
      "[398]\ttraining's rmse: 0.792503\tvalid_1's rmse: 0.798792\n",
      "[399]\ttraining's rmse: 0.792385\tvalid_1's rmse: 0.79914\n",
      "[400]\ttraining's rmse: 0.792251\tvalid_1's rmse: 0.799439\n",
      "[401]\ttraining's rmse: 0.792048\tvalid_1's rmse: 0.799683\n",
      "[402]\ttraining's rmse: 0.791911\tvalid_1's rmse: 0.800081\n",
      "[403]\ttraining's rmse: 0.791814\tvalid_1's rmse: 0.800236\n",
      "[404]\ttraining's rmse: 0.791691\tvalid_1's rmse: 0.800734\n",
      "[405]\ttraining's rmse: 0.791589\tvalid_1's rmse: 0.800954\n",
      "[406]\ttraining's rmse: 0.791476\tvalid_1's rmse: 0.801301\n",
      "[407]\ttraining's rmse: 0.791292\tvalid_1's rmse: 0.801796\n",
      "[408]\ttraining's rmse: 0.791145\tvalid_1's rmse: 0.802076\n",
      "[409]\ttraining's rmse: 0.791051\tvalid_1's rmse: 0.802225\n",
      "[410]\ttraining's rmse: 0.790933\tvalid_1's rmse: 0.802447\n",
      "[411]\ttraining's rmse: 0.790803\tvalid_1's rmse: 0.802427\n",
      "[412]\ttraining's rmse: 0.790702\tvalid_1's rmse: 0.80278\n",
      "[413]\ttraining's rmse: 0.79052\tvalid_1's rmse: 0.803118\n",
      "[414]\ttraining's rmse: 0.790396\tvalid_1's rmse: 0.803454\n",
      "[415]\ttraining's rmse: 0.790314\tvalid_1's rmse: 0.803602\n",
      "[416]\ttraining's rmse: 0.790191\tvalid_1's rmse: 0.80375\n",
      "[417]\ttraining's rmse: 0.79\tvalid_1's rmse: 0.803895\n",
      "[418]\ttraining's rmse: 0.789884\tvalid_1's rmse: 0.804204\n",
      "[419]\ttraining's rmse: 0.789679\tvalid_1's rmse: 0.804531\n",
      "[420]\ttraining's rmse: 0.789532\tvalid_1's rmse: 0.804893\n",
      "[421]\ttraining's rmse: 0.789397\tvalid_1's rmse: 0.805086\n",
      "[422]\ttraining's rmse: 0.789281\tvalid_1's rmse: 0.805086\n",
      "[423]\ttraining's rmse: 0.789191\tvalid_1's rmse: 0.805399\n",
      "[424]\ttraining's rmse: 0.789062\tvalid_1's rmse: 0.805444\n",
      "[425]\ttraining's rmse: 0.788962\tvalid_1's rmse: 0.805673\n",
      "[426]\ttraining's rmse: 0.788839\tvalid_1's rmse: 0.805715\n",
      "[427]\ttraining's rmse: 0.78867\tvalid_1's rmse: 0.806252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[428]\ttraining's rmse: 0.788542\tvalid_1's rmse: 0.806373\n",
      "[429]\ttraining's rmse: 0.788362\tvalid_1's rmse: 0.806755\n",
      "[430]\ttraining's rmse: 0.788179\tvalid_1's rmse: 0.807068\n",
      "[431]\ttraining's rmse: 0.788077\tvalid_1's rmse: 0.807164\n",
      "[432]\ttraining's rmse: 0.787936\tvalid_1's rmse: 0.807372\n",
      "[433]\ttraining's rmse: 0.787789\tvalid_1's rmse: 0.807691\n",
      "[434]\ttraining's rmse: 0.787632\tvalid_1's rmse: 0.808106\n",
      "[435]\ttraining's rmse: 0.787507\tvalid_1's rmse: 0.808209\n",
      "[436]\ttraining's rmse: 0.787403\tvalid_1's rmse: 0.808337\n",
      "[437]\ttraining's rmse: 0.787303\tvalid_1's rmse: 0.808434\n",
      "[438]\ttraining's rmse: 0.787099\tvalid_1's rmse: 0.80887\n",
      "[439]\ttraining's rmse: 0.787001\tvalid_1's rmse: 0.809158\n",
      "[440]\ttraining's rmse: 0.786886\tvalid_1's rmse: 0.809299\n",
      "[441]\ttraining's rmse: 0.786781\tvalid_1's rmse: 0.809577\n",
      "[442]\ttraining's rmse: 0.786659\tvalid_1's rmse: 0.809614\n",
      "[443]\ttraining's rmse: 0.78656\tvalid_1's rmse: 0.809712\n",
      "[444]\ttraining's rmse: 0.786466\tvalid_1's rmse: 0.809742\n",
      "[445]\ttraining's rmse: 0.78635\tvalid_1's rmse: 0.809837\n",
      "[446]\ttraining's rmse: 0.786158\tvalid_1's rmse: 0.810259\n",
      "[447]\ttraining's rmse: 0.78604\tvalid_1's rmse: 0.810289\n",
      "[448]\ttraining's rmse: 0.785966\tvalid_1's rmse: 0.810394\n",
      "[449]\ttraining's rmse: 0.785858\tvalid_1's rmse: 0.810521\n",
      "[450]\ttraining's rmse: 0.785783\tvalid_1's rmse: 0.810636\n",
      "[451]\ttraining's rmse: 0.785699\tvalid_1's rmse: 0.810856\n",
      "[452]\ttraining's rmse: 0.785608\tvalid_1's rmse: 0.811029\n",
      "[453]\ttraining's rmse: 0.785508\tvalid_1's rmse: 0.811105\n",
      "[454]\ttraining's rmse: 0.785402\tvalid_1's rmse: 0.811231\n",
      "[455]\ttraining's rmse: 0.785323\tvalid_1's rmse: 0.811443\n",
      "[456]\ttraining's rmse: 0.785217\tvalid_1's rmse: 0.811989\n",
      "[457]\ttraining's rmse: 0.785086\tvalid_1's rmse: 0.812083\n",
      "[458]\ttraining's rmse: 0.784958\tvalid_1's rmse: 0.812302\n",
      "[459]\ttraining's rmse: 0.784873\tvalid_1's rmse: 0.812487\n",
      "[460]\ttraining's rmse: 0.784786\tvalid_1's rmse: 0.812561\n",
      "[461]\ttraining's rmse: 0.784688\tvalid_1's rmse: 0.812755\n",
      "[462]\ttraining's rmse: 0.784576\tvalid_1's rmse: 0.813065\n",
      "[463]\ttraining's rmse: 0.784469\tvalid_1's rmse: 0.813265\n",
      "[464]\ttraining's rmse: 0.784389\tvalid_1's rmse: 0.813284\n",
      "[465]\ttraining's rmse: 0.784321\tvalid_1's rmse: 0.813311\n",
      "[466]\ttraining's rmse: 0.784191\tvalid_1's rmse: 0.813425\n",
      "[467]\ttraining's rmse: 0.784074\tvalid_1's rmse: 0.813585\n",
      "[468]\ttraining's rmse: 0.783677\tvalid_1's rmse: 0.813843\n",
      "[469]\ttraining's rmse: 0.783486\tvalid_1's rmse: 0.813933\n",
      "[470]\ttraining's rmse: 0.783366\tvalid_1's rmse: 0.814071\n",
      "[471]\ttraining's rmse: 0.783296\tvalid_1's rmse: 0.814181\n",
      "[472]\ttraining's rmse: 0.783187\tvalid_1's rmse: 0.814347\n",
      "[473]\ttraining's rmse: 0.783092\tvalid_1's rmse: 0.814633\n",
      "[474]\ttraining's rmse: 0.782969\tvalid_1's rmse: 0.814617\n",
      "[475]\ttraining's rmse: 0.782872\tvalid_1's rmse: 0.814884\n",
      "[476]\ttraining's rmse: 0.782794\tvalid_1's rmse: 0.815082\n",
      "[477]\ttraining's rmse: 0.782734\tvalid_1's rmse: 0.815155\n",
      "[478]\ttraining's rmse: 0.782606\tvalid_1's rmse: 0.815302\n",
      "[479]\ttraining's rmse: 0.782545\tvalid_1's rmse: 0.815356\n",
      "[480]\ttraining's rmse: 0.782472\tvalid_1's rmse: 0.815364\n",
      "[481]\ttraining's rmse: 0.782386\tvalid_1's rmse: 0.815509\n",
      "[482]\ttraining's rmse: 0.782288\tvalid_1's rmse: 0.815659\n",
      "[483]\ttraining's rmse: 0.78219\tvalid_1's rmse: 0.815667\n",
      "[484]\ttraining's rmse: 0.78212\tvalid_1's rmse: 0.815794\n",
      "[485]\ttraining's rmse: 0.781912\tvalid_1's rmse: 0.816187\n",
      "[486]\ttraining's rmse: 0.781814\tvalid_1's rmse: 0.816361\n",
      "[487]\ttraining's rmse: 0.781719\tvalid_1's rmse: 0.816533\n",
      "[488]\ttraining's rmse: 0.781654\tvalid_1's rmse: 0.816702\n",
      "[489]\ttraining's rmse: 0.781594\tvalid_1's rmse: 0.816746\n",
      "[490]\ttraining's rmse: 0.781486\tvalid_1's rmse: 0.816945\n",
      "[491]\ttraining's rmse: 0.781423\tvalid_1's rmse: 0.817004\n",
      "[492]\ttraining's rmse: 0.781322\tvalid_1's rmse: 0.817047\n",
      "[493]\ttraining's rmse: 0.781173\tvalid_1's rmse: 0.81753\n",
      "[494]\ttraining's rmse: 0.781095\tvalid_1's rmse: 0.817649\n",
      "[495]\ttraining's rmse: 0.780975\tvalid_1's rmse: 0.817708\n",
      "[496]\ttraining's rmse: 0.780875\tvalid_1's rmse: 0.817829\n",
      "[497]\ttraining's rmse: 0.780723\tvalid_1's rmse: 0.818225\n",
      "[498]\ttraining's rmse: 0.780623\tvalid_1's rmse: 0.818187\n",
      "[499]\ttraining's rmse: 0.780471\tvalid_1's rmse: 0.818175\n",
      "[500]\ttraining's rmse: 0.780384\tvalid_1's rmse: 0.81838\n"
     ]
    }
   ],
   "source": [
    "trainData = matrix[matrix['date_block_num'] < 34]\n",
    "label_train = trainData['item_cnt_month']\n",
    "X_train = trainData.drop('item_cnt_month', axis=1)\n",
    "\n",
    "validData = matrix[matrix['date_block_num'] == 34]\n",
    "label_valid = validData['item_cnt_month']\n",
    "X_valid = validData.drop('item_cnt_month', axis=1)\n",
    "\n",
    "import lightgbm as lgb\n",
    "train_data = lgb.Dataset(data=X_train, label=label_train)\n",
    "valid_data = lgb.Dataset(data=X_valid, label=label_valid)\n",
    "params = {\n",
    "    'objective': 'regression',  # 回归\n",
    "    'metric': 'rmse',   # 回归问题选择rmse\n",
    "    'n_estimators': 500,\n",
    "    'num_leaves': 150,   # 每个弱学习器拥有的叶子的数量\n",
    "    'learning_rate': 0.01,\n",
    "    'bagging_fraction': 0.9,    # 每次训练“弱学习器”用的数据比例（应该也是随机的），用于加快训练速度和减小过拟合\n",
    "    'feature_fraction': 0.3,   # 每次迭代过程中，随机选择30%的特征建树（弱学习器）\n",
    "    'bagging_seed': 0,\n",
    "    'early_stop_rounds': 50\n",
    "}\n",
    "lgb_model = lgb.train(params, train_data, valid_sets=[train_data, valid_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8183262547941259"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "valid_prediction = lgb_model.predict(X_valid).clip(0,20)\n",
    "rmse_valid = np.sqrt(mean_squared_error(valid_prediction, label_valid))\n",
    "rmse_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取最近6个月销售量为0的数据\n",
    "six_zero = sales_by_item_id[(sales_by_item_id['28'] == 0) & (sales_by_item_id['29'] == 0) & (sales_by_item_id['30'] == 0) & (sales_by_item_id['31'] == 0) & (sales_by_item_id['32'] == 0) & (sales_by_item_id['33'] == 0)]\n",
    "six_zero_item_id = list(six_zero['item_id'].values)   # item_id列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12893"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(six_zero_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>item_cnt_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>5</td>\n",
       "      <td>18729</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>493</td>\n",
       "      <td>5</td>\n",
       "      <td>562</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>817</td>\n",
       "      <td>5</td>\n",
       "      <td>18698</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>953</td>\n",
       "      <td>5</td>\n",
       "      <td>19813</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>1165</td>\n",
       "      <td>5</td>\n",
       "      <td>12715</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214165</th>\n",
       "      <td>214165</td>\n",
       "      <td>45</td>\n",
       "      <td>18589</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214167</th>\n",
       "      <td>214167</td>\n",
       "      <td>45</td>\n",
       "      <td>11467</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214176</th>\n",
       "      <td>214176</td>\n",
       "      <td>45</td>\n",
       "      <td>11137</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214179</th>\n",
       "      <td>214179</td>\n",
       "      <td>45</td>\n",
       "      <td>2972</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214180</th>\n",
       "      <td>214180</td>\n",
       "      <td>45</td>\n",
       "      <td>19889</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7812 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  shop_id  item_id  date_block_num  item_cnt_month\n",
       "298        298        5    18729              34             0.0\n",
       "493        493        5      562              34             0.0\n",
       "817        817        5    18698              34             0.0\n",
       "953        953        5    19813              34             0.0\n",
       "1165      1165        5    12715              34             0.0\n",
       "...        ...      ...      ...             ...             ...\n",
       "214165  214165       45    18589              34             0.0\n",
       "214167  214167       45    11467              34             0.0\n",
       "214176  214176       45    11137              34             0.0\n",
       "214179  214179       45     2972              34             0.0\n",
       "214180  214180       45    19889              34             0.0\n",
       "\n",
       "[7812 rows x 5 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.item_id.isin(six_zero_item_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test数据\n",
    "testData = matrix[matrix['date_block_num'] == 34]\n",
    "X_test = testData.drop('item_cnt_month', axis=1)\n",
    "\n",
    "# 预测&生成文件\n",
    "y_test = lgb_model.predict(X_test).clip(0, 20)\n",
    "submission = pd.DataFrame({ 'ID': range(0, 214200), 'item_cnt_month': y_test})\n",
    "\n",
    "test0 = test[test.item_id.isin(six_zero_item_id)]\n",
    "ids = list(test0.ID.values)\n",
    "submission.loc[submission.ID.isin(ids), 'item_cnt_month'] = 0.0\n",
    "submission.to_csv('./submit/sub10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
