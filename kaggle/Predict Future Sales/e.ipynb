{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from icecream import ic\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from itertools import product\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "sales_train = pd.read_csv('./data/sales_train.csv')\n",
    "test = pd.read_csv('./data/test.csv')   # (214200, 3)\n",
    "\n",
    "# 计算每个商品每个月的销售量，假如某个商品在某个月没有数据，则填充0（即这个月的销售量为0）\n",
    "sales_by_item_id = sales_train.pivot_table(index=['item_id'], values=['item_cnt_day'], columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\n",
    "sales_by_item_id.columns = sales_by_item_id.columns.droplevel().map(str)   # 去掉第一层索引\n",
    "sales_by_item_id.columns.values[0] = 'item_id'\n",
    "sales_by_item_id = sales_by_item_id.rename_axis(None, axis=1)\n",
    "\n",
    "# 获取最近6个月销售量为0的数据\n",
    "six_zero = sales_by_item_id[(sales_by_item_id['28'] == 0) & (sales_by_item_id['29'] == 0) & (sales_by_item_id['30'] == 0) & (sales_by_item_id['31'] == 0) & (sales_by_item_id['32'] == 0) & (sales_by_item_id['33'] == 0)]\n",
    "six_zero_item_id = list(six_zero['item_id'].values)   # item_id列表\n",
    "# test.loc[test.item_id.isin(six_zero_item_id), 'item_cnt_month'] = 0  # 将test数据中（最近六个月销量为0）的数据月销量设为0，有7812个\n",
    "\n",
    "# 计算每个商店每个月的销量\n",
    "sales_by_shop_id = sales_train.pivot_table(index=['shop_id'], values=['item_cnt_day'], aggfunc=np.sum, fill_value=0, columns='date_block_num').reset_index()\n",
    "sales_by_shop_id.columns = sales_by_shop_id.columns.droplevel().map(str)    # 将两层column转化为一层column,保留下层column\n",
    "sales_by_shop_id.columns.values[0] = 'shop_id'\n",
    "sales_by_shop_id = sales_by_shop_id.rename_axis(None, axis=1)   # 将列方向的轴重命名为none\n",
    "\n",
    "# zero = sales_train[sales_train.date_block_num==0]\n",
    "# ic(zero.shop_id.unique(), len(zero.item_id.unique()), len(zero.shop_id.unique()), len(zero.shop_id.unique()) * len(zero.item_id.unique()))\n",
    "# ic(sales_train.shop_id.unique(), len(sales_train.item_id.unique()), len(sales_train.shop_id.unique()), len(sales_train.shop_id.unique()) * len(sales_train.item_id.unique()))\n",
    "\n",
    "\"\"\"组合date_block_num,shop_id,item_id(部分) 总量：10913850\"\"\"\n",
    "matrix = []\n",
    "cols = ['date_block_num','shop_id','item_id']\n",
    "for i in range(34):\n",
    "    sales = sales_train[sales_train.date_block_num==i]\n",
    "    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n",
    "matrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n",
    "matrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\n",
    "matrix['shop_id'] = matrix['shop_id'].astype(np.int8)\n",
    "matrix.sort_values(cols, inplace=True)  # 排序\n",
    "sales_train['revenue'] = sales_train['item_price'] * sales_train['item_cnt_day']    # 某一天的销售额\n",
    "\n",
    "# 分组\n",
    "groupby = sales_train.groupby(['shop_id','item_id','date_block_num']).agg({'item_cnt_day': 'sum'}).reset_index()\n",
    "groupby = groupby.rename(columns={'item_cnt_day': 'item_cnt_month'})\n",
    "matrix = matrix.merge(groupby, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "matrix['item_cnt_month'] = matrix['item_cnt_month'].fillna(0).clip(0, 20)\n",
    "matrix['item_cnt_month'] = matrix['item_cnt_month'].astype(np.float16)\n",
    "\n",
    "# test数据\n",
    "test['date_block_num'] = 34\n",
    "test['date_block_num'] = test['date_block_num'].astype(np.int8)\n",
    "test['shop_id'] = test['shop_id'].astype(np.int8)\n",
    "test['item_id'] = test['item_id'].astype(np.int16)\n",
    "\n",
    "# 合并matrix,test\n",
    "matrix = pd.concat([matrix, test[cols]], ignore_index=True, axis=0)\n",
    "matrix['item_cnt_month'].fillna(0, inplace=True)\n",
    "\n",
    "# 商品信息\n",
    "items = pd.read_csv('./data/items.csv')\n",
    "items = items[['item_id', 'item_category_id']]\n",
    "matrix = pd.merge(left=matrix, right=items, on='item_id', how='left')  # 合并\n",
    "\n",
    "# 商品类别\n",
    "le = LabelEncoder()\n",
    "categories = pd.read_csv('./data/item_categories.csv')\n",
    "categories['split'] = categories['item_category_name'].str.split('-')\n",
    "categories['type'] = categories['split'].map(lambda x:x[0].strip())\n",
    "categories['subtype'] = categories['split'].map(lambda x:x[1].strip() if len(x)>1 else x[0].strip())\n",
    "categories = categories[['item_category_id','type','subtype']]\n",
    "categories['cat_type_code'] = le.fit_transform(categories['type'])\n",
    "categories['cat_subtype_code'] = le.fit_transform(categories['subtype'])\n",
    "matrix = pd.merge(left=matrix, right=categories[['item_category_id','cat_type_code','cat_subtype_code']], on='item_category_id', how='left')    # 合并\n",
    "\n",
    "# 商店信息\n",
    "shops = pd.read_csv('./data/shops.csv')\n",
    "shops['split']=shops.shop_name.str.split(' ')\n",
    "shops['shop_city'] = shops['split'].map(lambda x:x[0])\n",
    "shops['shop_city_code'] = le.fit_transform(shops['shop_city'])\n",
    "\n",
    "def st(name):\n",
    "    if 'ТЦ' in name or 'ТРЦ' in name:\n",
    "        shopt = 'ТЦ'\n",
    "    elif 'ТК' in name:\n",
    "        shopt = 'ТК'\n",
    "    elif 'ТРК' in name:\n",
    "        shopt = 'ТРК'\n",
    "    elif 'МТРЦ' in name:\n",
    "        shopt = 'МТРЦ'\n",
    "    else:\n",
    "        shopt = 'UNKNOWN'\n",
    "    return shopt\n",
    "shops['shop_type'] = shops['shop_name'].apply(st)\n",
    "\n",
    "shops.loc[shops.shop_id == 21, 'shop_type'] = 'МТРЦ'   # 修正\n",
    "shops['shop_type_code'] = le.fit_transform(shops['shop_type'])\n",
    "matrix = pd.merge(left=matrix, right=shops[['shop_id','shop_city_code','shop_type_code']], on='shop_id', how='left')    # 合并\n",
    "matrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\n",
    "matrix['cat_type_code'] = matrix['cat_type_code'].astype(np.int8)\n",
    "matrix['cat_subtype_code'] = matrix['cat_subtype_code'].astype(np.int8)\n",
    "matrix['shop_city_code'] = matrix['shop_city_code'].astype(np.int8)\n",
    "matrix['shop_type_code'] = matrix['shop_type_code'].astype(np.int8)\n",
    "\n",
    "\n",
    "\"\"\"历史信息\"\"\"\n",
    "\n",
    "def lag_features(df, lags, col):\n",
    "    tmp = df[['date_block_num','shop_id','item_id',col]]\n",
    "    for i in lags:\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id',col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] = shifted['date_block_num'] + i\n",
    "        df = pd.merge(left=df, right=shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "# 月销量（所有商品）\n",
    "group = matrix.groupby('date_block_num').agg({'item_cnt_month': 'mean'}).reset_index()\n",
    "group.columns = ['date_block_num', 'date_avg_item_cnt']\n",
    "matrix = pd.merge(left=matrix, right=group, on='date_block_num', how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'date_avg_item_cnt')\n",
    "matrix.drop('date_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（每一件商品）\n",
    "group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_item_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num','item_id'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'date_item_avg_item_cnt')\n",
    "matrix.drop('date_item_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（每个商店 ）\n",
    "group = matrix.groupby(['date_block_num','shop_id']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['date_shop_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num','shop_id'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'date_shop_avg_item_cnt')\n",
    "matrix.drop('date_shop_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# 月销量（每个类别）\n",
    "group = matrix.groupby(['date_block_num','item_category_id']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['date_cat_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix=pd.merge(left=matrix, right=group, on=['date_block_num','item_category_id'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'date_cat_avg_item_cnt')\n",
    "matrix.drop('date_cat_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（商品类别-商店）\n",
    "group = matrix.groupby(['date_block_num','item_category_id','shop_id']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['date_cat_shop_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num','item_category_id','shop_id'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'date_cat_shop_avg_item_cnt')\n",
    "matrix.drop('date_cat_shop_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（商品大类）\n",
    "group = matrix.groupby(['date_block_num','cat_type_code']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['date_type_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num','cat_type_code'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'date_type_avg_item_cnt')\n",
    "matrix.drop('date_type_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（商品-商品大类） ++++++++++++ 和 月销量（商品）是重复的，因为每一个商品，类别是确定的，大类也是确定的\n",
    "group = matrix.groupby(['date_block_num', 'item_id', 'cat_type_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_item_type_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num', 'item_id', 'cat_type_code'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'date_item_type_avg_item_cnt')\n",
    "matrix.drop('date_item_type_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（商店城市）\n",
    "group = matrix.groupby(['date_block_num','shop_city_code']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['date_city_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num','shop_city_code'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'date_city_avg_item_cnt')\n",
    "matrix.drop('date_city_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（商品-商店城市）\n",
    "group = matrix.groupby(['date_block_num', 'item_id', 'shop_city_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_item_city_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix=pd.merge(left=matrix, right=group, on=['date_block_num', 'item_id', 'shop_city_code'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'date_item_city_avg_item_cnt')\n",
    "matrix.drop('date_item_city_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# 趋势特征\n",
    "group = sales_train.groupby('item_id').agg({'item_price': 'mean'})\n",
    "group.columns = ['item_avg_item_price']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on='item_id', how='left')\n",
    "\n",
    "group = sales_train.groupby(['date_block_num','item_id']).agg({'item_price': 'mean'})\n",
    "group.columns = ['date_item_avg_item_price']\n",
    "group = group.reset_index()\n",
    "matrix=pd.merge(left=matrix, right=group, on=['date_block_num','item_id'], how='left')\n",
    "\n",
    "matrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n",
    "matrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "# 计算matrix中商品的历史价格\n",
    "lags = [1,2,3,4,5]\n",
    "matrix = lag_features(matrix, lags, 'date_item_avg_item_price')\n",
    "for i in lags:\n",
    "    matrix['delta_price_lag_'+str(i)] = (matrix['date_item_avg_item_price_lag_' + str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n",
    "\n",
    "def select_trend(row):\n",
    "    for i in lags:\n",
    "        if pd.notnull(row['delta_price_lag_'+str(i)]):  # 如果不是NaN\n",
    "            return row['delta_price_lag_'+str(i)]\n",
    "    return 0   #  如果delta_price_lag_都为空，那么将趋势设为0，0代表没有趋势\n",
    "\n",
    "matrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\n",
    "matrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\n",
    "\n",
    "features_to_drop = ['item_avg_item_price','date_item_avg_item_price']\n",
    "for i in lags:\n",
    "    features_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n",
    "    features_to_drop += ['delta_price_lag_'+str(i)]\n",
    "matrix.drop(features_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# 每个月的天数\n",
    "matrix['month'] = matrix['date_block_num'] % 12\n",
    "days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n",
    "matrix['days'] = matrix['month'].map(days)\n",
    "matrix['days'] = matrix['days'].astype(np.int8)\n",
    "\n",
    "# 开始销量\n",
    "matrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n",
    "matrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\n",
    "\n",
    "# 月销量（商店类型）\n",
    "group = matrix.groupby(['date_block_num','shop_type_code']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['date_shoptype_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num','shop_type_code'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'date_shoptype_avg_item_cnt')\n",
    "matrix.drop('date_shoptype_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（商品-商店类型）\n",
    "group = matrix.groupby(['date_block_num', 'item_id', 'shop_type_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_item_shoptype_avg_item_cnt']\n",
    "group = group.reset_index()\n",
    "matrix=pd.merge(left=matrix, right=group, on=['date_block_num', 'item_id', 'shop_type_code'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'date_item_shoptype_avg_item_cnt')\n",
    "matrix.drop('date_item_shoptype_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 月销量（商店-商品）\n",
    "group = matrix.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_shopitem_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num', 'shop_id', 'item_id'], how='left')\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'date_shopitem_avg_item_cnt')\n",
    "matrix.drop('date_shopitem_avg_item_cnt', axis=1, inplace=True)\n",
    "\n",
    "# 趋势特征--item_cnt_month\n",
    "group = matrix.groupby('item_id').agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['trend_item_avg_cnt_month']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on='item_id', how='left')\n",
    "\n",
    "group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['trend_date_item_avg_cnt_month']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['date_block_num', 'item_id'], how='left')\n",
    "\n",
    "# 计算matrix中商品的历史价格\n",
    "lags = [1,2,3,4,5]\n",
    "matrix = lag_features(matrix, lags, 'trend_date_item_avg_cnt_month')\n",
    "for i in lags:\n",
    "    matrix['delta_cnt_month_lag_' + str(i)] = (matrix['trend_date_item_avg_cnt_month_lag_' + str(i)] - matrix[\n",
    "        'trend_item_avg_cnt_month']) / matrix['trend_item_avg_cnt_month']\n",
    "\n",
    "\n",
    "def select_trend2(row):\n",
    "    for i in lags:\n",
    "        if pd.notnull(row['delta_cnt_month_lag_' + str(i)]):  # 如果不是NaN\n",
    "            return row['delta_cnt_month_lag_' + str(i)]\n",
    "    return 0  # 如果delta_price_lag_都为空，那么将趋势设为0，0代表没有趋势\n",
    "\n",
    "\n",
    "matrix['delta_cnt_month_lag'] = matrix.apply(select_trend2, axis=1)\n",
    "\n",
    "matrix['delta_cnt_month_lag'] = matrix['delta_cnt_month_lag'].astype(np.float16)\n",
    "\n",
    "features_to_drop = ['trend_item_avg_cnt_month', 'trend_date_item_avg_cnt_month']\n",
    "for i in lags:\n",
    "    features_to_drop += ['trend_date_item_avg_cnt_month_lag_' + str(i)]\n",
    "    features_to_drop += ['delta_cnt_month_lag_' + str(i)]\n",
    "matrix.drop(features_to_drop, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# 趋势特征 +++++++++++++++++++++++++++++++++++++\n",
    "group = matrix.groupby(['shop_id', 'item_id']).agg({'item_cnt_month': 'mean'})\n",
    "group.columns = ['qushi_shop_item_avg_cnt_month']\n",
    "group = group.reset_index()\n",
    "matrix = pd.merge(left=matrix, right=group, on=['shop_id', 'item_id'], how='left')\n",
    "\n",
    "# 计算matrix中商品的历史价格\n",
    "matrix = lag_features(matrix, [1,2,3,4,5], 'item_cnt_month')\n",
    "                               \n",
    "for i in lags:\n",
    "    matrix['delta2_cnt_month_lag_'+str(i)] = (matrix['item_cnt_month_lag_' + str(i)] - matrix['qushi_shop_item_avg_cnt_month']) / matrix['qushi_shop_item_avg_cnt_month']\n",
    "\n",
    "def select_trend3(row):\n",
    "    for i in lags:\n",
    "        if pd.notnull(row['delta2_cnt_month_lag_'+str(i)]):  # 如果不是NaN\n",
    "            return row['delta2_cnt_month_lag_'+str(i)]\n",
    "    return 0   #  如果delta_price_lag_都为空，那么将趋势设为0，0代表没有趋势\n",
    "\n",
    "matrix['delta2_cnt_month_lag'] = matrix.apply(select_trend3, axis=1)\n",
    "matrix['delta2_cnt_month_lag'] = matrix['delta2_cnt_month_lag'].astype(np.float16)\n",
    "\n",
    "features_to_drop = ['qushi_shop_item_avg_cnt_month']\n",
    "for i in lags:\n",
    "    features_to_drop += ['delta2_cnt_month_lag_'+str(i)]\n",
    "matrix.drop(features_to_drop, axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11128050, 81)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_11128050_81_bak = matrix.copy()   # 已运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为有12个月的延迟特征（1，2，3，6，12）（1，2，3，4，5，6，12），所以需要删除前12月的数据\n",
    "matrix = matrix[matrix['date_block_num'] > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9255330, 81)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_block_num': 9255330,\n",
       " 'shop_id': 9255330,\n",
       " 'item_id': 9255330,\n",
       " 'item_cnt_month': 9255330,\n",
       " 'item_category_id': 9255330,\n",
       " 'cat_type_code': 9255330,\n",
       " 'cat_subtype_code': 9255330,\n",
       " 'shop_city_code': 9255330,\n",
       " 'shop_type_code': 9255330,\n",
       " 'date_avg_item_cnt_lag_1': 7358574,\n",
       " 'date_avg_item_cnt_lag_2': 7072182,\n",
       " 'date_avg_item_cnt_lag_3': 6798279,\n",
       " 'date_avg_item_cnt_lag_4': 6513575,\n",
       " 'date_avg_item_cnt_lag_5': 6222805,\n",
       " 'date_item_avg_item_cnt_lag_1': 7358574,\n",
       " 'date_item_avg_item_cnt_lag_2': 7072182,\n",
       " 'date_item_avg_item_cnt_lag_3': 6798279,\n",
       " 'date_item_avg_item_cnt_lag_4': 6513575,\n",
       " 'date_item_avg_item_cnt_lag_5': 6222805,\n",
       " 'date_shop_avg_item_cnt_lag_1': 7358574,\n",
       " 'date_shop_avg_item_cnt_lag_2': 7072182,\n",
       " 'date_shop_avg_item_cnt_lag_3': 6798279,\n",
       " 'date_shop_avg_item_cnt_lag_4': 6513575,\n",
       " 'date_shop_avg_item_cnt_lag_5': 6222805,\n",
       " 'date_cat_avg_item_cnt_lag_1': 7358574,\n",
       " 'date_cat_avg_item_cnt_lag_2': 7072182,\n",
       " 'date_cat_avg_item_cnt_lag_3': 6798279,\n",
       " 'date_cat_avg_item_cnt_lag_4': 6513575,\n",
       " 'date_cat_avg_item_cnt_lag_5': 6222805,\n",
       " 'date_cat_shop_avg_item_cnt_lag_1': 7358574,\n",
       " 'date_cat_shop_avg_item_cnt_lag_2': 7072182,\n",
       " 'date_cat_shop_avg_item_cnt_lag_3': 6798279,\n",
       " 'date_cat_shop_avg_item_cnt_lag_4': 6513575,\n",
       " 'date_cat_shop_avg_item_cnt_lag_5': 6222805,\n",
       " 'date_type_avg_item_cnt_lag_1': 7358574,\n",
       " 'date_type_avg_item_cnt_lag_2': 7072182,\n",
       " 'date_type_avg_item_cnt_lag_3': 6798279,\n",
       " 'date_type_avg_item_cnt_lag_4': 6513575,\n",
       " 'date_type_avg_item_cnt_lag_5': 6222805,\n",
       " 'date_item_type_avg_item_cnt_lag_1': 7358574,\n",
       " 'date_item_type_avg_item_cnt_lag_2': 7072182,\n",
       " 'date_item_type_avg_item_cnt_lag_3': 6798279,\n",
       " 'date_item_type_avg_item_cnt_lag_4': 6513575,\n",
       " 'date_item_type_avg_item_cnt_lag_5': 6222805,\n",
       " 'date_city_avg_item_cnt_lag_1': 7358574,\n",
       " 'date_city_avg_item_cnt_lag_2': 7072182,\n",
       " 'date_city_avg_item_cnt_lag_3': 6798279,\n",
       " 'date_city_avg_item_cnt_lag_4': 6513575,\n",
       " 'date_city_avg_item_cnt_lag_5': 6222805,\n",
       " 'date_item_city_avg_item_cnt_lag_1': 7358574,\n",
       " 'date_item_city_avg_item_cnt_lag_2': 7072182,\n",
       " 'date_item_city_avg_item_cnt_lag_3': 6798279,\n",
       " 'date_item_city_avg_item_cnt_lag_4': 6513575,\n",
       " 'date_item_city_avg_item_cnt_lag_5': 6222805,\n",
       " 'delta_price_lag': 9255330,\n",
       " 'month': 9255330,\n",
       " 'days': 9255330,\n",
       " 'item_shop_first_sale': 9255330,\n",
       " 'item_first_sale': 9255330,\n",
       " 'date_shoptype_avg_item_cnt_lag_1': 7358574,\n",
       " 'date_shoptype_avg_item_cnt_lag_2': 7072182,\n",
       " 'date_shoptype_avg_item_cnt_lag_3': 6798279,\n",
       " 'date_shoptype_avg_item_cnt_lag_4': 6513575,\n",
       " 'date_shoptype_avg_item_cnt_lag_5': 6222805,\n",
       " 'date_item_shoptype_avg_item_cnt_lag_1': 7358574,\n",
       " 'date_item_shoptype_avg_item_cnt_lag_2': 7072182,\n",
       " 'date_item_shoptype_avg_item_cnt_lag_3': 6798279,\n",
       " 'date_item_shoptype_avg_item_cnt_lag_4': 6513575,\n",
       " 'date_item_shoptype_avg_item_cnt_lag_5': 6222805,\n",
       " 'date_shopitem_avg_item_cnt_lag_1': 7358574,\n",
       " 'date_shopitem_avg_item_cnt_lag_2': 7072182,\n",
       " 'date_shopitem_avg_item_cnt_lag_3': 6798279,\n",
       " 'date_shopitem_avg_item_cnt_lag_4': 6513575,\n",
       " 'date_shopitem_avg_item_cnt_lag_5': 6222805,\n",
       " 'delta_cnt_month_lag': 9255330,\n",
       " 'item_cnt_month_lag_1': 7358574,\n",
       " 'item_cnt_month_lag_2': 7072182,\n",
       " 'item_cnt_month_lag_3': 6798279,\n",
       " 'item_cnt_month_lag_4': 6513575,\n",
       " 'item_cnt_month_lag_5': 6222805,\n",
       " 'delta2_cnt_month_lag': 9255330}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(matrix.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到有NaN值的列，然后把那些列中的NaN值填充0\n",
    "columns = matrix.columns\n",
    "column_null = []\n",
    "for i in columns:\n",
    "    if len(matrix[matrix[i].isnull()]) > 0:\n",
    "        column_null.append(i)\n",
    "\n",
    "for i in column_null:\n",
    "    matrix[i].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_block_num': 9255330,\n",
       " 'shop_id': 9255330,\n",
       " 'item_id': 9255330,\n",
       " 'item_cnt_month': 9255330,\n",
       " 'item_category_id': 9255330,\n",
       " 'cat_type_code': 9255330,\n",
       " 'cat_subtype_code': 9255330,\n",
       " 'shop_city_code': 9255330,\n",
       " 'shop_type_code': 9255330,\n",
       " 'date_avg_item_cnt_lag_1': 9255330,\n",
       " 'date_avg_item_cnt_lag_2': 9255330,\n",
       " 'date_avg_item_cnt_lag_3': 9255330,\n",
       " 'date_avg_item_cnt_lag_4': 9255330,\n",
       " 'date_avg_item_cnt_lag_5': 9255330,\n",
       " 'date_item_avg_item_cnt_lag_1': 9255330,\n",
       " 'date_item_avg_item_cnt_lag_2': 9255330,\n",
       " 'date_item_avg_item_cnt_lag_3': 9255330,\n",
       " 'date_item_avg_item_cnt_lag_4': 9255330,\n",
       " 'date_item_avg_item_cnt_lag_5': 9255330,\n",
       " 'date_shop_avg_item_cnt_lag_1': 9255330,\n",
       " 'date_shop_avg_item_cnt_lag_2': 9255330,\n",
       " 'date_shop_avg_item_cnt_lag_3': 9255330,\n",
       " 'date_shop_avg_item_cnt_lag_4': 9255330,\n",
       " 'date_shop_avg_item_cnt_lag_5': 9255330,\n",
       " 'date_cat_avg_item_cnt_lag_1': 9255330,\n",
       " 'date_cat_avg_item_cnt_lag_2': 9255330,\n",
       " 'date_cat_avg_item_cnt_lag_3': 9255330,\n",
       " 'date_cat_avg_item_cnt_lag_4': 9255330,\n",
       " 'date_cat_avg_item_cnt_lag_5': 9255330,\n",
       " 'date_cat_shop_avg_item_cnt_lag_1': 9255330,\n",
       " 'date_cat_shop_avg_item_cnt_lag_2': 9255330,\n",
       " 'date_cat_shop_avg_item_cnt_lag_3': 9255330,\n",
       " 'date_cat_shop_avg_item_cnt_lag_4': 9255330,\n",
       " 'date_cat_shop_avg_item_cnt_lag_5': 9255330,\n",
       " 'date_type_avg_item_cnt_lag_1': 9255330,\n",
       " 'date_type_avg_item_cnt_lag_2': 9255330,\n",
       " 'date_type_avg_item_cnt_lag_3': 9255330,\n",
       " 'date_type_avg_item_cnt_lag_4': 9255330,\n",
       " 'date_type_avg_item_cnt_lag_5': 9255330,\n",
       " 'date_item_type_avg_item_cnt_lag_1': 9255330,\n",
       " 'date_item_type_avg_item_cnt_lag_2': 9255330,\n",
       " 'date_item_type_avg_item_cnt_lag_3': 9255330,\n",
       " 'date_item_type_avg_item_cnt_lag_4': 9255330,\n",
       " 'date_item_type_avg_item_cnt_lag_5': 9255330,\n",
       " 'date_city_avg_item_cnt_lag_1': 9255330,\n",
       " 'date_city_avg_item_cnt_lag_2': 9255330,\n",
       " 'date_city_avg_item_cnt_lag_3': 9255330,\n",
       " 'date_city_avg_item_cnt_lag_4': 9255330,\n",
       " 'date_city_avg_item_cnt_lag_5': 9255330,\n",
       " 'date_item_city_avg_item_cnt_lag_1': 9255330,\n",
       " 'date_item_city_avg_item_cnt_lag_2': 9255330,\n",
       " 'date_item_city_avg_item_cnt_lag_3': 9255330,\n",
       " 'date_item_city_avg_item_cnt_lag_4': 9255330,\n",
       " 'date_item_city_avg_item_cnt_lag_5': 9255330,\n",
       " 'delta_price_lag': 9255330,\n",
       " 'month': 9255330,\n",
       " 'days': 9255330,\n",
       " 'item_shop_first_sale': 9255330,\n",
       " 'item_first_sale': 9255330,\n",
       " 'date_shoptype_avg_item_cnt_lag_1': 9255330,\n",
       " 'date_shoptype_avg_item_cnt_lag_2': 9255330,\n",
       " 'date_shoptype_avg_item_cnt_lag_3': 9255330,\n",
       " 'date_shoptype_avg_item_cnt_lag_4': 9255330,\n",
       " 'date_shoptype_avg_item_cnt_lag_5': 9255330,\n",
       " 'date_item_shoptype_avg_item_cnt_lag_1': 9255330,\n",
       " 'date_item_shoptype_avg_item_cnt_lag_2': 9255330,\n",
       " 'date_item_shoptype_avg_item_cnt_lag_3': 9255330,\n",
       " 'date_item_shoptype_avg_item_cnt_lag_4': 9255330,\n",
       " 'date_item_shoptype_avg_item_cnt_lag_5': 9255330,\n",
       " 'date_shopitem_avg_item_cnt_lag_1': 9255330,\n",
       " 'date_shopitem_avg_item_cnt_lag_2': 9255330,\n",
       " 'date_shopitem_avg_item_cnt_lag_3': 9255330,\n",
       " 'date_shopitem_avg_item_cnt_lag_4': 9255330,\n",
       " 'date_shopitem_avg_item_cnt_lag_5': 9255330,\n",
       " 'delta_cnt_month_lag': 9255330,\n",
       " 'item_cnt_month_lag_1': 9255330,\n",
       " 'item_cnt_month_lag_2': 9255330,\n",
       " 'item_cnt_month_lag_3': 9255330,\n",
       " 'item_cnt_month_lag_4': 9255330,\n",
       " 'item_cnt_month_lag_5': 9255330,\n",
       " 'delta2_cnt_month_lag': 9255330}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(matrix.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"建模\"\"\"\n",
    "\n",
    "trainData = matrix[matrix['date_block_num'] < 33]\n",
    "label_train = trainData['item_cnt_month']\n",
    "X_train = trainData.drop('item_cnt_month', axis=1)\n",
    "\n",
    "validData = matrix[matrix['date_block_num'] == 33]\n",
    "label_valid = validData['item_cnt_month']\n",
    "X_valid = validData.drop('item_cnt_month', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: early_stop_rounds\n",
      "[LightGBM] [Warning] Unknown parameter: early_stop_rounds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.418241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13541\n",
      "[LightGBM] [Info] Number of data points in the train set: 8802958, number of used features: 80\n",
      "[LightGBM] [Warning] Unknown parameter: early_stop_rounds\n",
      "[LightGBM] [Info] Start training from score 0.296062\n",
      "[1]\ttraining's rmse: 1.21589\tvalid_1's rmse: 1.13306\n",
      "[2]\ttraining's rmse: 1.21108\tvalid_1's rmse: 1.12934\n",
      "[3]\ttraining's rmse: 1.20633\tvalid_1's rmse: 1.12595\n",
      "[4]\ttraining's rmse: 1.20112\tvalid_1's rmse: 1.122\n",
      "[5]\ttraining's rmse: 1.19603\tvalid_1's rmse: 1.11792\n",
      "[6]\ttraining's rmse: 1.19133\tvalid_1's rmse: 1.11441\n",
      "[7]\ttraining's rmse: 1.18671\tvalid_1's rmse: 1.11092\n",
      "[8]\ttraining's rmse: 1.18197\tvalid_1's rmse: 1.10721\n",
      "[9]\ttraining's rmse: 1.17757\tvalid_1's rmse: 1.10384\n",
      "[10]\ttraining's rmse: 1.17302\tvalid_1's rmse: 1.10028\n",
      "[11]\ttraining's rmse: 1.16833\tvalid_1's rmse: 1.09699\n",
      "[12]\ttraining's rmse: 1.16392\tvalid_1's rmse: 1.09406\n",
      "[13]\ttraining's rmse: 1.15935\tvalid_1's rmse: 1.09062\n",
      "[14]\ttraining's rmse: 1.15519\tvalid_1's rmse: 1.08745\n",
      "[15]\ttraining's rmse: 1.15097\tvalid_1's rmse: 1.08472\n",
      "[16]\ttraining's rmse: 1.14776\tvalid_1's rmse: 1.08233\n",
      "[17]\ttraining's rmse: 1.1431\tvalid_1's rmse: 1.07847\n",
      "[18]\ttraining's rmse: 1.13883\tvalid_1's rmse: 1.07538\n",
      "[19]\ttraining's rmse: 1.13489\tvalid_1's rmse: 1.07259\n",
      "[20]\ttraining's rmse: 1.13086\tvalid_1's rmse: 1.06968\n",
      "[21]\ttraining's rmse: 1.1271\tvalid_1's rmse: 1.0672\n",
      "[22]\ttraining's rmse: 1.12341\tvalid_1's rmse: 1.06467\n",
      "[23]\ttraining's rmse: 1.11981\tvalid_1's rmse: 1.06213\n",
      "[24]\ttraining's rmse: 1.11616\tvalid_1's rmse: 1.05932\n",
      "[25]\ttraining's rmse: 1.11272\tvalid_1's rmse: 1.05695\n",
      "[26]\ttraining's rmse: 1.10919\tvalid_1's rmse: 1.05464\n",
      "[27]\ttraining's rmse: 1.10557\tvalid_1's rmse: 1.05187\n",
      "[28]\ttraining's rmse: 1.10162\tvalid_1's rmse: 1.04877\n",
      "[29]\ttraining's rmse: 1.098\tvalid_1's rmse: 1.04598\n",
      "[30]\ttraining's rmse: 1.09433\tvalid_1's rmse: 1.04313\n",
      "[31]\ttraining's rmse: 1.09081\tvalid_1's rmse: 1.04077\n",
      "[32]\ttraining's rmse: 1.08718\tvalid_1's rmse: 1.03799\n",
      "[33]\ttraining's rmse: 1.08377\tvalid_1's rmse: 1.03557\n",
      "[34]\ttraining's rmse: 1.08074\tvalid_1's rmse: 1.03356\n",
      "[35]\ttraining's rmse: 1.07802\tvalid_1's rmse: 1.03165\n",
      "[36]\ttraining's rmse: 1.07415\tvalid_1's rmse: 1.02855\n",
      "[37]\ttraining's rmse: 1.07124\tvalid_1's rmse: 1.02653\n",
      "[38]\ttraining's rmse: 1.06829\tvalid_1's rmse: 1.0247\n",
      "[39]\ttraining's rmse: 1.06518\tvalid_1's rmse: 1.02275\n",
      "[40]\ttraining's rmse: 1.06229\tvalid_1's rmse: 1.02064\n",
      "[41]\ttraining's rmse: 1.05977\tvalid_1's rmse: 1.01897\n",
      "[42]\ttraining's rmse: 1.05714\tvalid_1's rmse: 1.01718\n",
      "[43]\ttraining's rmse: 1.05419\tvalid_1's rmse: 1.01513\n",
      "[44]\ttraining's rmse: 1.05162\tvalid_1's rmse: 1.01353\n",
      "[45]\ttraining's rmse: 1.04883\tvalid_1's rmse: 1.01171\n",
      "[46]\ttraining's rmse: 1.04612\tvalid_1's rmse: 1.0101\n",
      "[47]\ttraining's rmse: 1.04328\tvalid_1's rmse: 1.00808\n",
      "[48]\ttraining's rmse: 1.04079\tvalid_1's rmse: 1.00624\n",
      "[49]\ttraining's rmse: 1.03807\tvalid_1's rmse: 1.00429\n",
      "[50]\ttraining's rmse: 1.03557\tvalid_1's rmse: 1.00235\n",
      "[51]\ttraining's rmse: 1.03276\tvalid_1's rmse: 1.00072\n",
      "[52]\ttraining's rmse: 1.03033\tvalid_1's rmse: 0.999156\n",
      "[53]\ttraining's rmse: 1.02779\tvalid_1's rmse: 0.997572\n",
      "[54]\ttraining's rmse: 1.02517\tvalid_1's rmse: 0.995809\n",
      "[55]\ttraining's rmse: 1.02257\tvalid_1's rmse: 0.994219\n",
      "[56]\ttraining's rmse: 1.01996\tvalid_1's rmse: 0.992249\n",
      "[57]\ttraining's rmse: 1.01759\tvalid_1's rmse: 0.99092\n",
      "[58]\ttraining's rmse: 1.01522\tvalid_1's rmse: 0.989343\n",
      "[59]\ttraining's rmse: 1.01309\tvalid_1's rmse: 0.987879\n",
      "[60]\ttraining's rmse: 1.0106\tvalid_1's rmse: 0.986161\n",
      "[61]\ttraining's rmse: 1.00828\tvalid_1's rmse: 0.984593\n",
      "[62]\ttraining's rmse: 1.00564\tvalid_1's rmse: 0.982916\n",
      "[63]\ttraining's rmse: 1.00347\tvalid_1's rmse: 0.981401\n",
      "[64]\ttraining's rmse: 1.00087\tvalid_1's rmse: 0.980107\n",
      "[65]\ttraining's rmse: 0.998605\tvalid_1's rmse: 0.978839\n",
      "[66]\ttraining's rmse: 0.997095\tvalid_1's rmse: 0.977845\n",
      "[67]\ttraining's rmse: 0.994871\tvalid_1's rmse: 0.976326\n",
      "[68]\ttraining's rmse: 0.992698\tvalid_1's rmse: 0.974886\n",
      "[69]\ttraining's rmse: 0.990773\tvalid_1's rmse: 0.973539\n",
      "[70]\ttraining's rmse: 0.988927\tvalid_1's rmse: 0.972508\n",
      "[71]\ttraining's rmse: 0.98682\tvalid_1's rmse: 0.971627\n",
      "[72]\ttraining's rmse: 0.984764\tvalid_1's rmse: 0.970244\n",
      "[73]\ttraining's rmse: 0.982372\tvalid_1's rmse: 0.968569\n",
      "[74]\ttraining's rmse: 0.980519\tvalid_1's rmse: 0.967491\n",
      "[75]\ttraining's rmse: 0.978757\tvalid_1's rmse: 0.966296\n",
      "[76]\ttraining's rmse: 0.976975\tvalid_1's rmse: 0.965207\n",
      "[77]\ttraining's rmse: 0.974799\tvalid_1's rmse: 0.963828\n",
      "[78]\ttraining's rmse: 0.972943\tvalid_1's rmse: 0.962892\n",
      "[79]\ttraining's rmse: 0.971151\tvalid_1's rmse: 0.961746\n",
      "[80]\ttraining's rmse: 0.969486\tvalid_1's rmse: 0.960534\n",
      "[81]\ttraining's rmse: 0.967481\tvalid_1's rmse: 0.959423\n",
      "[82]\ttraining's rmse: 0.965915\tvalid_1's rmse: 0.958449\n",
      "[83]\ttraining's rmse: 0.964406\tvalid_1's rmse: 0.957571\n",
      "[84]\ttraining's rmse: 0.962502\tvalid_1's rmse: 0.95635\n",
      "[85]\ttraining's rmse: 0.960781\tvalid_1's rmse: 0.955217\n",
      "[86]\ttraining's rmse: 0.959406\tvalid_1's rmse: 0.95425\n",
      "[87]\ttraining's rmse: 0.95786\tvalid_1's rmse: 0.953291\n",
      "[88]\ttraining's rmse: 0.956276\tvalid_1's rmse: 0.952302\n",
      "[89]\ttraining's rmse: 0.954723\tvalid_1's rmse: 0.951384\n",
      "[90]\ttraining's rmse: 0.952838\tvalid_1's rmse: 0.950066\n",
      "[91]\ttraining's rmse: 0.951529\tvalid_1's rmse: 0.949406\n",
      "[92]\ttraining's rmse: 0.950085\tvalid_1's rmse: 0.948684\n",
      "[93]\ttraining's rmse: 0.948806\tvalid_1's rmse: 0.948034\n",
      "[94]\ttraining's rmse: 0.947262\tvalid_1's rmse: 0.947268\n",
      "[95]\ttraining's rmse: 0.945881\tvalid_1's rmse: 0.946409\n",
      "[96]\ttraining's rmse: 0.944533\tvalid_1's rmse: 0.945652\n",
      "[97]\ttraining's rmse: 0.943\tvalid_1's rmse: 0.944902\n",
      "[98]\ttraining's rmse: 0.941804\tvalid_1's rmse: 0.944274\n",
      "[99]\ttraining's rmse: 0.940487\tvalid_1's rmse: 0.943601\n",
      "[100]\ttraining's rmse: 0.938769\tvalid_1's rmse: 0.942477\n",
      "[101]\ttraining's rmse: 0.937039\tvalid_1's rmse: 0.941972\n",
      "[102]\ttraining's rmse: 0.935241\tvalid_1's rmse: 0.941559\n",
      "[103]\ttraining's rmse: 0.934113\tvalid_1's rmse: 0.941018\n",
      "[104]\ttraining's rmse: 0.932675\tvalid_1's rmse: 0.940257\n",
      "[105]\ttraining's rmse: 0.931443\tvalid_1's rmse: 0.939525\n",
      "[106]\ttraining's rmse: 0.930251\tvalid_1's rmse: 0.938881\n",
      "[107]\ttraining's rmse: 0.928744\tvalid_1's rmse: 0.937881\n",
      "[108]\ttraining's rmse: 0.927711\tvalid_1's rmse: 0.937366\n",
      "[109]\ttraining's rmse: 0.926626\tvalid_1's rmse: 0.936909\n",
      "[110]\ttraining's rmse: 0.92552\tvalid_1's rmse: 0.936377\n",
      "[111]\ttraining's rmse: 0.924381\tvalid_1's rmse: 0.935716\n",
      "[112]\ttraining's rmse: 0.923177\tvalid_1's rmse: 0.935203\n",
      "[113]\ttraining's rmse: 0.921597\tvalid_1's rmse: 0.934398\n",
      "[114]\ttraining's rmse: 0.920121\tvalid_1's rmse: 0.933493\n",
      "[115]\ttraining's rmse: 0.918772\tvalid_1's rmse: 0.9327\n",
      "[116]\ttraining's rmse: 0.917236\tvalid_1's rmse: 0.931715\n",
      "[117]\ttraining's rmse: 0.91631\tvalid_1's rmse: 0.931345\n",
      "[118]\ttraining's rmse: 0.915076\tvalid_1's rmse: 0.930766\n",
      "[119]\ttraining's rmse: 0.913582\tvalid_1's rmse: 0.930401\n",
      "[120]\ttraining's rmse: 0.912716\tvalid_1's rmse: 0.930049\n",
      "[121]\ttraining's rmse: 0.911671\tvalid_1's rmse: 0.929674\n",
      "[122]\ttraining's rmse: 0.910493\tvalid_1's rmse: 0.929227\n",
      "[123]\ttraining's rmse: 0.9094\tvalid_1's rmse: 0.928872\n",
      "[124]\ttraining's rmse: 0.908294\tvalid_1's rmse: 0.928226\n",
      "[125]\ttraining's rmse: 0.907443\tvalid_1's rmse: 0.927853\n",
      "[126]\ttraining's rmse: 0.906128\tvalid_1's rmse: 0.927073\n",
      "[127]\ttraining's rmse: 0.904904\tvalid_1's rmse: 0.92632\n",
      "[128]\ttraining's rmse: 0.90343\tvalid_1's rmse: 0.925562\n",
      "[129]\ttraining's rmse: 0.902563\tvalid_1's rmse: 0.925275\n",
      "[130]\ttraining's rmse: 0.901747\tvalid_1's rmse: 0.924922\n",
      "[131]\ttraining's rmse: 0.90109\tvalid_1's rmse: 0.924659\n",
      "[132]\ttraining's rmse: 0.899805\tvalid_1's rmse: 0.923903\n",
      "[133]\ttraining's rmse: 0.89897\tvalid_1's rmse: 0.923517\n",
      "[134]\ttraining's rmse: 0.897702\tvalid_1's rmse: 0.922911\n",
      "[135]\ttraining's rmse: 0.896905\tvalid_1's rmse: 0.922543\n",
      "[136]\ttraining's rmse: 0.896035\tvalid_1's rmse: 0.922252\n",
      "[137]\ttraining's rmse: 0.895087\tvalid_1's rmse: 0.921921\n",
      "[138]\ttraining's rmse: 0.894017\tvalid_1's rmse: 0.921368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139]\ttraining's rmse: 0.893292\tvalid_1's rmse: 0.921315\n",
      "[140]\ttraining's rmse: 0.892635\tvalid_1's rmse: 0.92105\n",
      "[141]\ttraining's rmse: 0.891913\tvalid_1's rmse: 0.920814\n",
      "[142]\ttraining's rmse: 0.891213\tvalid_1's rmse: 0.920577\n",
      "[143]\ttraining's rmse: 0.890417\tvalid_1's rmse: 0.920437\n",
      "[144]\ttraining's rmse: 0.889676\tvalid_1's rmse: 0.92022\n",
      "[145]\ttraining's rmse: 0.88854\tvalid_1's rmse: 0.920089\n",
      "[146]\ttraining's rmse: 0.887872\tvalid_1's rmse: 0.9199\n",
      "[147]\ttraining's rmse: 0.887203\tvalid_1's rmse: 0.91957\n",
      "[148]\ttraining's rmse: 0.886352\tvalid_1's rmse: 0.919307\n",
      "[149]\ttraining's rmse: 0.885253\tvalid_1's rmse: 0.91875\n",
      "[150]\ttraining's rmse: 0.884436\tvalid_1's rmse: 0.918598\n",
      "[151]\ttraining's rmse: 0.883433\tvalid_1's rmse: 0.918116\n",
      "[152]\ttraining's rmse: 0.88248\tvalid_1's rmse: 0.917707\n",
      "[153]\ttraining's rmse: 0.881492\tvalid_1's rmse: 0.917295\n",
      "[154]\ttraining's rmse: 0.880875\tvalid_1's rmse: 0.917147\n",
      "[155]\ttraining's rmse: 0.880192\tvalid_1's rmse: 0.91693\n",
      "[156]\ttraining's rmse: 0.87942\tvalid_1's rmse: 0.916694\n",
      "[157]\ttraining's rmse: 0.878788\tvalid_1's rmse: 0.916465\n",
      "[158]\ttraining's rmse: 0.878215\tvalid_1's rmse: 0.916364\n",
      "[159]\ttraining's rmse: 0.877277\tvalid_1's rmse: 0.916457\n",
      "[160]\ttraining's rmse: 0.87663\tvalid_1's rmse: 0.916268\n",
      "[161]\ttraining's rmse: 0.876056\tvalid_1's rmse: 0.91616\n",
      "[162]\ttraining's rmse: 0.875544\tvalid_1's rmse: 0.915956\n",
      "[163]\ttraining's rmse: 0.874972\tvalid_1's rmse: 0.915785\n",
      "[164]\ttraining's rmse: 0.874124\tvalid_1's rmse: 0.915427\n",
      "[165]\ttraining's rmse: 0.873558\tvalid_1's rmse: 0.915232\n",
      "[166]\ttraining's rmse: 0.872936\tvalid_1's rmse: 0.915241\n",
      "[167]\ttraining's rmse: 0.872035\tvalid_1's rmse: 0.914882\n",
      "[168]\ttraining's rmse: 0.871226\tvalid_1's rmse: 0.914428\n",
      "[169]\ttraining's rmse: 0.870818\tvalid_1's rmse: 0.914335\n",
      "[170]\ttraining's rmse: 0.870396\tvalid_1's rmse: 0.914227\n",
      "[171]\ttraining's rmse: 0.869774\tvalid_1's rmse: 0.913974\n",
      "[172]\ttraining's rmse: 0.868873\tvalid_1's rmse: 0.913518\n",
      "[173]\ttraining's rmse: 0.868428\tvalid_1's rmse: 0.91335\n",
      "[174]\ttraining's rmse: 0.867803\tvalid_1's rmse: 0.913157\n",
      "[175]\ttraining's rmse: 0.867369\tvalid_1's rmse: 0.913065\n",
      "[176]\ttraining's rmse: 0.866634\tvalid_1's rmse: 0.912832\n",
      "[177]\ttraining's rmse: 0.865813\tvalid_1's rmse: 0.912777\n",
      "[178]\ttraining's rmse: 0.865206\tvalid_1's rmse: 0.912645\n",
      "[179]\ttraining's rmse: 0.864813\tvalid_1's rmse: 0.912556\n",
      "[180]\ttraining's rmse: 0.864338\tvalid_1's rmse: 0.912487\n",
      "[181]\ttraining's rmse: 0.863954\tvalid_1's rmse: 0.912432\n",
      "[182]\ttraining's rmse: 0.863455\tvalid_1's rmse: 0.912398\n",
      "[183]\ttraining's rmse: 0.862974\tvalid_1's rmse: 0.912297\n",
      "[184]\ttraining's rmse: 0.862527\tvalid_1's rmse: 0.91216\n",
      "[185]\ttraining's rmse: 0.861815\tvalid_1's rmse: 0.912097\n",
      "[186]\ttraining's rmse: 0.861406\tvalid_1's rmse: 0.912107\n",
      "[187]\ttraining's rmse: 0.860809\tvalid_1's rmse: 0.911956\n",
      "[188]\ttraining's rmse: 0.860294\tvalid_1's rmse: 0.911815\n",
      "[189]\ttraining's rmse: 0.859857\tvalid_1's rmse: 0.911734\n",
      "[190]\ttraining's rmse: 0.859405\tvalid_1's rmse: 0.911719\n",
      "[191]\ttraining's rmse: 0.859017\tvalid_1's rmse: 0.911592\n",
      "[192]\ttraining's rmse: 0.858421\tvalid_1's rmse: 0.911376\n",
      "[193]\ttraining's rmse: 0.857965\tvalid_1's rmse: 0.911327\n",
      "[194]\ttraining's rmse: 0.857454\tvalid_1's rmse: 0.911268\n",
      "[195]\ttraining's rmse: 0.856754\tvalid_1's rmse: 0.911086\n",
      "[196]\ttraining's rmse: 0.856127\tvalid_1's rmse: 0.910854\n",
      "[197]\ttraining's rmse: 0.855693\tvalid_1's rmse: 0.910862\n",
      "[198]\ttraining's rmse: 0.85532\tvalid_1's rmse: 0.910549\n",
      "[199]\ttraining's rmse: 0.854864\tvalid_1's rmse: 0.910494\n",
      "[200]\ttraining's rmse: 0.854505\tvalid_1's rmse: 0.910478\n",
      "[201]\ttraining's rmse: 0.854146\tvalid_1's rmse: 0.910374\n",
      "[202]\ttraining's rmse: 0.853719\tvalid_1's rmse: 0.910215\n",
      "[203]\ttraining's rmse: 0.853157\tvalid_1's rmse: 0.910003\n",
      "[204]\ttraining's rmse: 0.852789\tvalid_1's rmse: 0.909934\n",
      "[205]\ttraining's rmse: 0.852143\tvalid_1's rmse: 0.909679\n",
      "[206]\ttraining's rmse: 0.851748\tvalid_1's rmse: 0.909678\n",
      "[207]\ttraining's rmse: 0.851041\tvalid_1's rmse: 0.909787\n",
      "[208]\ttraining's rmse: 0.850492\tvalid_1's rmse: 0.90966\n",
      "[209]\ttraining's rmse: 0.850084\tvalid_1's rmse: 0.909674\n",
      "[210]\ttraining's rmse: 0.849726\tvalid_1's rmse: 0.909688\n",
      "[211]\ttraining's rmse: 0.849372\tvalid_1's rmse: 0.909562\n",
      "[212]\ttraining's rmse: 0.849039\tvalid_1's rmse: 0.909498\n",
      "[213]\ttraining's rmse: 0.84871\tvalid_1's rmse: 0.909469\n",
      "[214]\ttraining's rmse: 0.848016\tvalid_1's rmse: 0.909185\n",
      "[215]\ttraining's rmse: 0.847674\tvalid_1's rmse: 0.909134\n",
      "[216]\ttraining's rmse: 0.847343\tvalid_1's rmse: 0.909099\n",
      "[217]\ttraining's rmse: 0.846896\tvalid_1's rmse: 0.908945\n",
      "[218]\ttraining's rmse: 0.846517\tvalid_1's rmse: 0.908926\n",
      "[219]\ttraining's rmse: 0.846041\tvalid_1's rmse: 0.908809\n",
      "[220]\ttraining's rmse: 0.845645\tvalid_1's rmse: 0.908752\n",
      "[221]\ttraining's rmse: 0.845392\tvalid_1's rmse: 0.90874\n",
      "[222]\ttraining's rmse: 0.844949\tvalid_1's rmse: 0.908725\n",
      "[223]\ttraining's rmse: 0.844537\tvalid_1's rmse: 0.908671\n",
      "[224]\ttraining's rmse: 0.844303\tvalid_1's rmse: 0.90863\n",
      "[225]\ttraining's rmse: 0.844025\tvalid_1's rmse: 0.908695\n",
      "[226]\ttraining's rmse: 0.843751\tvalid_1's rmse: 0.90868\n",
      "[227]\ttraining's rmse: 0.843312\tvalid_1's rmse: 0.90856\n",
      "[228]\ttraining's rmse: 0.842957\tvalid_1's rmse: 0.90887\n",
      "[229]\ttraining's rmse: 0.84251\tvalid_1's rmse: 0.908746\n",
      "[230]\ttraining's rmse: 0.842115\tvalid_1's rmse: 0.908683\n",
      "[231]\ttraining's rmse: 0.841738\tvalid_1's rmse: 0.908609\n",
      "[232]\ttraining's rmse: 0.841218\tvalid_1's rmse: 0.908491\n",
      "[233]\ttraining's rmse: 0.840895\tvalid_1's rmse: 0.908465\n",
      "[234]\ttraining's rmse: 0.840451\tvalid_1's rmse: 0.908267\n",
      "[235]\ttraining's rmse: 0.839779\tvalid_1's rmse: 0.908481\n",
      "[236]\ttraining's rmse: 0.839376\tvalid_1's rmse: 0.908353\n",
      "[237]\ttraining's rmse: 0.839125\tvalid_1's rmse: 0.908436\n",
      "[238]\ttraining's rmse: 0.838787\tvalid_1's rmse: 0.908345\n",
      "[239]\ttraining's rmse: 0.838556\tvalid_1's rmse: 0.908358\n",
      "[240]\ttraining's rmse: 0.838264\tvalid_1's rmse: 0.908407\n",
      "[241]\ttraining's rmse: 0.837841\tvalid_1's rmse: 0.908313\n",
      "[242]\ttraining's rmse: 0.837608\tvalid_1's rmse: 0.908297\n",
      "[243]\ttraining's rmse: 0.837155\tvalid_1's rmse: 0.908203\n",
      "[244]\ttraining's rmse: 0.836792\tvalid_1's rmse: 0.908007\n",
      "[245]\ttraining's rmse: 0.836439\tvalid_1's rmse: 0.907953\n",
      "[246]\ttraining's rmse: 0.836082\tvalid_1's rmse: 0.907946\n",
      "[247]\ttraining's rmse: 0.835831\tvalid_1's rmse: 0.907853\n",
      "[248]\ttraining's rmse: 0.835309\tvalid_1's rmse: 0.9077\n",
      "[249]\ttraining's rmse: 0.834994\tvalid_1's rmse: 0.907668\n",
      "[250]\ttraining's rmse: 0.83454\tvalid_1's rmse: 0.907532\n",
      "[251]\ttraining's rmse: 0.834171\tvalid_1's rmse: 0.907402\n",
      "[252]\ttraining's rmse: 0.833997\tvalid_1's rmse: 0.907442\n",
      "[253]\ttraining's rmse: 0.833709\tvalid_1's rmse: 0.907366\n",
      "[254]\ttraining's rmse: 0.833252\tvalid_1's rmse: 0.907322\n",
      "[255]\ttraining's rmse: 0.83288\tvalid_1's rmse: 0.907318\n",
      "[256]\ttraining's rmse: 0.832683\tvalid_1's rmse: 0.907289\n",
      "[257]\ttraining's rmse: 0.832253\tvalid_1's rmse: 0.907083\n",
      "[258]\ttraining's rmse: 0.832042\tvalid_1's rmse: 0.907099\n",
      "[259]\ttraining's rmse: 0.83186\tvalid_1's rmse: 0.907132\n",
      "[260]\ttraining's rmse: 0.831562\tvalid_1's rmse: 0.9072\n",
      "[261]\ttraining's rmse: 0.831315\tvalid_1's rmse: 0.907218\n",
      "[262]\ttraining's rmse: 0.830949\tvalid_1's rmse: 0.907114\n",
      "[263]\ttraining's rmse: 0.830474\tvalid_1's rmse: 0.907008\n",
      "[264]\ttraining's rmse: 0.830102\tvalid_1's rmse: 0.906872\n",
      "[265]\ttraining's rmse: 0.829527\tvalid_1's rmse: 0.907194\n",
      "[266]\ttraining's rmse: 0.829268\tvalid_1's rmse: 0.90715\n",
      "[267]\ttraining's rmse: 0.828995\tvalid_1's rmse: 0.907146\n",
      "[268]\ttraining's rmse: 0.828614\tvalid_1's rmse: 0.907097\n",
      "[269]\ttraining's rmse: 0.828341\tvalid_1's rmse: 0.907089\n",
      "[270]\ttraining's rmse: 0.827991\tvalid_1's rmse: 0.907088\n",
      "[271]\ttraining's rmse: 0.827809\tvalid_1's rmse: 0.907157\n",
      "[272]\ttraining's rmse: 0.827471\tvalid_1's rmse: 0.906999\n",
      "[273]\ttraining's rmse: 0.827215\tvalid_1's rmse: 0.907027\n",
      "[274]\ttraining's rmse: 0.826881\tvalid_1's rmse: 0.906924\n",
      "[275]\ttraining's rmse: 0.826656\tvalid_1's rmse: 0.906977\n",
      "[276]\ttraining's rmse: 0.82646\tvalid_1's rmse: 0.906997\n",
      "[277]\ttraining's rmse: 0.826133\tvalid_1's rmse: 0.906825\n",
      "[278]\ttraining's rmse: 0.825945\tvalid_1's rmse: 0.906822\n",
      "[279]\ttraining's rmse: 0.825643\tvalid_1's rmse: 0.906769\n",
      "[280]\ttraining's rmse: 0.825335\tvalid_1's rmse: 0.906722\n",
      "[281]\ttraining's rmse: 0.82514\tvalid_1's rmse: 0.906725\n",
      "[282]\ttraining's rmse: 0.824883\tvalid_1's rmse: 0.906817\n",
      "[283]\ttraining's rmse: 0.824623\tvalid_1's rmse: 0.906817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[284]\ttraining's rmse: 0.824361\tvalid_1's rmse: 0.906761\n",
      "[285]\ttraining's rmse: 0.823991\tvalid_1's rmse: 0.906659\n",
      "[286]\ttraining's rmse: 0.823539\tvalid_1's rmse: 0.906798\n",
      "[287]\ttraining's rmse: 0.823343\tvalid_1's rmse: 0.90677\n",
      "[288]\ttraining's rmse: 0.823029\tvalid_1's rmse: 0.906815\n",
      "[289]\ttraining's rmse: 0.822873\tvalid_1's rmse: 0.906784\n",
      "[290]\ttraining's rmse: 0.822574\tvalid_1's rmse: 0.906696\n",
      "[291]\ttraining's rmse: 0.822181\tvalid_1's rmse: 0.906833\n",
      "[292]\ttraining's rmse: 0.821881\tvalid_1's rmse: 0.90675\n",
      "[293]\ttraining's rmse: 0.82153\tvalid_1's rmse: 0.90709\n",
      "[294]\ttraining's rmse: 0.821175\tvalid_1's rmse: 0.907376\n",
      "[295]\ttraining's rmse: 0.820875\tvalid_1's rmse: 0.907388\n",
      "[296]\ttraining's rmse: 0.82068\tvalid_1's rmse: 0.907764\n",
      "[297]\ttraining's rmse: 0.820179\tvalid_1's rmse: 0.908041\n",
      "[298]\ttraining's rmse: 0.819896\tvalid_1's rmse: 0.90791\n",
      "[299]\ttraining's rmse: 0.819537\tvalid_1's rmse: 0.907823\n",
      "[300]\ttraining's rmse: 0.819356\tvalid_1's rmse: 0.907841\n",
      "[301]\ttraining's rmse: 0.819156\tvalid_1's rmse: 0.907877\n",
      "[302]\ttraining's rmse: 0.818991\tvalid_1's rmse: 0.90786\n",
      "[303]\ttraining's rmse: 0.81883\tvalid_1's rmse: 0.907868\n",
      "[304]\ttraining's rmse: 0.818525\tvalid_1's rmse: 0.907792\n",
      "[305]\ttraining's rmse: 0.81828\tvalid_1's rmse: 0.907574\n",
      "[306]\ttraining's rmse: 0.817981\tvalid_1's rmse: 0.907469\n",
      "[307]\ttraining's rmse: 0.817749\tvalid_1's rmse: 0.907401\n",
      "[308]\ttraining's rmse: 0.817378\tvalid_1's rmse: 0.907911\n",
      "[309]\ttraining's rmse: 0.817116\tvalid_1's rmse: 0.907871\n",
      "[310]\ttraining's rmse: 0.816835\tvalid_1's rmse: 0.907869\n",
      "[311]\ttraining's rmse: 0.816599\tvalid_1's rmse: 0.907759\n",
      "[312]\ttraining's rmse: 0.816259\tvalid_1's rmse: 0.907903\n",
      "[313]\ttraining's rmse: 0.816027\tvalid_1's rmse: 0.907682\n",
      "[314]\ttraining's rmse: 0.815755\tvalid_1's rmse: 0.907568\n",
      "[315]\ttraining's rmse: 0.815573\tvalid_1's rmse: 0.907557\n",
      "[316]\ttraining's rmse: 0.815316\tvalid_1's rmse: 0.907502\n",
      "[317]\ttraining's rmse: 0.815102\tvalid_1's rmse: 0.907518\n",
      "[318]\ttraining's rmse: 0.814822\tvalid_1's rmse: 0.907412\n",
      "[319]\ttraining's rmse: 0.814574\tvalid_1's rmse: 0.907294\n",
      "[320]\ttraining's rmse: 0.814383\tvalid_1's rmse: 0.907259\n",
      "[321]\ttraining's rmse: 0.814115\tvalid_1's rmse: 0.907192\n",
      "[322]\ttraining's rmse: 0.81392\tvalid_1's rmse: 0.907113\n",
      "[323]\ttraining's rmse: 0.813684\tvalid_1's rmse: 0.907061\n",
      "[324]\ttraining's rmse: 0.813473\tvalid_1's rmse: 0.907094\n",
      "[325]\ttraining's rmse: 0.813238\tvalid_1's rmse: 0.907039\n",
      "[326]\ttraining's rmse: 0.813098\tvalid_1's rmse: 0.907075\n",
      "[327]\ttraining's rmse: 0.812851\tvalid_1's rmse: 0.907125\n",
      "[328]\ttraining's rmse: 0.812719\tvalid_1's rmse: 0.90718\n",
      "[329]\ttraining's rmse: 0.812494\tvalid_1's rmse: 0.907187\n",
      "[330]\ttraining's rmse: 0.812253\tvalid_1's rmse: 0.907306\n",
      "[331]\ttraining's rmse: 0.811967\tvalid_1's rmse: 0.907331\n",
      "[332]\ttraining's rmse: 0.811818\tvalid_1's rmse: 0.907282\n",
      "[333]\ttraining's rmse: 0.811647\tvalid_1's rmse: 0.907278\n",
      "[334]\ttraining's rmse: 0.811464\tvalid_1's rmse: 0.907274\n",
      "[335]\ttraining's rmse: 0.811229\tvalid_1's rmse: 0.907263\n",
      "[336]\ttraining's rmse: 0.81097\tvalid_1's rmse: 0.907156\n",
      "[337]\ttraining's rmse: 0.810833\tvalid_1's rmse: 0.907135\n",
      "[338]\ttraining's rmse: 0.810719\tvalid_1's rmse: 0.907127\n",
      "[339]\ttraining's rmse: 0.810414\tvalid_1's rmse: 0.906934\n",
      "[340]\ttraining's rmse: 0.810242\tvalid_1's rmse: 0.906971\n",
      "[341]\ttraining's rmse: 0.810004\tvalid_1's rmse: 0.906932\n",
      "[342]\ttraining's rmse: 0.809729\tvalid_1's rmse: 0.906841\n",
      "[343]\ttraining's rmse: 0.809625\tvalid_1's rmse: 0.906808\n",
      "[344]\ttraining's rmse: 0.809476\tvalid_1's rmse: 0.906862\n",
      "[345]\ttraining's rmse: 0.809177\tvalid_1's rmse: 0.906723\n",
      "[346]\ttraining's rmse: 0.808834\tvalid_1's rmse: 0.906663\n",
      "[347]\ttraining's rmse: 0.808543\tvalid_1's rmse: 0.906945\n",
      "[348]\ttraining's rmse: 0.808404\tvalid_1's rmse: 0.90694\n",
      "[349]\ttraining's rmse: 0.808232\tvalid_1's rmse: 0.906887\n",
      "[350]\ttraining's rmse: 0.808107\tvalid_1's rmse: 0.906902\n",
      "[351]\ttraining's rmse: 0.807898\tvalid_1's rmse: 0.906834\n",
      "[352]\ttraining's rmse: 0.807798\tvalid_1's rmse: 0.90679\n",
      "[353]\ttraining's rmse: 0.807505\tvalid_1's rmse: 0.906695\n",
      "[354]\ttraining's rmse: 0.807335\tvalid_1's rmse: 0.906603\n",
      "[355]\ttraining's rmse: 0.807132\tvalid_1's rmse: 0.906564\n",
      "[356]\ttraining's rmse: 0.806975\tvalid_1's rmse: 0.906569\n",
      "[357]\ttraining's rmse: 0.806821\tvalid_1's rmse: 0.906643\n",
      "[358]\ttraining's rmse: 0.806621\tvalid_1's rmse: 0.906447\n",
      "[359]\ttraining's rmse: 0.806495\tvalid_1's rmse: 0.906418\n",
      "[360]\ttraining's rmse: 0.806313\tvalid_1's rmse: 0.906484\n",
      "[361]\ttraining's rmse: 0.806133\tvalid_1's rmse: 0.906475\n",
      "[362]\ttraining's rmse: 0.806008\tvalid_1's rmse: 0.906528\n",
      "[363]\ttraining's rmse: 0.805824\tvalid_1's rmse: 0.906535\n",
      "[364]\ttraining's rmse: 0.805637\tvalid_1's rmse: 0.906565\n",
      "[365]\ttraining's rmse: 0.805453\tvalid_1's rmse: 0.906536\n",
      "[366]\ttraining's rmse: 0.805343\tvalid_1's rmse: 0.906545\n",
      "[367]\ttraining's rmse: 0.805194\tvalid_1's rmse: 0.906457\n",
      "[368]\ttraining's rmse: 0.804928\tvalid_1's rmse: 0.906383\n",
      "[369]\ttraining's rmse: 0.804658\tvalid_1's rmse: 0.906402\n",
      "[370]\ttraining's rmse: 0.804529\tvalid_1's rmse: 0.906463\n",
      "[371]\ttraining's rmse: 0.804298\tvalid_1's rmse: 0.906377\n",
      "[372]\ttraining's rmse: 0.804096\tvalid_1's rmse: 0.906361\n",
      "[373]\ttraining's rmse: 0.80397\tvalid_1's rmse: 0.906462\n",
      "[374]\ttraining's rmse: 0.803799\tvalid_1's rmse: 0.906474\n",
      "[375]\ttraining's rmse: 0.803702\tvalid_1's rmse: 0.906498\n",
      "[376]\ttraining's rmse: 0.803464\tvalid_1's rmse: 0.906289\n",
      "[377]\ttraining's rmse: 0.803315\tvalid_1's rmse: 0.906289\n",
      "[378]\ttraining's rmse: 0.803216\tvalid_1's rmse: 0.906257\n",
      "[379]\ttraining's rmse: 0.803045\tvalid_1's rmse: 0.906251\n",
      "[380]\ttraining's rmse: 0.802846\tvalid_1's rmse: 0.906261\n",
      "[381]\ttraining's rmse: 0.802738\tvalid_1's rmse: 0.906296\n",
      "[382]\ttraining's rmse: 0.802557\tvalid_1's rmse: 0.906334\n",
      "[383]\ttraining's rmse: 0.802407\tvalid_1's rmse: 0.906426\n",
      "[384]\ttraining's rmse: 0.802273\tvalid_1's rmse: 0.906452\n",
      "[385]\ttraining's rmse: 0.802169\tvalid_1's rmse: 0.906479\n",
      "[386]\ttraining's rmse: 0.801834\tvalid_1's rmse: 0.906714\n",
      "[387]\ttraining's rmse: 0.801715\tvalid_1's rmse: 0.906616\n",
      "[388]\ttraining's rmse: 0.801467\tvalid_1's rmse: 0.906569\n",
      "[389]\ttraining's rmse: 0.801378\tvalid_1's rmse: 0.906654\n",
      "[390]\ttraining's rmse: 0.801175\tvalid_1's rmse: 0.906667\n",
      "[391]\ttraining's rmse: 0.801013\tvalid_1's rmse: 0.906538\n",
      "[392]\ttraining's rmse: 0.800819\tvalid_1's rmse: 0.906585\n",
      "[393]\ttraining's rmse: 0.800736\tvalid_1's rmse: 0.906597\n",
      "[394]\ttraining's rmse: 0.800556\tvalid_1's rmse: 0.906524\n",
      "[395]\ttraining's rmse: 0.800443\tvalid_1's rmse: 0.90651\n",
      "[396]\ttraining's rmse: 0.800286\tvalid_1's rmse: 0.90653\n",
      "[397]\ttraining's rmse: 0.800023\tvalid_1's rmse: 0.906377\n",
      "[398]\ttraining's rmse: 0.799887\tvalid_1's rmse: 0.906367\n",
      "[399]\ttraining's rmse: 0.799726\tvalid_1's rmse: 0.906421\n",
      "[400]\ttraining's rmse: 0.799604\tvalid_1's rmse: 0.906297\n",
      "[401]\ttraining's rmse: 0.799447\tvalid_1's rmse: 0.906267\n",
      "[402]\ttraining's rmse: 0.799354\tvalid_1's rmse: 0.906316\n",
      "[403]\ttraining's rmse: 0.799239\tvalid_1's rmse: 0.906323\n",
      "[404]\ttraining's rmse: 0.799151\tvalid_1's rmse: 0.90633\n",
      "[405]\ttraining's rmse: 0.798934\tvalid_1's rmse: 0.906317\n",
      "[406]\ttraining's rmse: 0.798866\tvalid_1's rmse: 0.906343\n",
      "[407]\ttraining's rmse: 0.798726\tvalid_1's rmse: 0.906389\n",
      "[408]\ttraining's rmse: 0.798503\tvalid_1's rmse: 0.906398\n",
      "[409]\ttraining's rmse: 0.798232\tvalid_1's rmse: 0.906089\n",
      "[410]\ttraining's rmse: 0.798079\tvalid_1's rmse: 0.906033\n",
      "[411]\ttraining's rmse: 0.797956\tvalid_1's rmse: 0.906041\n",
      "[412]\ttraining's rmse: 0.797742\tvalid_1's rmse: 0.906\n",
      "[413]\ttraining's rmse: 0.797601\tvalid_1's rmse: 0.905991\n",
      "[414]\ttraining's rmse: 0.797488\tvalid_1's rmse: 0.905983\n",
      "[415]\ttraining's rmse: 0.797286\tvalid_1's rmse: 0.905784\n",
      "[416]\ttraining's rmse: 0.797129\tvalid_1's rmse: 0.905828\n",
      "[417]\ttraining's rmse: 0.796987\tvalid_1's rmse: 0.90583\n",
      "[418]\ttraining's rmse: 0.796859\tvalid_1's rmse: 0.90582\n",
      "[419]\ttraining's rmse: 0.79677\tvalid_1's rmse: 0.905795\n",
      "[420]\ttraining's rmse: 0.796632\tvalid_1's rmse: 0.905783\n",
      "[421]\ttraining's rmse: 0.796562\tvalid_1's rmse: 0.905823\n",
      "[422]\ttraining's rmse: 0.796457\tvalid_1's rmse: 0.905783\n",
      "[423]\ttraining's rmse: 0.796285\tvalid_1's rmse: 0.905811\n",
      "[424]\ttraining's rmse: 0.796137\tvalid_1's rmse: 0.905789\n",
      "[425]\ttraining's rmse: 0.795954\tvalid_1's rmse: 0.90562\n",
      "[426]\ttraining's rmse: 0.795801\tvalid_1's rmse: 0.905571\n",
      "[427]\ttraining's rmse: 0.795639\tvalid_1's rmse: 0.905581\n",
      "[428]\ttraining's rmse: 0.795542\tvalid_1's rmse: 0.90565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[429]\ttraining's rmse: 0.795405\tvalid_1's rmse: 0.905635\n",
      "[430]\ttraining's rmse: 0.795327\tvalid_1's rmse: 0.905712\n",
      "[431]\ttraining's rmse: 0.795196\tvalid_1's rmse: 0.905724\n",
      "[432]\ttraining's rmse: 0.795112\tvalid_1's rmse: 0.905699\n",
      "[433]\ttraining's rmse: 0.794942\tvalid_1's rmse: 0.905632\n",
      "[434]\ttraining's rmse: 0.794779\tvalid_1's rmse: 0.905698\n",
      "[435]\ttraining's rmse: 0.794666\tvalid_1's rmse: 0.905715\n",
      "[436]\ttraining's rmse: 0.794486\tvalid_1's rmse: 0.90566\n",
      "[437]\ttraining's rmse: 0.794353\tvalid_1's rmse: 0.905633\n",
      "[438]\ttraining's rmse: 0.794213\tvalid_1's rmse: 0.905748\n",
      "[439]\ttraining's rmse: 0.794095\tvalid_1's rmse: 0.905744\n",
      "[440]\ttraining's rmse: 0.793986\tvalid_1's rmse: 0.905758\n",
      "[441]\ttraining's rmse: 0.793906\tvalid_1's rmse: 0.905754\n",
      "[442]\ttraining's rmse: 0.793794\tvalid_1's rmse: 0.905759\n",
      "[443]\ttraining's rmse: 0.793665\tvalid_1's rmse: 0.905703\n",
      "[444]\ttraining's rmse: 0.793555\tvalid_1's rmse: 0.905665\n",
      "[445]\ttraining's rmse: 0.793432\tvalid_1's rmse: 0.905683\n",
      "[446]\ttraining's rmse: 0.79323\tvalid_1's rmse: 0.905607\n",
      "[447]\ttraining's rmse: 0.793083\tvalid_1's rmse: 0.905662\n",
      "[448]\ttraining's rmse: 0.792944\tvalid_1's rmse: 0.905634\n",
      "[449]\ttraining's rmse: 0.792537\tvalid_1's rmse: 0.905955\n",
      "[450]\ttraining's rmse: 0.792447\tvalid_1's rmse: 0.905955\n",
      "[451]\ttraining's rmse: 0.792247\tvalid_1's rmse: 0.905915\n",
      "[452]\ttraining's rmse: 0.792075\tvalid_1's rmse: 0.90593\n",
      "[453]\ttraining's rmse: 0.791886\tvalid_1's rmse: 0.905979\n",
      "[454]\ttraining's rmse: 0.791805\tvalid_1's rmse: 0.905979\n",
      "[455]\ttraining's rmse: 0.791677\tvalid_1's rmse: 0.905984\n",
      "[456]\ttraining's rmse: 0.791569\tvalid_1's rmse: 0.906076\n",
      "[457]\ttraining's rmse: 0.791488\tvalid_1's rmse: 0.906118\n",
      "[458]\ttraining's rmse: 0.791372\tvalid_1's rmse: 0.90608\n",
      "[459]\ttraining's rmse: 0.791259\tvalid_1's rmse: 0.906132\n",
      "[460]\ttraining's rmse: 0.791188\tvalid_1's rmse: 0.906115\n",
      "[461]\ttraining's rmse: 0.791017\tvalid_1's rmse: 0.906031\n",
      "[462]\ttraining's rmse: 0.790866\tvalid_1's rmse: 0.906027\n",
      "[463]\ttraining's rmse: 0.790788\tvalid_1's rmse: 0.90603\n",
      "[464]\ttraining's rmse: 0.790669\tvalid_1's rmse: 0.905996\n",
      "[465]\ttraining's rmse: 0.790598\tvalid_1's rmse: 0.906043\n",
      "[466]\ttraining's rmse: 0.790453\tvalid_1's rmse: 0.906035\n",
      "[467]\ttraining's rmse: 0.790308\tvalid_1's rmse: 0.905961\n",
      "[468]\ttraining's rmse: 0.790203\tvalid_1's rmse: 0.905967\n",
      "[469]\ttraining's rmse: 0.789795\tvalid_1's rmse: 0.905874\n",
      "[470]\ttraining's rmse: 0.789653\tvalid_1's rmse: 0.905752\n",
      "[471]\ttraining's rmse: 0.789536\tvalid_1's rmse: 0.905677\n",
      "[472]\ttraining's rmse: 0.789412\tvalid_1's rmse: 0.905696\n",
      "[473]\ttraining's rmse: 0.789214\tvalid_1's rmse: 0.905647\n",
      "[474]\ttraining's rmse: 0.789081\tvalid_1's rmse: 0.905509\n",
      "[475]\ttraining's rmse: 0.788962\tvalid_1's rmse: 0.905615\n",
      "[476]\ttraining's rmse: 0.788832\tvalid_1's rmse: 0.905612\n",
      "[477]\ttraining's rmse: 0.788742\tvalid_1's rmse: 0.905641\n",
      "[478]\ttraining's rmse: 0.78866\tvalid_1's rmse: 0.905661\n",
      "[479]\ttraining's rmse: 0.788577\tvalid_1's rmse: 0.90565\n",
      "[480]\ttraining's rmse: 0.788492\tvalid_1's rmse: 0.905653\n",
      "[481]\ttraining's rmse: 0.788417\tvalid_1's rmse: 0.905654\n",
      "[482]\ttraining's rmse: 0.788251\tvalid_1's rmse: 0.905606\n",
      "[483]\ttraining's rmse: 0.788133\tvalid_1's rmse: 0.905633\n",
      "[484]\ttraining's rmse: 0.78798\tvalid_1's rmse: 0.905437\n",
      "[485]\ttraining's rmse: 0.78785\tvalid_1's rmse: 0.905392\n",
      "[486]\ttraining's rmse: 0.787748\tvalid_1's rmse: 0.905375\n",
      "[487]\ttraining's rmse: 0.787693\tvalid_1's rmse: 0.905386\n",
      "[488]\ttraining's rmse: 0.787602\tvalid_1's rmse: 0.905405\n",
      "[489]\ttraining's rmse: 0.787459\tvalid_1's rmse: 0.905378\n",
      "[490]\ttraining's rmse: 0.787349\tvalid_1's rmse: 0.905425\n",
      "[491]\ttraining's rmse: 0.787272\tvalid_1's rmse: 0.905445\n",
      "[492]\ttraining's rmse: 0.787168\tvalid_1's rmse: 0.905464\n",
      "[493]\ttraining's rmse: 0.787047\tvalid_1's rmse: 0.905426\n",
      "[494]\ttraining's rmse: 0.786841\tvalid_1's rmse: 0.905415\n",
      "[495]\ttraining's rmse: 0.786748\tvalid_1's rmse: 0.905396\n",
      "[496]\ttraining's rmse: 0.786641\tvalid_1's rmse: 0.905413\n",
      "[497]\ttraining's rmse: 0.786513\tvalid_1's rmse: 0.905401\n",
      "[498]\ttraining's rmse: 0.786368\tvalid_1's rmse: 0.905311\n",
      "[499]\ttraining's rmse: 0.786246\tvalid_1's rmse: 0.905327\n",
      "[500]\ttraining's rmse: 0.786177\tvalid_1's rmse: 0.905335\n",
      "[501]\ttraining's rmse: 0.786088\tvalid_1's rmse: 0.905367\n",
      "[502]\ttraining's rmse: 0.785998\tvalid_1's rmse: 0.905364\n",
      "[503]\ttraining's rmse: 0.785938\tvalid_1's rmse: 0.905392\n",
      "[504]\ttraining's rmse: 0.785814\tvalid_1's rmse: 0.905439\n",
      "[505]\ttraining's rmse: 0.785711\tvalid_1's rmse: 0.905408\n",
      "[506]\ttraining's rmse: 0.785634\tvalid_1's rmse: 0.90542\n",
      "[507]\ttraining's rmse: 0.785562\tvalid_1's rmse: 0.905416\n",
      "[508]\ttraining's rmse: 0.785484\tvalid_1's rmse: 0.905406\n",
      "[509]\ttraining's rmse: 0.78538\tvalid_1's rmse: 0.905422\n",
      "[510]\ttraining's rmse: 0.785292\tvalid_1's rmse: 0.905486\n",
      "[511]\ttraining's rmse: 0.785173\tvalid_1's rmse: 0.905522\n",
      "[512]\ttraining's rmse: 0.785104\tvalid_1's rmse: 0.905543\n",
      "[513]\ttraining's rmse: 0.785005\tvalid_1's rmse: 0.90557\n",
      "[514]\ttraining's rmse: 0.784924\tvalid_1's rmse: 0.90555\n",
      "[515]\ttraining's rmse: 0.784816\tvalid_1's rmse: 0.90557\n",
      "[516]\ttraining's rmse: 0.78475\tvalid_1's rmse: 0.905512\n",
      "[517]\ttraining's rmse: 0.78464\tvalid_1's rmse: 0.905542\n",
      "[518]\ttraining's rmse: 0.784499\tvalid_1's rmse: 0.905509\n",
      "[519]\ttraining's rmse: 0.784435\tvalid_1's rmse: 0.905506\n",
      "[520]\ttraining's rmse: 0.784112\tvalid_1's rmse: 0.9059\n",
      "[521]\ttraining's rmse: 0.783985\tvalid_1's rmse: 0.905719\n",
      "[522]\ttraining's rmse: 0.783889\tvalid_1's rmse: 0.905705\n",
      "[523]\ttraining's rmse: 0.783799\tvalid_1's rmse: 0.905606\n",
      "[524]\ttraining's rmse: 0.783728\tvalid_1's rmse: 0.905624\n",
      "[525]\ttraining's rmse: 0.783551\tvalid_1's rmse: 0.90556\n",
      "[526]\ttraining's rmse: 0.783439\tvalid_1's rmse: 0.905537\n",
      "[527]\ttraining's rmse: 0.783372\tvalid_1's rmse: 0.905538\n",
      "[528]\ttraining's rmse: 0.783279\tvalid_1's rmse: 0.905595\n",
      "[529]\ttraining's rmse: 0.783173\tvalid_1's rmse: 0.905638\n",
      "[530]\ttraining's rmse: 0.783089\tvalid_1's rmse: 0.905663\n",
      "[531]\ttraining's rmse: 0.783026\tvalid_1's rmse: 0.905651\n",
      "[532]\ttraining's rmse: 0.782947\tvalid_1's rmse: 0.905616\n",
      "[533]\ttraining's rmse: 0.782773\tvalid_1's rmse: 0.905535\n",
      "[534]\ttraining's rmse: 0.782669\tvalid_1's rmse: 0.905476\n",
      "[535]\ttraining's rmse: 0.782586\tvalid_1's rmse: 0.905493\n",
      "[536]\ttraining's rmse: 0.782521\tvalid_1's rmse: 0.90554\n",
      "[537]\ttraining's rmse: 0.782407\tvalid_1's rmse: 0.905566\n",
      "[538]\ttraining's rmse: 0.782295\tvalid_1's rmse: 0.905474\n",
      "[539]\ttraining's rmse: 0.78221\tvalid_1's rmse: 0.905465\n",
      "[540]\ttraining's rmse: 0.782147\tvalid_1's rmse: 0.905443\n",
      "[541]\ttraining's rmse: 0.781992\tvalid_1's rmse: 0.905512\n",
      "[542]\ttraining's rmse: 0.781876\tvalid_1's rmse: 0.905483\n",
      "[543]\ttraining's rmse: 0.781813\tvalid_1's rmse: 0.905451\n",
      "[544]\ttraining's rmse: 0.781713\tvalid_1's rmse: 0.905422\n",
      "[545]\ttraining's rmse: 0.781611\tvalid_1's rmse: 0.905432\n",
      "[546]\ttraining's rmse: 0.781547\tvalid_1's rmse: 0.905432\n",
      "[547]\ttraining's rmse: 0.781447\tvalid_1's rmse: 0.905406\n",
      "[548]\ttraining's rmse: 0.781396\tvalid_1's rmse: 0.905392\n",
      "[549]\ttraining's rmse: 0.781331\tvalid_1's rmse: 0.905394\n",
      "[550]\ttraining's rmse: 0.781269\tvalid_1's rmse: 0.905407\n",
      "[551]\ttraining's rmse: 0.781138\tvalid_1's rmse: 0.9054\n",
      "[552]\ttraining's rmse: 0.781027\tvalid_1's rmse: 0.905329\n",
      "[553]\ttraining's rmse: 0.780956\tvalid_1's rmse: 0.905361\n",
      "[554]\ttraining's rmse: 0.780867\tvalid_1's rmse: 0.905282\n",
      "[555]\ttraining's rmse: 0.780795\tvalid_1's rmse: 0.905317\n",
      "[556]\ttraining's rmse: 0.780669\tvalid_1's rmse: 0.905316\n",
      "[557]\ttraining's rmse: 0.780602\tvalid_1's rmse: 0.905299\n",
      "[558]\ttraining's rmse: 0.780557\tvalid_1's rmse: 0.905287\n",
      "[559]\ttraining's rmse: 0.780463\tvalid_1's rmse: 0.905304\n",
      "[560]\ttraining's rmse: 0.780393\tvalid_1's rmse: 0.905373\n",
      "[561]\ttraining's rmse: 0.780298\tvalid_1's rmse: 0.905404\n",
      "[562]\ttraining's rmse: 0.780244\tvalid_1's rmse: 0.905395\n",
      "[563]\ttraining's rmse: 0.780158\tvalid_1's rmse: 0.905321\n",
      "[564]\ttraining's rmse: 0.779984\tvalid_1's rmse: 0.905257\n",
      "[565]\ttraining's rmse: 0.779933\tvalid_1's rmse: 0.9052\n",
      "[566]\ttraining's rmse: 0.779858\tvalid_1's rmse: 0.90517\n",
      "[567]\ttraining's rmse: 0.779806\tvalid_1's rmse: 0.90517\n",
      "[568]\ttraining's rmse: 0.779581\tvalid_1's rmse: 0.905534\n",
      "[569]\ttraining's rmse: 0.779506\tvalid_1's rmse: 0.905545\n",
      "[570]\ttraining's rmse: 0.77945\tvalid_1's rmse: 0.905553\n",
      "[571]\ttraining's rmse: 0.779347\tvalid_1's rmse: 0.905459\n",
      "[572]\ttraining's rmse: 0.779289\tvalid_1's rmse: 0.905515\n",
      "[573]\ttraining's rmse: 0.779226\tvalid_1's rmse: 0.905477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[574]\ttraining's rmse: 0.77917\tvalid_1's rmse: 0.905437\n",
      "[575]\ttraining's rmse: 0.779069\tvalid_1's rmse: 0.90542\n",
      "[576]\ttraining's rmse: 0.778964\tvalid_1's rmse: 0.905395\n",
      "[577]\ttraining's rmse: 0.778838\tvalid_1's rmse: 0.905385\n",
      "[578]\ttraining's rmse: 0.778747\tvalid_1's rmse: 0.905385\n",
      "[579]\ttraining's rmse: 0.778692\tvalid_1's rmse: 0.905385\n",
      "[580]\ttraining's rmse: 0.778623\tvalid_1's rmse: 0.905371\n",
      "[581]\ttraining's rmse: 0.778554\tvalid_1's rmse: 0.905365\n",
      "[582]\ttraining's rmse: 0.778462\tvalid_1's rmse: 0.905359\n",
      "[583]\ttraining's rmse: 0.778383\tvalid_1's rmse: 0.905377\n",
      "[584]\ttraining's rmse: 0.778225\tvalid_1's rmse: 0.905263\n",
      "[585]\ttraining's rmse: 0.77815\tvalid_1's rmse: 0.905276\n",
      "[586]\ttraining's rmse: 0.778064\tvalid_1's rmse: 0.905256\n",
      "[587]\ttraining's rmse: 0.777971\tvalid_1's rmse: 0.905247\n",
      "[588]\ttraining's rmse: 0.777548\tvalid_1's rmse: 0.905808\n",
      "[589]\ttraining's rmse: 0.77741\tvalid_1's rmse: 0.905711\n",
      "[590]\ttraining's rmse: 0.777317\tvalid_1's rmse: 0.905715\n",
      "[591]\ttraining's rmse: 0.777261\tvalid_1's rmse: 0.905691\n",
      "[592]\ttraining's rmse: 0.777201\tvalid_1's rmse: 0.905619\n",
      "[593]\ttraining's rmse: 0.777136\tvalid_1's rmse: 0.905616\n",
      "[594]\ttraining's rmse: 0.777059\tvalid_1's rmse: 0.90559\n",
      "[595]\ttraining's rmse: 0.77698\tvalid_1's rmse: 0.905593\n",
      "[596]\ttraining's rmse: 0.776907\tvalid_1's rmse: 0.905588\n",
      "[597]\ttraining's rmse: 0.776781\tvalid_1's rmse: 0.905549\n",
      "[598]\ttraining's rmse: 0.776614\tvalid_1's rmse: 0.905486\n",
      "[599]\ttraining's rmse: 0.776536\tvalid_1's rmse: 0.905476\n",
      "[600]\ttraining's rmse: 0.776381\tvalid_1's rmse: 0.905399\n",
      "[601]\ttraining's rmse: 0.776298\tvalid_1's rmse: 0.905401\n",
      "[602]\ttraining's rmse: 0.776236\tvalid_1's rmse: 0.905359\n",
      "[603]\ttraining's rmse: 0.77612\tvalid_1's rmse: 0.905344\n",
      "[604]\ttraining's rmse: 0.77605\tvalid_1's rmse: 0.905398\n",
      "[605]\ttraining's rmse: 0.775987\tvalid_1's rmse: 0.905398\n",
      "[606]\ttraining's rmse: 0.775928\tvalid_1's rmse: 0.905411\n",
      "[607]\ttraining's rmse: 0.775861\tvalid_1's rmse: 0.905418\n",
      "[608]\ttraining's rmse: 0.775766\tvalid_1's rmse: 0.905426\n",
      "[609]\ttraining's rmse: 0.775692\tvalid_1's rmse: 0.905387\n",
      "[610]\ttraining's rmse: 0.775536\tvalid_1's rmse: 0.905345\n",
      "[611]\ttraining's rmse: 0.775469\tvalid_1's rmse: 0.905346\n",
      "[612]\ttraining's rmse: 0.775347\tvalid_1's rmse: 0.905233\n",
      "[613]\ttraining's rmse: 0.775264\tvalid_1's rmse: 0.905257\n",
      "[614]\ttraining's rmse: 0.775216\tvalid_1's rmse: 0.905257\n",
      "[615]\ttraining's rmse: 0.77513\tvalid_1's rmse: 0.905243\n",
      "[616]\ttraining's rmse: 0.775063\tvalid_1's rmse: 0.905275\n",
      "[617]\ttraining's rmse: 0.775009\tvalid_1's rmse: 0.905304\n",
      "[618]\ttraining's rmse: 0.774933\tvalid_1's rmse: 0.90526\n",
      "[619]\ttraining's rmse: 0.774865\tvalid_1's rmse: 0.905275\n",
      "[620]\ttraining's rmse: 0.774784\tvalid_1's rmse: 0.905321\n",
      "[621]\ttraining's rmse: 0.774565\tvalid_1's rmse: 0.905718\n",
      "[622]\ttraining's rmse: 0.774483\tvalid_1's rmse: 0.905733\n",
      "[623]\ttraining's rmse: 0.774387\tvalid_1's rmse: 0.905676\n",
      "[624]\ttraining's rmse: 0.774316\tvalid_1's rmse: 0.905693\n",
      "[625]\ttraining's rmse: 0.774261\tvalid_1's rmse: 0.905637\n",
      "[626]\ttraining's rmse: 0.774188\tvalid_1's rmse: 0.905672\n",
      "[627]\ttraining's rmse: 0.774123\tvalid_1's rmse: 0.905693\n",
      "[628]\ttraining's rmse: 0.774047\tvalid_1's rmse: 0.905685\n",
      "[629]\ttraining's rmse: 0.77396\tvalid_1's rmse: 0.905638\n",
      "[630]\ttraining's rmse: 0.773896\tvalid_1's rmse: 0.905636\n",
      "[631]\ttraining's rmse: 0.773597\tvalid_1's rmse: 0.905914\n",
      "[632]\ttraining's rmse: 0.773504\tvalid_1's rmse: 0.905865\n",
      "[633]\ttraining's rmse: 0.77345\tvalid_1's rmse: 0.905803\n",
      "[634]\ttraining's rmse: 0.77334\tvalid_1's rmse: 0.905744\n",
      "[635]\ttraining's rmse: 0.773259\tvalid_1's rmse: 0.905744\n",
      "[636]\ttraining's rmse: 0.773151\tvalid_1's rmse: 0.905686\n",
      "[637]\ttraining's rmse: 0.773076\tvalid_1's rmse: 0.905713\n",
      "[638]\ttraining's rmse: 0.772994\tvalid_1's rmse: 0.90571\n",
      "[639]\ttraining's rmse: 0.772907\tvalid_1's rmse: 0.905717\n",
      "[640]\ttraining's rmse: 0.772843\tvalid_1's rmse: 0.905736\n",
      "[641]\ttraining's rmse: 0.772796\tvalid_1's rmse: 0.905731\n",
      "[642]\ttraining's rmse: 0.772464\tvalid_1's rmse: 0.905996\n",
      "[643]\ttraining's rmse: 0.772397\tvalid_1's rmse: 0.90597\n",
      "[644]\ttraining's rmse: 0.772343\tvalid_1's rmse: 0.906036\n",
      "[645]\ttraining's rmse: 0.772258\tvalid_1's rmse: 0.905921\n",
      "[646]\ttraining's rmse: 0.772121\tvalid_1's rmse: 0.905834\n",
      "[647]\ttraining's rmse: 0.772058\tvalid_1's rmse: 0.905854\n",
      "[648]\ttraining's rmse: 0.771972\tvalid_1's rmse: 0.905854\n",
      "[649]\ttraining's rmse: 0.77192\tvalid_1's rmse: 0.905841\n",
      "[650]\ttraining's rmse: 0.77186\tvalid_1's rmse: 0.905817\n",
      "[651]\ttraining's rmse: 0.771747\tvalid_1's rmse: 0.905743\n",
      "[652]\ttraining's rmse: 0.771699\tvalid_1's rmse: 0.905723\n",
      "[653]\ttraining's rmse: 0.771643\tvalid_1's rmse: 0.905691\n",
      "[654]\ttraining's rmse: 0.771557\tvalid_1's rmse: 0.905682\n",
      "[655]\ttraining's rmse: 0.771504\tvalid_1's rmse: 0.905672\n",
      "[656]\ttraining's rmse: 0.771416\tvalid_1's rmse: 0.905613\n",
      "[657]\ttraining's rmse: 0.771323\tvalid_1's rmse: 0.905599\n",
      "[658]\ttraining's rmse: 0.771244\tvalid_1's rmse: 0.905594\n",
      "[659]\ttraining's rmse: 0.771175\tvalid_1's rmse: 0.905554\n",
      "[660]\ttraining's rmse: 0.771128\tvalid_1's rmse: 0.905553\n",
      "[661]\ttraining's rmse: 0.771069\tvalid_1's rmse: 0.905633\n",
      "[662]\ttraining's rmse: 0.770954\tvalid_1's rmse: 0.905566\n",
      "[663]\ttraining's rmse: 0.770885\tvalid_1's rmse: 0.905592\n",
      "[664]\ttraining's rmse: 0.770787\tvalid_1's rmse: 0.905524\n",
      "[665]\ttraining's rmse: 0.770689\tvalid_1's rmse: 0.905467\n",
      "[666]\ttraining's rmse: 0.770635\tvalid_1's rmse: 0.905506\n",
      "[667]\ttraining's rmse: 0.77056\tvalid_1's rmse: 0.905492\n",
      "[668]\ttraining's rmse: 0.770427\tvalid_1's rmse: 0.905429\n",
      "[669]\ttraining's rmse: 0.770349\tvalid_1's rmse: 0.905415\n",
      "[670]\ttraining's rmse: 0.770284\tvalid_1's rmse: 0.905427\n",
      "[671]\ttraining's rmse: 0.770247\tvalid_1's rmse: 0.90542\n",
      "[672]\ttraining's rmse: 0.770132\tvalid_1's rmse: 0.905253\n",
      "[673]\ttraining's rmse: 0.769852\tvalid_1's rmse: 0.905249\n",
      "[674]\ttraining's rmse: 0.769773\tvalid_1's rmse: 0.905248\n",
      "[675]\ttraining's rmse: 0.769689\tvalid_1's rmse: 0.905305\n",
      "[676]\ttraining's rmse: 0.76959\tvalid_1's rmse: 0.905237\n",
      "[677]\ttraining's rmse: 0.769504\tvalid_1's rmse: 0.905155\n",
      "[678]\ttraining's rmse: 0.769461\tvalid_1's rmse: 0.905158\n",
      "[679]\ttraining's rmse: 0.769421\tvalid_1's rmse: 0.905158\n",
      "[680]\ttraining's rmse: 0.769355\tvalid_1's rmse: 0.905109\n",
      "[681]\ttraining's rmse: 0.769217\tvalid_1's rmse: 0.905045\n",
      "[682]\ttraining's rmse: 0.769175\tvalid_1's rmse: 0.905037\n",
      "[683]\ttraining's rmse: 0.769106\tvalid_1's rmse: 0.905042\n",
      "[684]\ttraining's rmse: 0.76894\tvalid_1's rmse: 0.905276\n",
      "[685]\ttraining's rmse: 0.768895\tvalid_1's rmse: 0.905267\n",
      "[686]\ttraining's rmse: 0.768831\tvalid_1's rmse: 0.905256\n",
      "[687]\ttraining's rmse: 0.768789\tvalid_1's rmse: 0.905248\n",
      "[688]\ttraining's rmse: 0.768713\tvalid_1's rmse: 0.905255\n",
      "[689]\ttraining's rmse: 0.768648\tvalid_1's rmse: 0.905238\n",
      "[690]\ttraining's rmse: 0.768586\tvalid_1's rmse: 0.905219\n",
      "[691]\ttraining's rmse: 0.768526\tvalid_1's rmse: 0.905235\n",
      "[692]\ttraining's rmse: 0.76847\tvalid_1's rmse: 0.905266\n",
      "[693]\ttraining's rmse: 0.768394\tvalid_1's rmse: 0.905249\n",
      "[694]\ttraining's rmse: 0.768324\tvalid_1's rmse: 0.905263\n",
      "[695]\ttraining's rmse: 0.768257\tvalid_1's rmse: 0.905251\n",
      "[696]\ttraining's rmse: 0.768196\tvalid_1's rmse: 0.905281\n",
      "[697]\ttraining's rmse: 0.768126\tvalid_1's rmse: 0.905322\n",
      "[698]\ttraining's rmse: 0.768086\tvalid_1's rmse: 0.905325\n",
      "[699]\ttraining's rmse: 0.768011\tvalid_1's rmse: 0.905349\n",
      "[700]\ttraining's rmse: 0.767934\tvalid_1's rmse: 0.905331\n",
      "[701]\ttraining's rmse: 0.767873\tvalid_1's rmse: 0.905324\n",
      "[702]\ttraining's rmse: 0.76779\tvalid_1's rmse: 0.905224\n",
      "[703]\ttraining's rmse: 0.767743\tvalid_1's rmse: 0.905203\n",
      "[704]\ttraining's rmse: 0.767676\tvalid_1's rmse: 0.905237\n",
      "[705]\ttraining's rmse: 0.767574\tvalid_1's rmse: 0.905221\n",
      "[706]\ttraining's rmse: 0.767505\tvalid_1's rmse: 0.905192\n",
      "[707]\ttraining's rmse: 0.767461\tvalid_1's rmse: 0.905101\n",
      "[708]\ttraining's rmse: 0.767389\tvalid_1's rmse: 0.905094\n",
      "[709]\ttraining's rmse: 0.767213\tvalid_1's rmse: 0.905494\n",
      "[710]\ttraining's rmse: 0.767166\tvalid_1's rmse: 0.905498\n",
      "[711]\ttraining's rmse: 0.767095\tvalid_1's rmse: 0.905472\n",
      "[712]\ttraining's rmse: 0.767038\tvalid_1's rmse: 0.90538\n",
      "[713]\ttraining's rmse: 0.766974\tvalid_1's rmse: 0.90535\n",
      "[714]\ttraining's rmse: 0.766891\tvalid_1's rmse: 0.905311\n",
      "[715]\ttraining's rmse: 0.766816\tvalid_1's rmse: 0.905257\n",
      "[716]\ttraining's rmse: 0.766779\tvalid_1's rmse: 0.905255\n",
      "[717]\ttraining's rmse: 0.766686\tvalid_1's rmse: 0.90508\n",
      "[718]\ttraining's rmse: 0.766628\tvalid_1's rmse: 0.905083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[719]\ttraining's rmse: 0.766573\tvalid_1's rmse: 0.905098\n",
      "[720]\ttraining's rmse: 0.766518\tvalid_1's rmse: 0.905097\n",
      "[721]\ttraining's rmse: 0.766464\tvalid_1's rmse: 0.905098\n",
      "[722]\ttraining's rmse: 0.766414\tvalid_1's rmse: 0.905092\n",
      "[723]\ttraining's rmse: 0.766365\tvalid_1's rmse: 0.905106\n",
      "[724]\ttraining's rmse: 0.766298\tvalid_1's rmse: 0.905076\n",
      "[725]\ttraining's rmse: 0.766217\tvalid_1's rmse: 0.905067\n",
      "[726]\ttraining's rmse: 0.766168\tvalid_1's rmse: 0.905044\n",
      "[727]\ttraining's rmse: 0.766119\tvalid_1's rmse: 0.905061\n",
      "[728]\ttraining's rmse: 0.766054\tvalid_1's rmse: 0.905098\n",
      "[729]\ttraining's rmse: 0.766\tvalid_1's rmse: 0.905102\n",
      "[730]\ttraining's rmse: 0.765927\tvalid_1's rmse: 0.905108\n",
      "[731]\ttraining's rmse: 0.765866\tvalid_1's rmse: 0.905096\n",
      "[732]\ttraining's rmse: 0.765815\tvalid_1's rmse: 0.905104\n",
      "[733]\ttraining's rmse: 0.765776\tvalid_1's rmse: 0.905113\n",
      "[734]\ttraining's rmse: 0.765723\tvalid_1's rmse: 0.905111\n",
      "[735]\ttraining's rmse: 0.765657\tvalid_1's rmse: 0.905125\n",
      "[736]\ttraining's rmse: 0.76562\tvalid_1's rmse: 0.905158\n",
      "[737]\ttraining's rmse: 0.76557\tvalid_1's rmse: 0.905183\n",
      "[738]\ttraining's rmse: 0.765499\tvalid_1's rmse: 0.905132\n",
      "[739]\ttraining's rmse: 0.765458\tvalid_1's rmse: 0.90509\n",
      "[740]\ttraining's rmse: 0.765405\tvalid_1's rmse: 0.905082\n",
      "[741]\ttraining's rmse: 0.765322\tvalid_1's rmse: 0.904952\n",
      "[742]\ttraining's rmse: 0.765264\tvalid_1's rmse: 0.904946\n",
      "[743]\ttraining's rmse: 0.76519\tvalid_1's rmse: 0.904885\n",
      "[744]\ttraining's rmse: 0.765148\tvalid_1's rmse: 0.904828\n",
      "[745]\ttraining's rmse: 0.765104\tvalid_1's rmse: 0.904798\n",
      "[746]\ttraining's rmse: 0.764977\tvalid_1's rmse: 0.90479\n",
      "[747]\ttraining's rmse: 0.764931\tvalid_1's rmse: 0.904834\n",
      "[748]\ttraining's rmse: 0.764847\tvalid_1's rmse: 0.904777\n",
      "[749]\ttraining's rmse: 0.764782\tvalid_1's rmse: 0.904789\n",
      "[750]\ttraining's rmse: 0.764727\tvalid_1's rmse: 0.904778\n",
      "[751]\ttraining's rmse: 0.764637\tvalid_1's rmse: 0.904735\n",
      "[752]\ttraining's rmse: 0.764571\tvalid_1's rmse: 0.904737\n",
      "[753]\ttraining's rmse: 0.764451\tvalid_1's rmse: 0.904804\n",
      "[754]\ttraining's rmse: 0.764401\tvalid_1's rmse: 0.904749\n",
      "[755]\ttraining's rmse: 0.764345\tvalid_1's rmse: 0.904725\n",
      "[756]\ttraining's rmse: 0.764299\tvalid_1's rmse: 0.904725\n",
      "[757]\ttraining's rmse: 0.764241\tvalid_1's rmse: 0.90471\n",
      "[758]\ttraining's rmse: 0.76419\tvalid_1's rmse: 0.904677\n",
      "[759]\ttraining's rmse: 0.764085\tvalid_1's rmse: 0.904612\n",
      "[760]\ttraining's rmse: 0.764017\tvalid_1's rmse: 0.904611\n",
      "[761]\ttraining's rmse: 0.763954\tvalid_1's rmse: 0.904595\n",
      "[762]\ttraining's rmse: 0.763895\tvalid_1's rmse: 0.904536\n",
      "[763]\ttraining's rmse: 0.763826\tvalid_1's rmse: 0.90454\n",
      "[764]\ttraining's rmse: 0.76379\tvalid_1's rmse: 0.904547\n",
      "[765]\ttraining's rmse: 0.763606\tvalid_1's rmse: 0.904492\n",
      "[766]\ttraining's rmse: 0.763547\tvalid_1's rmse: 0.904451\n",
      "[767]\ttraining's rmse: 0.763492\tvalid_1's rmse: 0.904455\n",
      "[768]\ttraining's rmse: 0.763416\tvalid_1's rmse: 0.904422\n",
      "[769]\ttraining's rmse: 0.763365\tvalid_1's rmse: 0.904381\n",
      "[770]\ttraining's rmse: 0.76332\tvalid_1's rmse: 0.904378\n",
      "[771]\ttraining's rmse: 0.763256\tvalid_1's rmse: 0.904392\n",
      "[772]\ttraining's rmse: 0.76321\tvalid_1's rmse: 0.904369\n",
      "[773]\ttraining's rmse: 0.76317\tvalid_1's rmse: 0.904334\n",
      "[774]\ttraining's rmse: 0.763111\tvalid_1's rmse: 0.904324\n",
      "[775]\ttraining's rmse: 0.76306\tvalid_1's rmse: 0.904287\n",
      "[776]\ttraining's rmse: 0.763009\tvalid_1's rmse: 0.904275\n",
      "[777]\ttraining's rmse: 0.762946\tvalid_1's rmse: 0.904309\n",
      "[778]\ttraining's rmse: 0.762914\tvalid_1's rmse: 0.904303\n",
      "[779]\ttraining's rmse: 0.762879\tvalid_1's rmse: 0.904315\n",
      "[780]\ttraining's rmse: 0.762836\tvalid_1's rmse: 0.904288\n",
      "[781]\ttraining's rmse: 0.762771\tvalid_1's rmse: 0.904286\n",
      "[782]\ttraining's rmse: 0.76273\tvalid_1's rmse: 0.904249\n",
      "[783]\ttraining's rmse: 0.76269\tvalid_1's rmse: 0.904219\n",
      "[784]\ttraining's rmse: 0.762635\tvalid_1's rmse: 0.904204\n",
      "[785]\ttraining's rmse: 0.762557\tvalid_1's rmse: 0.904143\n",
      "[786]\ttraining's rmse: 0.762497\tvalid_1's rmse: 0.904131\n",
      "[787]\ttraining's rmse: 0.762444\tvalid_1's rmse: 0.904135\n",
      "[788]\ttraining's rmse: 0.762381\tvalid_1's rmse: 0.904096\n",
      "[789]\ttraining's rmse: 0.762308\tvalid_1's rmse: 0.904159\n",
      "[790]\ttraining's rmse: 0.762248\tvalid_1's rmse: 0.904166\n",
      "[791]\ttraining's rmse: 0.762174\tvalid_1's rmse: 0.904139\n",
      "[792]\ttraining's rmse: 0.761978\tvalid_1's rmse: 0.904076\n",
      "[793]\ttraining's rmse: 0.761939\tvalid_1's rmse: 0.90403\n",
      "[794]\ttraining's rmse: 0.761887\tvalid_1's rmse: 0.903922\n",
      "[795]\ttraining's rmse: 0.761841\tvalid_1's rmse: 0.903922\n",
      "[796]\ttraining's rmse: 0.761802\tvalid_1's rmse: 0.90394\n",
      "[797]\ttraining's rmse: 0.761746\tvalid_1's rmse: 0.903948\n",
      "[798]\ttraining's rmse: 0.761693\tvalid_1's rmse: 0.903908\n",
      "[799]\ttraining's rmse: 0.761654\tvalid_1's rmse: 0.903886\n",
      "[800]\ttraining's rmse: 0.761606\tvalid_1's rmse: 0.903907\n",
      "[801]\ttraining's rmse: 0.761544\tvalid_1's rmse: 0.903865\n",
      "[802]\ttraining's rmse: 0.761501\tvalid_1's rmse: 0.903854\n",
      "[803]\ttraining's rmse: 0.761426\tvalid_1's rmse: 0.903846\n",
      "[804]\ttraining's rmse: 0.761394\tvalid_1's rmse: 0.903853\n",
      "[805]\ttraining's rmse: 0.761317\tvalid_1's rmse: 0.903786\n",
      "[806]\ttraining's rmse: 0.761268\tvalid_1's rmse: 0.903793\n",
      "[807]\ttraining's rmse: 0.761175\tvalid_1's rmse: 0.90377\n",
      "[808]\ttraining's rmse: 0.761091\tvalid_1's rmse: 0.903721\n",
      "[809]\ttraining's rmse: 0.761013\tvalid_1's rmse: 0.903682\n",
      "[810]\ttraining's rmse: 0.760974\tvalid_1's rmse: 0.903673\n",
      "[811]\ttraining's rmse: 0.760922\tvalid_1's rmse: 0.903678\n",
      "[812]\ttraining's rmse: 0.760883\tvalid_1's rmse: 0.903647\n",
      "[813]\ttraining's rmse: 0.760847\tvalid_1's rmse: 0.903647\n",
      "[814]\ttraining's rmse: 0.760804\tvalid_1's rmse: 0.903654\n",
      "[815]\ttraining's rmse: 0.760754\tvalid_1's rmse: 0.903743\n",
      "[816]\ttraining's rmse: 0.760716\tvalid_1's rmse: 0.903749\n",
      "[817]\ttraining's rmse: 0.760677\tvalid_1's rmse: 0.903677\n",
      "[818]\ttraining's rmse: 0.760612\tvalid_1's rmse: 0.903641\n",
      "[819]\ttraining's rmse: 0.760582\tvalid_1's rmse: 0.903638\n",
      "[820]\ttraining's rmse: 0.76052\tvalid_1's rmse: 0.903662\n",
      "[821]\ttraining's rmse: 0.760448\tvalid_1's rmse: 0.903597\n",
      "[822]\ttraining's rmse: 0.760389\tvalid_1's rmse: 0.903573\n",
      "[823]\ttraining's rmse: 0.760333\tvalid_1's rmse: 0.903536\n",
      "[824]\ttraining's rmse: 0.7603\tvalid_1's rmse: 0.903525\n",
      "[825]\ttraining's rmse: 0.76026\tvalid_1's rmse: 0.90353\n",
      "[826]\ttraining's rmse: 0.760202\tvalid_1's rmse: 0.903524\n",
      "[827]\ttraining's rmse: 0.760145\tvalid_1's rmse: 0.903453\n",
      "[828]\ttraining's rmse: 0.760092\tvalid_1's rmse: 0.903422\n",
      "[829]\ttraining's rmse: 0.760037\tvalid_1's rmse: 0.903357\n",
      "[830]\ttraining's rmse: 0.759997\tvalid_1's rmse: 0.903358\n",
      "[831]\ttraining's rmse: 0.759847\tvalid_1's rmse: 0.903356\n",
      "[832]\ttraining's rmse: 0.759795\tvalid_1's rmse: 0.903313\n",
      "[833]\ttraining's rmse: 0.759729\tvalid_1's rmse: 0.903269\n",
      "[834]\ttraining's rmse: 0.759642\tvalid_1's rmse: 0.903195\n",
      "[835]\ttraining's rmse: 0.759552\tvalid_1's rmse: 0.903106\n",
      "[836]\ttraining's rmse: 0.759494\tvalid_1's rmse: 0.903095\n",
      "[837]\ttraining's rmse: 0.759342\tvalid_1's rmse: 0.90325\n",
      "[838]\ttraining's rmse: 0.759306\tvalid_1's rmse: 0.903259\n",
      "[839]\ttraining's rmse: 0.759229\tvalid_1's rmse: 0.903437\n",
      "[840]\ttraining's rmse: 0.759181\tvalid_1's rmse: 0.903427\n",
      "[841]\ttraining's rmse: 0.759125\tvalid_1's rmse: 0.903396\n",
      "[842]\ttraining's rmse: 0.759085\tvalid_1's rmse: 0.903385\n",
      "[843]\ttraining's rmse: 0.759001\tvalid_1's rmse: 0.903357\n",
      "[844]\ttraining's rmse: 0.758958\tvalid_1's rmse: 0.9034\n",
      "[845]\ttraining's rmse: 0.758875\tvalid_1's rmse: 0.90337\n",
      "[846]\ttraining's rmse: 0.758798\tvalid_1's rmse: 0.90333\n",
      "[847]\ttraining's rmse: 0.758758\tvalid_1's rmse: 0.903284\n",
      "[848]\ttraining's rmse: 0.758719\tvalid_1's rmse: 0.903277\n",
      "[849]\ttraining's rmse: 0.758649\tvalid_1's rmse: 0.903203\n",
      "[850]\ttraining's rmse: 0.758606\tvalid_1's rmse: 0.903189\n",
      "[851]\ttraining's rmse: 0.758541\tvalid_1's rmse: 0.903165\n",
      "[852]\ttraining's rmse: 0.758498\tvalid_1's rmse: 0.903136\n",
      "[853]\ttraining's rmse: 0.758445\tvalid_1's rmse: 0.903161\n",
      "[854]\ttraining's rmse: 0.758381\tvalid_1's rmse: 0.903153\n",
      "[855]\ttraining's rmse: 0.75831\tvalid_1's rmse: 0.903101\n",
      "[856]\ttraining's rmse: 0.758201\tvalid_1's rmse: 0.903042\n",
      "[857]\ttraining's rmse: 0.758158\tvalid_1's rmse: 0.903017\n",
      "[858]\ttraining's rmse: 0.758116\tvalid_1's rmse: 0.902986\n",
      "[859]\ttraining's rmse: 0.758076\tvalid_1's rmse: 0.902995\n",
      "[860]\ttraining's rmse: 0.758041\tvalid_1's rmse: 0.902957\n",
      "[861]\ttraining's rmse: 0.757982\tvalid_1's rmse: 0.902921\n",
      "[862]\ttraining's rmse: 0.757911\tvalid_1's rmse: 0.902907\n",
      "[863]\ttraining's rmse: 0.757842\tvalid_1's rmse: 0.902847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[864]\ttraining's rmse: 0.757774\tvalid_1's rmse: 0.902837\n",
      "[865]\ttraining's rmse: 0.757726\tvalid_1's rmse: 0.902839\n",
      "[866]\ttraining's rmse: 0.757674\tvalid_1's rmse: 0.902764\n",
      "[867]\ttraining's rmse: 0.757619\tvalid_1's rmse: 0.902759\n",
      "[868]\ttraining's rmse: 0.757569\tvalid_1's rmse: 0.90278\n",
      "[869]\ttraining's rmse: 0.757515\tvalid_1's rmse: 0.902745\n",
      "[870]\ttraining's rmse: 0.757474\tvalid_1's rmse: 0.902712\n",
      "[871]\ttraining's rmse: 0.75744\tvalid_1's rmse: 0.902704\n",
      "[872]\ttraining's rmse: 0.757394\tvalid_1's rmse: 0.902691\n",
      "[873]\ttraining's rmse: 0.757341\tvalid_1's rmse: 0.902684\n",
      "[874]\ttraining's rmse: 0.757302\tvalid_1's rmse: 0.902651\n",
      "[875]\ttraining's rmse: 0.757268\tvalid_1's rmse: 0.902638\n",
      "[876]\ttraining's rmse: 0.757222\tvalid_1's rmse: 0.902598\n",
      "[877]\ttraining's rmse: 0.757183\tvalid_1's rmse: 0.902639\n",
      "[878]\ttraining's rmse: 0.757113\tvalid_1's rmse: 0.90264\n",
      "[879]\ttraining's rmse: 0.757055\tvalid_1's rmse: 0.902642\n",
      "[880]\ttraining's rmse: 0.757019\tvalid_1's rmse: 0.902594\n",
      "[881]\ttraining's rmse: 0.756957\tvalid_1's rmse: 0.902551\n",
      "[882]\ttraining's rmse: 0.756906\tvalid_1's rmse: 0.902567\n",
      "[883]\ttraining's rmse: 0.756861\tvalid_1's rmse: 0.902561\n",
      "[884]\ttraining's rmse: 0.75681\tvalid_1's rmse: 0.902533\n",
      "[885]\ttraining's rmse: 0.75677\tvalid_1's rmse: 0.902542\n",
      "[886]\ttraining's rmse: 0.756732\tvalid_1's rmse: 0.902532\n",
      "[887]\ttraining's rmse: 0.756693\tvalid_1's rmse: 0.902512\n",
      "[888]\ttraining's rmse: 0.756665\tvalid_1's rmse: 0.902511\n",
      "[889]\ttraining's rmse: 0.756612\tvalid_1's rmse: 0.902514\n",
      "[890]\ttraining's rmse: 0.75657\tvalid_1's rmse: 0.902511\n",
      "[891]\ttraining's rmse: 0.756541\tvalid_1's rmse: 0.902495\n",
      "[892]\ttraining's rmse: 0.7565\tvalid_1's rmse: 0.902485\n",
      "[893]\ttraining's rmse: 0.756453\tvalid_1's rmse: 0.902435\n",
      "[894]\ttraining's rmse: 0.756404\tvalid_1's rmse: 0.902426\n",
      "[895]\ttraining's rmse: 0.756365\tvalid_1's rmse: 0.902413\n",
      "[896]\ttraining's rmse: 0.756315\tvalid_1's rmse: 0.902357\n",
      "[897]\ttraining's rmse: 0.756284\tvalid_1's rmse: 0.902355\n",
      "[898]\ttraining's rmse: 0.756244\tvalid_1's rmse: 0.902345\n",
      "[899]\ttraining's rmse: 0.756164\tvalid_1's rmse: 0.902327\n",
      "[900]\ttraining's rmse: 0.75612\tvalid_1's rmse: 0.902318\n",
      "[901]\ttraining's rmse: 0.756062\tvalid_1's rmse: 0.9023\n",
      "[902]\ttraining's rmse: 0.756014\tvalid_1's rmse: 0.902225\n",
      "[903]\ttraining's rmse: 0.755973\tvalid_1's rmse: 0.902189\n",
      "[904]\ttraining's rmse: 0.755917\tvalid_1's rmse: 0.902212\n",
      "[905]\ttraining's rmse: 0.755885\tvalid_1's rmse: 0.902206\n",
      "[906]\ttraining's rmse: 0.755832\tvalid_1's rmse: 0.902194\n",
      "[907]\ttraining's rmse: 0.755772\tvalid_1's rmse: 0.902195\n",
      "[908]\ttraining's rmse: 0.755689\tvalid_1's rmse: 0.902125\n",
      "[909]\ttraining's rmse: 0.755652\tvalid_1's rmse: 0.902128\n",
      "[910]\ttraining's rmse: 0.755614\tvalid_1's rmse: 0.902143\n",
      "[911]\ttraining's rmse: 0.75557\tvalid_1's rmse: 0.902148\n",
      "[912]\ttraining's rmse: 0.755539\tvalid_1's rmse: 0.902141\n",
      "[913]\ttraining's rmse: 0.755462\tvalid_1's rmse: 0.902103\n",
      "[914]\ttraining's rmse: 0.755415\tvalid_1's rmse: 0.902118\n",
      "[915]\ttraining's rmse: 0.755367\tvalid_1's rmse: 0.902126\n",
      "[916]\ttraining's rmse: 0.75532\tvalid_1's rmse: 0.902099\n",
      "[917]\ttraining's rmse: 0.755284\tvalid_1's rmse: 0.902099\n",
      "[918]\ttraining's rmse: 0.755241\tvalid_1's rmse: 0.902093\n",
      "[919]\ttraining's rmse: 0.755202\tvalid_1's rmse: 0.901988\n",
      "[920]\ttraining's rmse: 0.755173\tvalid_1's rmse: 0.901981\n",
      "[921]\ttraining's rmse: 0.755132\tvalid_1's rmse: 0.901959\n",
      "[922]\ttraining's rmse: 0.755083\tvalid_1's rmse: 0.901967\n",
      "[923]\ttraining's rmse: 0.755046\tvalid_1's rmse: 0.901922\n",
      "[924]\ttraining's rmse: 0.755001\tvalid_1's rmse: 0.90193\n",
      "[925]\ttraining's rmse: 0.754952\tvalid_1's rmse: 0.901957\n",
      "[926]\ttraining's rmse: 0.75491\tvalid_1's rmse: 0.901919\n",
      "[927]\ttraining's rmse: 0.754801\tvalid_1's rmse: 0.901856\n",
      "[928]\ttraining's rmse: 0.754754\tvalid_1's rmse: 0.901782\n",
      "[929]\ttraining's rmse: 0.754704\tvalid_1's rmse: 0.901683\n",
      "[930]\ttraining's rmse: 0.754668\tvalid_1's rmse: 0.901678\n",
      "[931]\ttraining's rmse: 0.754623\tvalid_1's rmse: 0.901661\n",
      "[932]\ttraining's rmse: 0.754568\tvalid_1's rmse: 0.901693\n",
      "[933]\ttraining's rmse: 0.754538\tvalid_1's rmse: 0.901687\n",
      "[934]\ttraining's rmse: 0.754504\tvalid_1's rmse: 0.901678\n",
      "[935]\ttraining's rmse: 0.754435\tvalid_1's rmse: 0.901633\n",
      "[936]\ttraining's rmse: 0.754396\tvalid_1's rmse: 0.901625\n",
      "[937]\ttraining's rmse: 0.754326\tvalid_1's rmse: 0.901563\n",
      "[938]\ttraining's rmse: 0.754285\tvalid_1's rmse: 0.901579\n",
      "[939]\ttraining's rmse: 0.754248\tvalid_1's rmse: 0.901589\n",
      "[940]\ttraining's rmse: 0.754212\tvalid_1's rmse: 0.901593\n",
      "[941]\ttraining's rmse: 0.754178\tvalid_1's rmse: 0.901487\n",
      "[942]\ttraining's rmse: 0.754144\tvalid_1's rmse: 0.901458\n",
      "[943]\ttraining's rmse: 0.754115\tvalid_1's rmse: 0.901401\n",
      "[944]\ttraining's rmse: 0.754074\tvalid_1's rmse: 0.901394\n",
      "[945]\ttraining's rmse: 0.754038\tvalid_1's rmse: 0.901407\n",
      "[946]\ttraining's rmse: 0.753978\tvalid_1's rmse: 0.90141\n",
      "[947]\ttraining's rmse: 0.753933\tvalid_1's rmse: 0.901402\n",
      "[948]\ttraining's rmse: 0.753882\tvalid_1's rmse: 0.901417\n",
      "[949]\ttraining's rmse: 0.753846\tvalid_1's rmse: 0.901407\n",
      "[950]\ttraining's rmse: 0.753773\tvalid_1's rmse: 0.901353\n",
      "[951]\ttraining's rmse: 0.753645\tvalid_1's rmse: 0.901288\n",
      "[952]\ttraining's rmse: 0.753599\tvalid_1's rmse: 0.901283\n",
      "[953]\ttraining's rmse: 0.753544\tvalid_1's rmse: 0.901288\n",
      "[954]\ttraining's rmse: 0.753508\tvalid_1's rmse: 0.901289\n",
      "[955]\ttraining's rmse: 0.753462\tvalid_1's rmse: 0.901313\n",
      "[956]\ttraining's rmse: 0.75341\tvalid_1's rmse: 0.901318\n",
      "[957]\ttraining's rmse: 0.753382\tvalid_1's rmse: 0.901313\n",
      "[958]\ttraining's rmse: 0.753344\tvalid_1's rmse: 0.901311\n",
      "[959]\ttraining's rmse: 0.753314\tvalid_1's rmse: 0.901307\n",
      "[960]\ttraining's rmse: 0.753279\tvalid_1's rmse: 0.901229\n",
      "[961]\ttraining's rmse: 0.753246\tvalid_1's rmse: 0.90124\n",
      "[962]\ttraining's rmse: 0.753197\tvalid_1's rmse: 0.901195\n",
      "[963]\ttraining's rmse: 0.753149\tvalid_1's rmse: 0.901196\n",
      "[964]\ttraining's rmse: 0.753102\tvalid_1's rmse: 0.901188\n",
      "[965]\ttraining's rmse: 0.753063\tvalid_1's rmse: 0.901195\n",
      "[966]\ttraining's rmse: 0.753017\tvalid_1's rmse: 0.901196\n",
      "[967]\ttraining's rmse: 0.752973\tvalid_1's rmse: 0.901191\n",
      "[968]\ttraining's rmse: 0.752925\tvalid_1's rmse: 0.901188\n",
      "[969]\ttraining's rmse: 0.752871\tvalid_1's rmse: 0.901176\n",
      "[970]\ttraining's rmse: 0.752826\tvalid_1's rmse: 0.901174\n",
      "[971]\ttraining's rmse: 0.752772\tvalid_1's rmse: 0.901123\n",
      "[972]\ttraining's rmse: 0.752732\tvalid_1's rmse: 0.901083\n",
      "[973]\ttraining's rmse: 0.752705\tvalid_1's rmse: 0.901075\n",
      "[974]\ttraining's rmse: 0.752672\tvalid_1's rmse: 0.901039\n",
      "[975]\ttraining's rmse: 0.752616\tvalid_1's rmse: 0.901031\n",
      "[976]\ttraining's rmse: 0.752579\tvalid_1's rmse: 0.900996\n",
      "[977]\ttraining's rmse: 0.752542\tvalid_1's rmse: 0.900995\n",
      "[978]\ttraining's rmse: 0.752463\tvalid_1's rmse: 0.900973\n",
      "[979]\ttraining's rmse: 0.752427\tvalid_1's rmse: 0.900969\n",
      "[980]\ttraining's rmse: 0.75239\tvalid_1's rmse: 0.900968\n",
      "[981]\ttraining's rmse: 0.752347\tvalid_1's rmse: 0.900955\n",
      "[982]\ttraining's rmse: 0.752314\tvalid_1's rmse: 0.900959\n",
      "[983]\ttraining's rmse: 0.752158\tvalid_1's rmse: 0.901111\n",
      "[984]\ttraining's rmse: 0.752108\tvalid_1's rmse: 0.901085\n",
      "[985]\ttraining's rmse: 0.752067\tvalid_1's rmse: 0.901084\n",
      "[986]\ttraining's rmse: 0.752032\tvalid_1's rmse: 0.901141\n",
      "[987]\ttraining's rmse: 0.752004\tvalid_1's rmse: 0.901145\n",
      "[988]\ttraining's rmse: 0.751969\tvalid_1's rmse: 0.901149\n",
      "[989]\ttraining's rmse: 0.751879\tvalid_1's rmse: 0.901066\n",
      "[990]\ttraining's rmse: 0.751816\tvalid_1's rmse: 0.900964\n",
      "[991]\ttraining's rmse: 0.751775\tvalid_1's rmse: 0.900998\n",
      "[992]\ttraining's rmse: 0.751742\tvalid_1's rmse: 0.900988\n",
      "[993]\ttraining's rmse: 0.7517\tvalid_1's rmse: 0.900917\n",
      "[994]\ttraining's rmse: 0.751666\tvalid_1's rmse: 0.90092\n",
      "[995]\ttraining's rmse: 0.751629\tvalid_1's rmse: 0.900917\n",
      "[996]\ttraining's rmse: 0.7516\tvalid_1's rmse: 0.900911\n",
      "[997]\ttraining's rmse: 0.751569\tvalid_1's rmse: 0.900918\n",
      "[998]\ttraining's rmse: 0.751527\tvalid_1's rmse: 0.900925\n",
      "[999]\ttraining's rmse: 0.751488\tvalid_1's rmse: 0.900933\n",
      "[1000]\ttraining's rmse: 0.751462\tvalid_1's rmse: 0.900918\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "train_data = lgb.Dataset(data=X_train, label=label_train)\n",
    "valid_data = lgb.Dataset(data=X_valid, label=label_valid)\n",
    "params = {\n",
    "    'objective': 'regression',  # 回归\n",
    "    'metric': 'rmse',   # 回归问题选择rmse\n",
    "    'n_estimators': 1000,\n",
    "    'num_leaves': 200,   # 每个弱学习器拥有的叶子的数量\n",
    "    'learning_rate': 0.01,\n",
    "    'bagging_fraction': 0.9,    # 每次训练“弱学习器”用的数据比例（应该也是随机的），用于加快训练速度和减小过拟合\n",
    "    'feature_fraction': 0.3,   # 每次迭代过程中，随机选择30%的特征建树（弱学习器）\n",
    "    'bagging_seed': 0,\n",
    "    'early_stop_rounds': 50\n",
    "}\n",
    "lgb_model = lgb.train(params, train_data, valid_sets=[train_data, valid_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练33个月"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: early_stop_rounds\n",
      "[LightGBM] [Warning] Unknown parameter: early_stop_rounds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.375121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13563\n",
      "[LightGBM] [Info] Number of data points in the train set: 9041130, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.295075\n",
      "[1]\ttraining's rmse: 1.21373\n",
      "[2]\ttraining's rmse: 1.20891\n",
      "[3]\ttraining's rmse: 1.20415\n",
      "[4]\ttraining's rmse: 1.19893\n",
      "[5]\ttraining's rmse: 1.19383\n",
      "[6]\ttraining's rmse: 1.18915\n",
      "[7]\ttraining's rmse: 1.18451\n",
      "[8]\ttraining's rmse: 1.17974\n",
      "[9]\ttraining's rmse: 1.17531\n",
      "[10]\ttraining's rmse: 1.17079\n",
      "[11]\ttraining's rmse: 1.16611\n",
      "[12]\ttraining's rmse: 1.16164\n",
      "[13]\ttraining's rmse: 1.15707\n",
      "[14]\ttraining's rmse: 1.15289\n",
      "[15]\ttraining's rmse: 1.14856\n",
      "[16]\ttraining's rmse: 1.14534\n",
      "[17]\ttraining's rmse: 1.14061\n",
      "[18]\ttraining's rmse: 1.13634\n",
      "[19]\ttraining's rmse: 1.13237\n",
      "[20]\ttraining's rmse: 1.12829\n",
      "[21]\ttraining's rmse: 1.12453\n",
      "[22]\ttraining's rmse: 1.12076\n",
      "[23]\ttraining's rmse: 1.11714\n",
      "[24]\ttraining's rmse: 1.1135\n",
      "[25]\ttraining's rmse: 1.11007\n",
      "[26]\ttraining's rmse: 1.10655\n",
      "[27]\ttraining's rmse: 1.10293\n",
      "[28]\ttraining's rmse: 1.09897\n",
      "[29]\ttraining's rmse: 1.09535\n",
      "[30]\ttraining's rmse: 1.09168\n",
      "[31]\ttraining's rmse: 1.08816\n",
      "[32]\ttraining's rmse: 1.08451\n",
      "[33]\ttraining's rmse: 1.0811\n",
      "[34]\ttraining's rmse: 1.07805\n",
      "[35]\ttraining's rmse: 1.0753\n",
      "[36]\ttraining's rmse: 1.07143\n",
      "[37]\ttraining's rmse: 1.06853\n",
      "[38]\ttraining's rmse: 1.06558\n",
      "[39]\ttraining's rmse: 1.06244\n",
      "[40]\ttraining's rmse: 1.05954\n",
      "[41]\ttraining's rmse: 1.05698\n",
      "[42]\ttraining's rmse: 1.05435\n",
      "[43]\ttraining's rmse: 1.05141\n",
      "[44]\ttraining's rmse: 1.04883\n",
      "[45]\ttraining's rmse: 1.04598\n",
      "[46]\ttraining's rmse: 1.04326\n",
      "[47]\ttraining's rmse: 1.0404\n",
      "[48]\ttraining's rmse: 1.03788\n",
      "[49]\ttraining's rmse: 1.03513\n",
      "[50]\ttraining's rmse: 1.03262\n",
      "[51]\ttraining's rmse: 1.02971\n",
      "[52]\ttraining's rmse: 1.02726\n",
      "[53]\ttraining's rmse: 1.02466\n",
      "[54]\ttraining's rmse: 1.02197\n",
      "[55]\ttraining's rmse: 1.01936\n",
      "[56]\ttraining's rmse: 1.01674\n",
      "[57]\ttraining's rmse: 1.01433\n",
      "[58]\ttraining's rmse: 1.01193\n",
      "[59]\ttraining's rmse: 1.00981\n",
      "[60]\ttraining's rmse: 1.0073\n",
      "[61]\ttraining's rmse: 1.00499\n",
      "[62]\ttraining's rmse: 1.00234\n",
      "[63]\ttraining's rmse: 1.00018\n",
      "[64]\ttraining's rmse: 0.997557\n",
      "[65]\ttraining's rmse: 0.995282\n",
      "[66]\ttraining's rmse: 0.993756\n",
      "[67]\ttraining's rmse: 0.99149\n",
      "[68]\ttraining's rmse: 0.989294\n",
      "[69]\ttraining's rmse: 0.987353\n",
      "[70]\ttraining's rmse: 0.985512\n",
      "[71]\ttraining's rmse: 0.983357\n",
      "[72]\ttraining's rmse: 0.981284\n",
      "[73]\ttraining's rmse: 0.978861\n",
      "[74]\ttraining's rmse: 0.977006\n",
      "[75]\ttraining's rmse: 0.975234\n",
      "[76]\ttraining's rmse: 0.973432\n",
      "[77]\ttraining's rmse: 0.97126\n",
      "[78]\ttraining's rmse: 0.969352\n",
      "[79]\ttraining's rmse: 0.967558\n",
      "[80]\ttraining's rmse: 0.965849\n",
      "[81]\ttraining's rmse: 0.963827\n",
      "[82]\ttraining's rmse: 0.962274\n",
      "[83]\ttraining's rmse: 0.960777\n",
      "[84]\ttraining's rmse: 0.958875\n",
      "[85]\ttraining's rmse: 0.957141\n",
      "[86]\ttraining's rmse: 0.955758\n",
      "[87]\ttraining's rmse: 0.954207\n",
      "[88]\ttraining's rmse: 0.952581\n",
      "[89]\ttraining's rmse: 0.951025\n",
      "[90]\ttraining's rmse: 0.949129\n",
      "[91]\ttraining's rmse: 0.947807\n",
      "[92]\ttraining's rmse: 0.946315\n",
      "[93]\ttraining's rmse: 0.944979\n",
      "[94]\ttraining's rmse: 0.943404\n",
      "[95]\ttraining's rmse: 0.942016\n",
      "[96]\ttraining's rmse: 0.940632\n",
      "[97]\ttraining's rmse: 0.939081\n",
      "[98]\ttraining's rmse: 0.937879\n",
      "[99]\ttraining's rmse: 0.936577\n",
      "[100]\ttraining's rmse: 0.93487\n",
      "[101]\ttraining's rmse: 0.933105\n",
      "[102]\ttraining's rmse: 0.931295\n",
      "[103]\ttraining's rmse: 0.930155\n",
      "[104]\ttraining's rmse: 0.928713\n",
      "[105]\ttraining's rmse: 0.927479\n",
      "[106]\ttraining's rmse: 0.926279\n",
      "[107]\ttraining's rmse: 0.924784\n",
      "[108]\ttraining's rmse: 0.923749\n",
      "[109]\ttraining's rmse: 0.922641\n",
      "[110]\ttraining's rmse: 0.921541\n",
      "[111]\ttraining's rmse: 0.920398\n",
      "[112]\ttraining's rmse: 0.919163\n",
      "[113]\ttraining's rmse: 0.917568\n",
      "[114]\ttraining's rmse: 0.916101\n",
      "[115]\ttraining's rmse: 0.914754\n",
      "[116]\ttraining's rmse: 0.91323\n",
      "[117]\ttraining's rmse: 0.912308\n",
      "[118]\ttraining's rmse: 0.911064\n",
      "[119]\ttraining's rmse: 0.909559\n",
      "[120]\ttraining's rmse: 0.908693\n",
      "[121]\ttraining's rmse: 0.907643\n",
      "[122]\ttraining's rmse: 0.906454\n",
      "[123]\ttraining's rmse: 0.90529\n",
      "[124]\ttraining's rmse: 0.904163\n",
      "[125]\ttraining's rmse: 0.90331\n",
      "[126]\ttraining's rmse: 0.901997\n",
      "[127]\ttraining's rmse: 0.900785\n",
      "[128]\ttraining's rmse: 0.899325\n",
      "[129]\ttraining's rmse: 0.898455\n",
      "[130]\ttraining's rmse: 0.897632\n",
      "[131]\ttraining's rmse: 0.896972\n",
      "[132]\ttraining's rmse: 0.895693\n",
      "[133]\ttraining's rmse: 0.894839\n",
      "[134]\ttraining's rmse: 0.893575\n",
      "[135]\ttraining's rmse: 0.892764\n",
      "[136]\ttraining's rmse: 0.891878\n",
      "[137]\ttraining's rmse: 0.890867\n",
      "[138]\ttraining's rmse: 0.889791\n",
      "[139]\ttraining's rmse: 0.889018\n",
      "[140]\ttraining's rmse: 0.888343\n",
      "[141]\ttraining's rmse: 0.887616\n",
      "[142]\ttraining's rmse: 0.886897\n",
      "[143]\ttraining's rmse: 0.886085\n",
      "[144]\ttraining's rmse: 0.885348\n",
      "[145]\ttraining's rmse: 0.88423\n",
      "[146]\ttraining's rmse: 0.883566\n",
      "[147]\ttraining's rmse: 0.882868\n",
      "[148]\ttraining's rmse: 0.882003\n",
      "[149]\ttraining's rmse: 0.880897\n",
      "[150]\ttraining's rmse: 0.88008\n",
      "[151]\ttraining's rmse: 0.879073\n",
      "[152]\ttraining's rmse: 0.878127\n",
      "[153]\ttraining's rmse: 0.877124\n",
      "[154]\ttraining's rmse: 0.876503\n",
      "[155]\ttraining's rmse: 0.875819\n",
      "[156]\ttraining's rmse: 0.875027\n",
      "[157]\ttraining's rmse: 0.874369\n",
      "[158]\ttraining's rmse: 0.873789\n",
      "[159]\ttraining's rmse: 0.872817\n",
      "[160]\ttraining's rmse: 0.872156\n",
      "[161]\ttraining's rmse: 0.871555\n",
      "[162]\ttraining's rmse: 0.871026\n",
      "[163]\ttraining's rmse: 0.870465\n",
      "[164]\ttraining's rmse: 0.86962\n",
      "[165]\ttraining's rmse: 0.86904\n",
      "[166]\ttraining's rmse: 0.868426\n",
      "[167]\ttraining's rmse: 0.867484\n",
      "[168]\ttraining's rmse: 0.866679\n",
      "[169]\ttraining's rmse: 0.86627\n",
      "[170]\ttraining's rmse: 0.865836\n",
      "[171]\ttraining's rmse: 0.865193\n",
      "[172]\ttraining's rmse: 0.86428\n",
      "[173]\ttraining's rmse: 0.863829\n",
      "[174]\ttraining's rmse: 0.863207\n",
      "[175]\ttraining's rmse: 0.862771\n",
      "[176]\ttraining's rmse: 0.86203\n",
      "[177]\ttraining's rmse: 0.861207\n",
      "[178]\ttraining's rmse: 0.860594\n",
      "[179]\ttraining's rmse: 0.860158\n",
      "[180]\ttraining's rmse: 0.859676\n",
      "[181]\ttraining's rmse: 0.859282\n",
      "[182]\ttraining's rmse: 0.858781\n",
      "[183]\ttraining's rmse: 0.858273\n",
      "[184]\ttraining's rmse: 0.857821\n",
      "[185]\ttraining's rmse: 0.857046\n",
      "[186]\ttraining's rmse: 0.856629\n",
      "[187]\ttraining's rmse: 0.855995\n",
      "[188]\ttraining's rmse: 0.855421\n",
      "[189]\ttraining's rmse: 0.854984\n",
      "[190]\ttraining's rmse: 0.85453\n",
      "[191]\ttraining's rmse: 0.854133\n",
      "[192]\ttraining's rmse: 0.853536\n",
      "[193]\ttraining's rmse: 0.853082\n",
      "[194]\ttraining's rmse: 0.852561\n",
      "[195]\ttraining's rmse: 0.851847\n",
      "[196]\ttraining's rmse: 0.851215\n",
      "[197]\ttraining's rmse: 0.850778\n",
      "[198]\ttraining's rmse: 0.850403\n",
      "[199]\ttraining's rmse: 0.849932\n",
      "[200]\ttraining's rmse: 0.849577\n",
      "[201]\ttraining's rmse: 0.849211\n",
      "[202]\ttraining's rmse: 0.848778\n",
      "[203]\ttraining's rmse: 0.848206\n",
      "[204]\ttraining's rmse: 0.847845\n",
      "[205]\ttraining's rmse: 0.847199\n",
      "[206]\ttraining's rmse: 0.846793\n",
      "[207]\ttraining's rmse: 0.846099\n",
      "[208]\ttraining's rmse: 0.845545\n",
      "[209]\ttraining's rmse: 0.84515\n",
      "[210]\ttraining's rmse: 0.844779\n",
      "[211]\ttraining's rmse: 0.844407\n",
      "[212]\ttraining's rmse: 0.844068\n",
      "[213]\ttraining's rmse: 0.843729\n",
      "[214]\ttraining's rmse: 0.843019\n",
      "[215]\ttraining's rmse: 0.842682\n",
      "[216]\ttraining's rmse: 0.842335\n",
      "[217]\ttraining's rmse: 0.841891\n",
      "[218]\ttraining's rmse: 0.841521\n",
      "[219]\ttraining's rmse: 0.841023\n",
      "[220]\ttraining's rmse: 0.840622\n",
      "[221]\ttraining's rmse: 0.84035\n",
      "[222]\ttraining's rmse: 0.839898\n",
      "[223]\ttraining's rmse: 0.839484\n",
      "[224]\ttraining's rmse: 0.839245\n",
      "[225]\ttraining's rmse: 0.838953\n",
      "[226]\ttraining's rmse: 0.838687\n",
      "[227]\ttraining's rmse: 0.83825\n",
      "[228]\ttraining's rmse: 0.8379\n",
      "[229]\ttraining's rmse: 0.837454\n",
      "[230]\ttraining's rmse: 0.837051\n",
      "[231]\ttraining's rmse: 0.836664\n",
      "[232]\ttraining's rmse: 0.836125\n",
      "[233]\ttraining's rmse: 0.835781\n",
      "[234]\ttraining's rmse: 0.835326\n",
      "[235]\ttraining's rmse: 0.834628\n",
      "[236]\ttraining's rmse: 0.834217\n",
      "[237]\ttraining's rmse: 0.833959\n",
      "[238]\ttraining's rmse: 0.83361\n",
      "[239]\ttraining's rmse: 0.833358\n",
      "[240]\ttraining's rmse: 0.833044\n",
      "[241]\ttraining's rmse: 0.832611\n",
      "[242]\ttraining's rmse: 0.832214\n",
      "[243]\ttraining's rmse: 0.831771\n",
      "[244]\ttraining's rmse: 0.831401\n",
      "[245]\ttraining's rmse: 0.831047\n",
      "[246]\ttraining's rmse: 0.830673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[247]\ttraining's rmse: 0.830423\n",
      "[248]\ttraining's rmse: 0.829891\n",
      "[249]\ttraining's rmse: 0.829568\n",
      "[250]\ttraining's rmse: 0.829098\n",
      "[251]\ttraining's rmse: 0.828706\n",
      "[252]\ttraining's rmse: 0.82853\n",
      "[253]\ttraining's rmse: 0.828218\n",
      "[254]\ttraining's rmse: 0.827774\n",
      "[255]\ttraining's rmse: 0.827401\n",
      "[256]\ttraining's rmse: 0.827201\n",
      "[257]\ttraining's rmse: 0.826776\n",
      "[258]\ttraining's rmse: 0.826564\n",
      "[259]\ttraining's rmse: 0.826366\n",
      "[260]\ttraining's rmse: 0.826037\n",
      "[261]\ttraining's rmse: 0.825792\n",
      "[262]\ttraining's rmse: 0.825431\n",
      "[263]\ttraining's rmse: 0.824982\n",
      "[264]\ttraining's rmse: 0.824606\n",
      "[265]\ttraining's rmse: 0.824006\n",
      "[266]\ttraining's rmse: 0.823742\n",
      "[267]\ttraining's rmse: 0.823465\n",
      "[268]\ttraining's rmse: 0.823087\n",
      "[269]\ttraining's rmse: 0.822787\n",
      "[270]\ttraining's rmse: 0.822443\n",
      "[271]\ttraining's rmse: 0.822267\n",
      "[272]\ttraining's rmse: 0.821924\n",
      "[273]\ttraining's rmse: 0.821662\n",
      "[274]\ttraining's rmse: 0.821324\n",
      "[275]\ttraining's rmse: 0.821101\n",
      "[276]\ttraining's rmse: 0.820923\n",
      "[277]\ttraining's rmse: 0.820593\n",
      "[278]\ttraining's rmse: 0.820394\n",
      "[279]\ttraining's rmse: 0.820097\n",
      "[280]\ttraining's rmse: 0.819788\n",
      "[281]\ttraining's rmse: 0.819481\n",
      "[282]\ttraining's rmse: 0.819183\n",
      "[283]\ttraining's rmse: 0.818901\n",
      "[284]\ttraining's rmse: 0.818578\n",
      "[285]\ttraining's rmse: 0.818205\n",
      "[286]\ttraining's rmse: 0.817831\n",
      "[287]\ttraining's rmse: 0.817635\n",
      "[288]\ttraining's rmse: 0.817327\n",
      "[289]\ttraining's rmse: 0.817171\n",
      "[290]\ttraining's rmse: 0.816877\n",
      "[291]\ttraining's rmse: 0.816473\n",
      "[292]\ttraining's rmse: 0.816175\n",
      "[293]\ttraining's rmse: 0.815795\n",
      "[294]\ttraining's rmse: 0.815447\n",
      "[295]\ttraining's rmse: 0.815161\n",
      "[296]\ttraining's rmse: 0.814977\n",
      "[297]\ttraining's rmse: 0.81443\n",
      "[298]\ttraining's rmse: 0.81413\n",
      "[299]\ttraining's rmse: 0.813769\n",
      "[300]\ttraining's rmse: 0.813588\n",
      "[301]\ttraining's rmse: 0.813386\n",
      "[302]\ttraining's rmse: 0.813229\n",
      "[303]\ttraining's rmse: 0.813072\n",
      "[304]\ttraining's rmse: 0.812755\n",
      "[305]\ttraining's rmse: 0.812492\n",
      "[306]\ttraining's rmse: 0.812189\n",
      "[307]\ttraining's rmse: 0.811961\n",
      "[308]\ttraining's rmse: 0.811602\n",
      "[309]\ttraining's rmse: 0.811333\n",
      "[310]\ttraining's rmse: 0.811014\n",
      "[311]\ttraining's rmse: 0.810772\n",
      "[312]\ttraining's rmse: 0.81043\n",
      "[313]\ttraining's rmse: 0.810201\n",
      "[314]\ttraining's rmse: 0.809929\n",
      "[315]\ttraining's rmse: 0.809745\n",
      "[316]\ttraining's rmse: 0.809492\n",
      "[317]\ttraining's rmse: 0.809253\n",
      "[318]\ttraining's rmse: 0.80896\n",
      "[319]\ttraining's rmse: 0.80872\n",
      "[320]\ttraining's rmse: 0.808525\n",
      "[321]\ttraining's rmse: 0.808259\n",
      "[322]\ttraining's rmse: 0.808063\n",
      "[323]\ttraining's rmse: 0.807813\n",
      "[324]\ttraining's rmse: 0.807584\n",
      "[325]\ttraining's rmse: 0.807348\n",
      "[326]\ttraining's rmse: 0.807196\n",
      "[327]\ttraining's rmse: 0.806934\n",
      "[328]\ttraining's rmse: 0.806809\n",
      "[329]\ttraining's rmse: 0.806566\n",
      "[330]\ttraining's rmse: 0.806268\n",
      "[331]\ttraining's rmse: 0.805956\n",
      "[332]\ttraining's rmse: 0.805811\n",
      "[333]\ttraining's rmse: 0.805621\n",
      "[334]\ttraining's rmse: 0.805392\n",
      "[335]\ttraining's rmse: 0.805165\n",
      "[336]\ttraining's rmse: 0.804891\n",
      "[337]\ttraining's rmse: 0.804745\n",
      "[338]\ttraining's rmse: 0.804631\n",
      "[339]\ttraining's rmse: 0.804313\n",
      "[340]\ttraining's rmse: 0.804136\n",
      "[341]\ttraining's rmse: 0.803894\n",
      "[342]\ttraining's rmse: 0.803623\n",
      "[343]\ttraining's rmse: 0.803511\n",
      "[344]\ttraining's rmse: 0.803355\n",
      "[345]\ttraining's rmse: 0.803039\n",
      "[346]\ttraining's rmse: 0.80268\n",
      "[347]\ttraining's rmse: 0.802397\n",
      "[348]\ttraining's rmse: 0.802257\n",
      "[349]\ttraining's rmse: 0.802081\n",
      "[350]\ttraining's rmse: 0.801937\n",
      "[351]\ttraining's rmse: 0.801747\n",
      "[352]\ttraining's rmse: 0.80164\n",
      "[353]\ttraining's rmse: 0.801353\n",
      "[354]\ttraining's rmse: 0.801185\n",
      "[355]\ttraining's rmse: 0.800985\n",
      "[356]\ttraining's rmse: 0.800827\n",
      "[357]\ttraining's rmse: 0.800674\n",
      "[358]\ttraining's rmse: 0.800475\n",
      "[359]\ttraining's rmse: 0.800344\n",
      "[360]\ttraining's rmse: 0.800166\n",
      "[361]\ttraining's rmse: 0.799984\n",
      "[362]\ttraining's rmse: 0.799786\n",
      "[363]\ttraining's rmse: 0.799599\n",
      "[364]\ttraining's rmse: 0.79943\n",
      "[365]\ttraining's rmse: 0.799246\n",
      "[366]\ttraining's rmse: 0.799134\n",
      "[367]\ttraining's rmse: 0.798971\n",
      "[368]\ttraining's rmse: 0.798694\n",
      "[369]\ttraining's rmse: 0.798447\n",
      "[370]\ttraining's rmse: 0.798319\n",
      "[371]\ttraining's rmse: 0.798086\n",
      "[372]\ttraining's rmse: 0.797875\n",
      "[373]\ttraining's rmse: 0.797707\n",
      "[374]\ttraining's rmse: 0.79753\n",
      "[375]\ttraining's rmse: 0.797416\n",
      "[376]\ttraining's rmse: 0.797172\n",
      "[377]\ttraining's rmse: 0.797012\n",
      "[378]\ttraining's rmse: 0.796901\n",
      "[379]\ttraining's rmse: 0.796729\n",
      "[380]\ttraining's rmse: 0.796542\n",
      "[381]\ttraining's rmse: 0.796417\n",
      "[382]\ttraining's rmse: 0.796232\n",
      "[383]\ttraining's rmse: 0.796064\n",
      "[384]\ttraining's rmse: 0.79591\n",
      "[385]\ttraining's rmse: 0.7958\n",
      "[386]\ttraining's rmse: 0.795464\n",
      "[387]\ttraining's rmse: 0.795335\n",
      "[388]\ttraining's rmse: 0.795083\n",
      "[389]\ttraining's rmse: 0.79499\n",
      "[390]\ttraining's rmse: 0.794762\n",
      "[391]\ttraining's rmse: 0.794591\n",
      "[392]\ttraining's rmse: 0.794395\n",
      "[393]\ttraining's rmse: 0.79431\n",
      "[394]\ttraining's rmse: 0.794123\n",
      "[395]\ttraining's rmse: 0.794013\n",
      "[396]\ttraining's rmse: 0.793858\n",
      "[397]\ttraining's rmse: 0.793587\n",
      "[398]\ttraining's rmse: 0.793434\n",
      "[399]\ttraining's rmse: 0.793257\n",
      "[400]\ttraining's rmse: 0.793118\n",
      "[401]\ttraining's rmse: 0.792963\n",
      "[402]\ttraining's rmse: 0.792868\n",
      "[403]\ttraining's rmse: 0.792732\n",
      "[404]\ttraining's rmse: 0.792642\n",
      "[405]\ttraining's rmse: 0.792409\n",
      "[406]\ttraining's rmse: 0.792338\n",
      "[407]\ttraining's rmse: 0.792199\n",
      "[408]\ttraining's rmse: 0.791955\n",
      "[409]\ttraining's rmse: 0.791683\n",
      "[410]\ttraining's rmse: 0.791526\n",
      "[411]\ttraining's rmse: 0.791398\n",
      "[412]\ttraining's rmse: 0.79118\n",
      "[413]\ttraining's rmse: 0.791028\n",
      "[414]\ttraining's rmse: 0.790914\n",
      "[415]\ttraining's rmse: 0.790716\n",
      "[416]\ttraining's rmse: 0.790571\n",
      "[417]\ttraining's rmse: 0.790396\n",
      "[418]\ttraining's rmse: 0.790252\n",
      "[419]\ttraining's rmse: 0.790168\n",
      "[420]\ttraining's rmse: 0.790012\n",
      "[421]\ttraining's rmse: 0.789944\n",
      "[422]\ttraining's rmse: 0.789833\n",
      "[423]\ttraining's rmse: 0.789661\n",
      "[424]\ttraining's rmse: 0.789511\n",
      "[425]\ttraining's rmse: 0.789326\n",
      "[426]\ttraining's rmse: 0.789164\n",
      "[427]\ttraining's rmse: 0.788996\n",
      "[428]\ttraining's rmse: 0.788901\n",
      "[429]\ttraining's rmse: 0.788762\n",
      "[430]\ttraining's rmse: 0.788673\n",
      "[431]\ttraining's rmse: 0.788549\n",
      "[432]\ttraining's rmse: 0.788454\n",
      "[433]\ttraining's rmse: 0.788283\n",
      "[434]\ttraining's rmse: 0.788123\n",
      "[435]\ttraining's rmse: 0.788006\n",
      "[436]\ttraining's rmse: 0.787833\n",
      "[437]\ttraining's rmse: 0.787701\n",
      "[438]\ttraining's rmse: 0.787534\n",
      "[439]\ttraining's rmse: 0.787415\n",
      "[440]\ttraining's rmse: 0.78729\n",
      "[441]\ttraining's rmse: 0.787205\n",
      "[442]\ttraining's rmse: 0.787084\n",
      "[443]\ttraining's rmse: 0.786949\n",
      "[444]\ttraining's rmse: 0.786843\n",
      "[445]\ttraining's rmse: 0.786702\n",
      "[446]\ttraining's rmse: 0.786495\n",
      "[447]\ttraining's rmse: 0.786349\n",
      "[448]\ttraining's rmse: 0.786207\n",
      "[449]\ttraining's rmse: 0.785759\n",
      "[450]\ttraining's rmse: 0.785661\n",
      "[451]\ttraining's rmse: 0.785463\n",
      "[452]\ttraining's rmse: 0.785293\n",
      "[453]\ttraining's rmse: 0.785064\n",
      "[454]\ttraining's rmse: 0.784979\n",
      "[455]\ttraining's rmse: 0.784834\n",
      "[456]\ttraining's rmse: 0.784734\n",
      "[457]\ttraining's rmse: 0.784636\n",
      "[458]\ttraining's rmse: 0.784518\n",
      "[459]\ttraining's rmse: 0.784402\n",
      "[460]\ttraining's rmse: 0.784332\n",
      "[461]\ttraining's rmse: 0.78416\n",
      "[462]\ttraining's rmse: 0.784008\n",
      "[463]\ttraining's rmse: 0.783926\n",
      "[464]\ttraining's rmse: 0.783813\n",
      "[465]\ttraining's rmse: 0.783741\n",
      "[466]\ttraining's rmse: 0.783595\n",
      "[467]\ttraining's rmse: 0.783444\n",
      "[468]\ttraining's rmse: 0.783339\n",
      "[469]\ttraining's rmse: 0.782918\n",
      "[470]\ttraining's rmse: 0.782774\n",
      "[471]\ttraining's rmse: 0.782647\n",
      "[472]\ttraining's rmse: 0.782507\n",
      "[473]\ttraining's rmse: 0.782295\n",
      "[474]\ttraining's rmse: 0.782168\n",
      "[475]\ttraining's rmse: 0.782039\n",
      "[476]\ttraining's rmse: 0.781905\n",
      "[477]\ttraining's rmse: 0.781816\n",
      "[478]\ttraining's rmse: 0.781732\n",
      "[479]\ttraining's rmse: 0.781646\n",
      "[480]\ttraining's rmse: 0.781561\n",
      "[481]\ttraining's rmse: 0.781481\n",
      "[482]\ttraining's rmse: 0.781315\n",
      "[483]\ttraining's rmse: 0.781195\n",
      "[484]\ttraining's rmse: 0.781038\n",
      "[485]\ttraining's rmse: 0.780902\n",
      "[486]\ttraining's rmse: 0.780796\n",
      "[487]\ttraining's rmse: 0.780741\n",
      "[488]\ttraining's rmse: 0.780638\n",
      "[489]\ttraining's rmse: 0.780495\n",
      "[490]\ttraining's rmse: 0.780373\n",
      "[491]\ttraining's rmse: 0.7803\n",
      "[492]\ttraining's rmse: 0.78019\n",
      "[493]\ttraining's rmse: 0.780107\n",
      "[494]\ttraining's rmse: 0.779893\n",
      "[495]\ttraining's rmse: 0.779794\n",
      "[496]\ttraining's rmse: 0.779696\n",
      "[497]\ttraining's rmse: 0.779562\n",
      "[498]\ttraining's rmse: 0.779414\n",
      "[499]\ttraining's rmse: 0.779292\n",
      "[500]\ttraining's rmse: 0.779205\n",
      "[501]\ttraining's rmse: 0.779124\n",
      "[502]\ttraining's rmse: 0.779032\n",
      "[503]\ttraining's rmse: 0.778976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[504]\ttraining's rmse: 0.778853\n",
      "[505]\ttraining's rmse: 0.778751\n",
      "[506]\ttraining's rmse: 0.778675\n",
      "[507]\ttraining's rmse: 0.778602\n",
      "[508]\ttraining's rmse: 0.77852\n",
      "[509]\ttraining's rmse: 0.778416\n",
      "[510]\ttraining's rmse: 0.778338\n",
      "[511]\ttraining's rmse: 0.778216\n",
      "[512]\ttraining's rmse: 0.778145\n",
      "[513]\ttraining's rmse: 0.778049\n",
      "[514]\ttraining's rmse: 0.777965\n",
      "[515]\ttraining's rmse: 0.777855\n",
      "[516]\ttraining's rmse: 0.77779\n",
      "[517]\ttraining's rmse: 0.777682\n",
      "[518]\ttraining's rmse: 0.777483\n",
      "[519]\ttraining's rmse: 0.777412\n",
      "[520]\ttraining's rmse: 0.777094\n",
      "[521]\ttraining's rmse: 0.776968\n",
      "[522]\ttraining's rmse: 0.776884\n",
      "[523]\ttraining's rmse: 0.776796\n",
      "[524]\ttraining's rmse: 0.776718\n",
      "[525]\ttraining's rmse: 0.776524\n",
      "[526]\ttraining's rmse: 0.776405\n",
      "[527]\ttraining's rmse: 0.776243\n",
      "[528]\ttraining's rmse: 0.776143\n",
      "[529]\ttraining's rmse: 0.776041\n",
      "[530]\ttraining's rmse: 0.775958\n",
      "[531]\ttraining's rmse: 0.775887\n",
      "[532]\ttraining's rmse: 0.775804\n",
      "[533]\ttraining's rmse: 0.775621\n",
      "[534]\ttraining's rmse: 0.775513\n",
      "[535]\ttraining's rmse: 0.775426\n",
      "[536]\ttraining's rmse: 0.775356\n",
      "[537]\ttraining's rmse: 0.775248\n",
      "[538]\ttraining's rmse: 0.775136\n",
      "[539]\ttraining's rmse: 0.775056\n",
      "[540]\ttraining's rmse: 0.774991\n",
      "[541]\ttraining's rmse: 0.774843\n",
      "[542]\ttraining's rmse: 0.774726\n",
      "[543]\ttraining's rmse: 0.774647\n",
      "[544]\ttraining's rmse: 0.774543\n",
      "[545]\ttraining's rmse: 0.774441\n",
      "[546]\ttraining's rmse: 0.774359\n",
      "[547]\ttraining's rmse: 0.774259\n",
      "[548]\ttraining's rmse: 0.774197\n",
      "[549]\ttraining's rmse: 0.774133\n",
      "[550]\ttraining's rmse: 0.77407\n",
      "[551]\ttraining's rmse: 0.773941\n",
      "[552]\ttraining's rmse: 0.773829\n",
      "[553]\ttraining's rmse: 0.773757\n",
      "[554]\ttraining's rmse: 0.773666\n",
      "[555]\ttraining's rmse: 0.773585\n",
      "[556]\ttraining's rmse: 0.773456\n",
      "[557]\ttraining's rmse: 0.773384\n",
      "[558]\ttraining's rmse: 0.773334\n",
      "[559]\ttraining's rmse: 0.773241\n",
      "[560]\ttraining's rmse: 0.773168\n",
      "[561]\ttraining's rmse: 0.773071\n",
      "[562]\ttraining's rmse: 0.773007\n",
      "[563]\ttraining's rmse: 0.772915\n",
      "[564]\ttraining's rmse: 0.772742\n",
      "[565]\ttraining's rmse: 0.77269\n",
      "[566]\ttraining's rmse: 0.772605\n",
      "[567]\ttraining's rmse: 0.77255\n",
      "[568]\ttraining's rmse: 0.772348\n",
      "[569]\ttraining's rmse: 0.772268\n",
      "[570]\ttraining's rmse: 0.77221\n",
      "[571]\ttraining's rmse: 0.772106\n",
      "[572]\ttraining's rmse: 0.772046\n",
      "[573]\ttraining's rmse: 0.77198\n",
      "[574]\ttraining's rmse: 0.771926\n",
      "[575]\ttraining's rmse: 0.771827\n",
      "[576]\ttraining's rmse: 0.771723\n",
      "[577]\ttraining's rmse: 0.771593\n",
      "[578]\ttraining's rmse: 0.771502\n",
      "[579]\ttraining's rmse: 0.771425\n",
      "[580]\ttraining's rmse: 0.771363\n",
      "[581]\ttraining's rmse: 0.77129\n",
      "[582]\ttraining's rmse: 0.771206\n",
      "[583]\ttraining's rmse: 0.771127\n",
      "[584]\ttraining's rmse: 0.770971\n",
      "[585]\ttraining's rmse: 0.770888\n",
      "[586]\ttraining's rmse: 0.770801\n",
      "[587]\ttraining's rmse: 0.770702\n",
      "[588]\ttraining's rmse: 0.770233\n",
      "[589]\ttraining's rmse: 0.770097\n",
      "[590]\ttraining's rmse: 0.770003\n",
      "[591]\ttraining's rmse: 0.769944\n",
      "[592]\ttraining's rmse: 0.769888\n",
      "[593]\ttraining's rmse: 0.769819\n",
      "[594]\ttraining's rmse: 0.769733\n",
      "[595]\ttraining's rmse: 0.769656\n",
      "[596]\ttraining's rmse: 0.769591\n",
      "[597]\ttraining's rmse: 0.769467\n",
      "[598]\ttraining's rmse: 0.769241\n",
      "[599]\ttraining's rmse: 0.769161\n",
      "[600]\ttraining's rmse: 0.769012\n",
      "[601]\ttraining's rmse: 0.768935\n",
      "[602]\ttraining's rmse: 0.768872\n",
      "[603]\ttraining's rmse: 0.768762\n",
      "[604]\ttraining's rmse: 0.76869\n",
      "[605]\ttraining's rmse: 0.768629\n",
      "[606]\ttraining's rmse: 0.768557\n",
      "[607]\ttraining's rmse: 0.768485\n",
      "[608]\ttraining's rmse: 0.768385\n",
      "[609]\ttraining's rmse: 0.76832\n",
      "[610]\ttraining's rmse: 0.768169\n",
      "[611]\ttraining's rmse: 0.768107\n",
      "[612]\ttraining's rmse: 0.767995\n",
      "[613]\ttraining's rmse: 0.76791\n",
      "[614]\ttraining's rmse: 0.767862\n",
      "[615]\ttraining's rmse: 0.767782\n",
      "[616]\ttraining's rmse: 0.767715\n",
      "[617]\ttraining's rmse: 0.767648\n",
      "[618]\ttraining's rmse: 0.767553\n",
      "[619]\ttraining's rmse: 0.767477\n",
      "[620]\ttraining's rmse: 0.767397\n",
      "[621]\ttraining's rmse: 0.767203\n",
      "[622]\ttraining's rmse: 0.767131\n",
      "[623]\ttraining's rmse: 0.767049\n",
      "[624]\ttraining's rmse: 0.766986\n",
      "[625]\ttraining's rmse: 0.766931\n",
      "[626]\ttraining's rmse: 0.766862\n",
      "[627]\ttraining's rmse: 0.766796\n",
      "[628]\ttraining's rmse: 0.766716\n",
      "[629]\ttraining's rmse: 0.766629\n",
      "[630]\ttraining's rmse: 0.766558\n",
      "[631]\ttraining's rmse: 0.766255\n",
      "[632]\ttraining's rmse: 0.766165\n",
      "[633]\ttraining's rmse: 0.766101\n",
      "[634]\ttraining's rmse: 0.765986\n",
      "[635]\ttraining's rmse: 0.765916\n",
      "[636]\ttraining's rmse: 0.765812\n",
      "[637]\ttraining's rmse: 0.765752\n",
      "[638]\ttraining's rmse: 0.765672\n",
      "[639]\ttraining's rmse: 0.765581\n",
      "[640]\ttraining's rmse: 0.765515\n",
      "[641]\ttraining's rmse: 0.765458\n",
      "[642]\ttraining's rmse: 0.765135\n",
      "[643]\ttraining's rmse: 0.765068\n",
      "[644]\ttraining's rmse: 0.765009\n",
      "[645]\ttraining's rmse: 0.764924\n",
      "[646]\ttraining's rmse: 0.764784\n",
      "[647]\ttraining's rmse: 0.764712\n",
      "[648]\ttraining's rmse: 0.764627\n",
      "[649]\ttraining's rmse: 0.764574\n",
      "[650]\ttraining's rmse: 0.76451\n",
      "[651]\ttraining's rmse: 0.764386\n",
      "[652]\ttraining's rmse: 0.764324\n",
      "[653]\ttraining's rmse: 0.764265\n",
      "[654]\ttraining's rmse: 0.764183\n",
      "[655]\ttraining's rmse: 0.764128\n",
      "[656]\ttraining's rmse: 0.764046\n",
      "[657]\ttraining's rmse: 0.763949\n",
      "[658]\ttraining's rmse: 0.763868\n",
      "[659]\ttraining's rmse: 0.763798\n",
      "[660]\ttraining's rmse: 0.763747\n",
      "[661]\ttraining's rmse: 0.763691\n",
      "[662]\ttraining's rmse: 0.763576\n",
      "[663]\ttraining's rmse: 0.763504\n",
      "[664]\ttraining's rmse: 0.763394\n",
      "[665]\ttraining's rmse: 0.76329\n",
      "[666]\ttraining's rmse: 0.763232\n",
      "[667]\ttraining's rmse: 0.763154\n",
      "[668]\ttraining's rmse: 0.763025\n",
      "[669]\ttraining's rmse: 0.762948\n",
      "[670]\ttraining's rmse: 0.762896\n",
      "[671]\ttraining's rmse: 0.762861\n",
      "[672]\ttraining's rmse: 0.762726\n",
      "[673]\ttraining's rmse: 0.76261\n",
      "[674]\ttraining's rmse: 0.762528\n",
      "[675]\ttraining's rmse: 0.76244\n",
      "[676]\ttraining's rmse: 0.762333\n",
      "[677]\ttraining's rmse: 0.762247\n",
      "[678]\ttraining's rmse: 0.762203\n",
      "[679]\ttraining's rmse: 0.762159\n",
      "[680]\ttraining's rmse: 0.7621\n",
      "[681]\ttraining's rmse: 0.761961\n",
      "[682]\ttraining's rmse: 0.761918\n",
      "[683]\ttraining's rmse: 0.761845\n",
      "[684]\ttraining's rmse: 0.761761\n",
      "[685]\ttraining's rmse: 0.761715\n",
      "[686]\ttraining's rmse: 0.761649\n",
      "[687]\ttraining's rmse: 0.761601\n",
      "[688]\ttraining's rmse: 0.761527\n",
      "[689]\ttraining's rmse: 0.761462\n",
      "[690]\ttraining's rmse: 0.761395\n",
      "[691]\ttraining's rmse: 0.761332\n",
      "[692]\ttraining's rmse: 0.761273\n",
      "[693]\ttraining's rmse: 0.761192\n",
      "[694]\ttraining's rmse: 0.761122\n",
      "[695]\ttraining's rmse: 0.76107\n",
      "[696]\ttraining's rmse: 0.761004\n",
      "[697]\ttraining's rmse: 0.760938\n",
      "[698]\ttraining's rmse: 0.76089\n",
      "[699]\ttraining's rmse: 0.760819\n",
      "[700]\ttraining's rmse: 0.760737\n",
      "[701]\ttraining's rmse: 0.760669\n",
      "[702]\ttraining's rmse: 0.760579\n",
      "[703]\ttraining's rmse: 0.760532\n",
      "[704]\ttraining's rmse: 0.760452\n",
      "[705]\ttraining's rmse: 0.760346\n",
      "[706]\ttraining's rmse: 0.760245\n",
      "[707]\ttraining's rmse: 0.760201\n",
      "[708]\ttraining's rmse: 0.760123\n",
      "[709]\ttraining's rmse: 0.760042\n",
      "[710]\ttraining's rmse: 0.759998\n",
      "[711]\ttraining's rmse: 0.759929\n",
      "[712]\ttraining's rmse: 0.759878\n",
      "[713]\ttraining's rmse: 0.759812\n",
      "[714]\ttraining's rmse: 0.759723\n",
      "[715]\ttraining's rmse: 0.759644\n",
      "[716]\ttraining's rmse: 0.759598\n",
      "[717]\ttraining's rmse: 0.7595\n",
      "[718]\ttraining's rmse: 0.759439\n",
      "[719]\ttraining's rmse: 0.759385\n",
      "[720]\ttraining's rmse: 0.759338\n",
      "[721]\ttraining's rmse: 0.759287\n",
      "[722]\ttraining's rmse: 0.759235\n",
      "[723]\ttraining's rmse: 0.759185\n",
      "[724]\ttraining's rmse: 0.759116\n",
      "[725]\ttraining's rmse: 0.759025\n",
      "[726]\ttraining's rmse: 0.758971\n",
      "[727]\ttraining's rmse: 0.758918\n",
      "[728]\ttraining's rmse: 0.758855\n",
      "[729]\ttraining's rmse: 0.75881\n",
      "[730]\ttraining's rmse: 0.758742\n",
      "[731]\ttraining's rmse: 0.758674\n",
      "[732]\ttraining's rmse: 0.758621\n",
      "[733]\ttraining's rmse: 0.758577\n",
      "[734]\ttraining's rmse: 0.758524\n",
      "[735]\ttraining's rmse: 0.758453\n",
      "[736]\ttraining's rmse: 0.758413\n",
      "[737]\ttraining's rmse: 0.758354\n",
      "[738]\ttraining's rmse: 0.758279\n",
      "[739]\ttraining's rmse: 0.758234\n",
      "[740]\ttraining's rmse: 0.758179\n",
      "[741]\ttraining's rmse: 0.758096\n",
      "[742]\ttraining's rmse: 0.758042\n",
      "[743]\ttraining's rmse: 0.757965\n",
      "[744]\ttraining's rmse: 0.757929\n",
      "[745]\ttraining's rmse: 0.757876\n",
      "[746]\ttraining's rmse: 0.757771\n",
      "[747]\ttraining's rmse: 0.757711\n",
      "[748]\ttraining's rmse: 0.757627\n",
      "[749]\ttraining's rmse: 0.75756\n",
      "[750]\ttraining's rmse: 0.757453\n",
      "[751]\ttraining's rmse: 0.757365\n",
      "[752]\ttraining's rmse: 0.757305\n",
      "[753]\ttraining's rmse: 0.757191\n",
      "[754]\ttraining's rmse: 0.757146\n",
      "[755]\ttraining's rmse: 0.757105\n",
      "[756]\ttraining's rmse: 0.757055\n",
      "[757]\ttraining's rmse: 0.757007\n",
      "[758]\ttraining's rmse: 0.756952\n",
      "[759]\ttraining's rmse: 0.756895\n",
      "[760]\ttraining's rmse: 0.756827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[761]\ttraining's rmse: 0.756752\n",
      "[762]\ttraining's rmse: 0.756689\n",
      "[763]\ttraining's rmse: 0.75661\n",
      "[764]\ttraining's rmse: 0.75657\n",
      "[765]\ttraining's rmse: 0.756326\n",
      "[766]\ttraining's rmse: 0.75627\n",
      "[767]\ttraining's rmse: 0.756213\n",
      "[768]\ttraining's rmse: 0.756151\n",
      "[769]\ttraining's rmse: 0.756065\n",
      "[770]\ttraining's rmse: 0.756005\n",
      "[771]\ttraining's rmse: 0.75594\n",
      "[772]\ttraining's rmse: 0.75589\n",
      "[773]\ttraining's rmse: 0.755844\n",
      "[774]\ttraining's rmse: 0.755781\n",
      "[775]\ttraining's rmse: 0.75573\n",
      "[776]\ttraining's rmse: 0.755686\n",
      "[777]\ttraining's rmse: 0.755617\n",
      "[778]\ttraining's rmse: 0.755579\n",
      "[779]\ttraining's rmse: 0.755542\n",
      "[780]\ttraining's rmse: 0.755497\n",
      "[781]\ttraining's rmse: 0.755423\n",
      "[782]\ttraining's rmse: 0.755381\n",
      "[783]\ttraining's rmse: 0.755341\n",
      "[784]\ttraining's rmse: 0.755286\n",
      "[785]\ttraining's rmse: 0.755195\n",
      "[786]\ttraining's rmse: 0.755137\n",
      "[787]\ttraining's rmse: 0.755084\n",
      "[788]\ttraining's rmse: 0.755018\n",
      "[789]\ttraining's rmse: 0.754954\n",
      "[790]\ttraining's rmse: 0.754892\n",
      "[791]\ttraining's rmse: 0.75482\n",
      "[792]\ttraining's rmse: 0.754619\n",
      "[793]\ttraining's rmse: 0.754574\n",
      "[794]\ttraining's rmse: 0.75452\n",
      "[795]\ttraining's rmse: 0.754473\n",
      "[796]\ttraining's rmse: 0.754425\n",
      "[797]\ttraining's rmse: 0.754366\n",
      "[798]\ttraining's rmse: 0.754313\n",
      "[799]\ttraining's rmse: 0.754268\n",
      "[800]\ttraining's rmse: 0.754217\n",
      "[801]\ttraining's rmse: 0.754156\n",
      "[802]\ttraining's rmse: 0.754035\n",
      "[803]\ttraining's rmse: 0.753959\n",
      "[804]\ttraining's rmse: 0.753929\n",
      "[805]\ttraining's rmse: 0.753858\n",
      "[806]\ttraining's rmse: 0.753811\n",
      "[807]\ttraining's rmse: 0.753729\n",
      "[808]\ttraining's rmse: 0.753681\n",
      "[809]\ttraining's rmse: 0.753608\n",
      "[810]\ttraining's rmse: 0.75356\n",
      "[811]\ttraining's rmse: 0.753503\n",
      "[812]\ttraining's rmse: 0.753466\n",
      "[813]\ttraining's rmse: 0.753423\n",
      "[814]\ttraining's rmse: 0.753382\n",
      "[815]\ttraining's rmse: 0.753335\n",
      "[816]\ttraining's rmse: 0.753293\n",
      "[817]\ttraining's rmse: 0.753253\n",
      "[818]\ttraining's rmse: 0.753188\n",
      "[819]\ttraining's rmse: 0.753158\n",
      "[820]\ttraining's rmse: 0.753097\n",
      "[821]\ttraining's rmse: 0.753044\n",
      "[822]\ttraining's rmse: 0.752982\n",
      "[823]\ttraining's rmse: 0.752919\n",
      "[824]\ttraining's rmse: 0.752884\n",
      "[825]\ttraining's rmse: 0.752843\n",
      "[826]\ttraining's rmse: 0.752789\n",
      "[827]\ttraining's rmse: 0.752734\n",
      "[828]\ttraining's rmse: 0.752681\n",
      "[829]\ttraining's rmse: 0.752625\n",
      "[830]\ttraining's rmse: 0.752573\n",
      "[831]\ttraining's rmse: 0.75245\n",
      "[832]\ttraining's rmse: 0.752397\n",
      "[833]\ttraining's rmse: 0.752332\n",
      "[834]\ttraining's rmse: 0.75225\n",
      "[835]\ttraining's rmse: 0.752165\n",
      "[836]\ttraining's rmse: 0.752104\n",
      "[837]\ttraining's rmse: 0.752065\n",
      "[838]\ttraining's rmse: 0.752031\n",
      "[839]\ttraining's rmse: 0.751883\n",
      "[840]\ttraining's rmse: 0.75183\n",
      "[841]\ttraining's rmse: 0.751775\n",
      "[842]\ttraining's rmse: 0.751739\n",
      "[843]\ttraining's rmse: 0.751653\n",
      "[844]\ttraining's rmse: 0.751607\n",
      "[845]\ttraining's rmse: 0.751506\n",
      "[846]\ttraining's rmse: 0.751421\n",
      "[847]\ttraining's rmse: 0.751381\n",
      "[848]\ttraining's rmse: 0.751343\n",
      "[849]\ttraining's rmse: 0.751279\n",
      "[850]\ttraining's rmse: 0.751235\n",
      "[851]\ttraining's rmse: 0.751164\n",
      "[852]\ttraining's rmse: 0.751115\n",
      "[853]\ttraining's rmse: 0.751063\n",
      "[854]\ttraining's rmse: 0.750998\n",
      "[855]\ttraining's rmse: 0.750932\n",
      "[856]\ttraining's rmse: 0.750892\n",
      "[857]\ttraining's rmse: 0.750858\n",
      "[858]\ttraining's rmse: 0.750815\n",
      "[859]\ttraining's rmse: 0.750778\n",
      "[860]\ttraining's rmse: 0.750745\n",
      "[861]\ttraining's rmse: 0.750682\n",
      "[862]\ttraining's rmse: 0.75061\n",
      "[863]\ttraining's rmse: 0.750544\n",
      "[864]\ttraining's rmse: 0.750476\n",
      "[865]\ttraining's rmse: 0.750425\n",
      "[866]\ttraining's rmse: 0.750373\n",
      "[867]\ttraining's rmse: 0.750318\n",
      "[868]\ttraining's rmse: 0.750268\n",
      "[869]\ttraining's rmse: 0.750217\n",
      "[870]\ttraining's rmse: 0.750182\n",
      "[871]\ttraining's rmse: 0.750146\n",
      "[872]\ttraining's rmse: 0.750099\n",
      "[873]\ttraining's rmse: 0.75004\n",
      "[874]\ttraining's rmse: 0.750002\n",
      "[875]\ttraining's rmse: 0.749964\n",
      "[876]\ttraining's rmse: 0.749916\n",
      "[877]\ttraining's rmse: 0.749873\n",
      "[878]\ttraining's rmse: 0.749801\n",
      "[879]\ttraining's rmse: 0.749738\n",
      "[880]\ttraining's rmse: 0.749703\n",
      "[881]\ttraining's rmse: 0.749632\n",
      "[882]\ttraining's rmse: 0.749581\n",
      "[883]\ttraining's rmse: 0.749531\n",
      "[884]\ttraining's rmse: 0.749474\n",
      "[885]\ttraining's rmse: 0.749438\n",
      "[886]\ttraining's rmse: 0.749395\n",
      "[887]\ttraining's rmse: 0.749355\n",
      "[888]\ttraining's rmse: 0.749328\n",
      "[889]\ttraining's rmse: 0.74928\n",
      "[890]\ttraining's rmse: 0.749244\n",
      "[891]\ttraining's rmse: 0.749215\n",
      "[892]\ttraining's rmse: 0.749176\n",
      "[893]\ttraining's rmse: 0.749123\n",
      "[894]\ttraining's rmse: 0.749073\n",
      "[895]\ttraining's rmse: 0.74903\n",
      "[896]\ttraining's rmse: 0.74897\n",
      "[897]\ttraining's rmse: 0.748938\n",
      "[898]\ttraining's rmse: 0.748898\n",
      "[899]\ttraining's rmse: 0.748815\n",
      "[900]\ttraining's rmse: 0.74877\n",
      "[901]\ttraining's rmse: 0.748709\n",
      "[902]\ttraining's rmse: 0.748663\n",
      "[903]\ttraining's rmse: 0.748619\n",
      "[904]\ttraining's rmse: 0.748564\n",
      "[905]\ttraining's rmse: 0.748478\n",
      "[906]\ttraining's rmse: 0.748423\n",
      "[907]\ttraining's rmse: 0.748374\n",
      "[908]\ttraining's rmse: 0.748325\n",
      "[909]\ttraining's rmse: 0.748287\n",
      "[910]\ttraining's rmse: 0.748249\n",
      "[911]\ttraining's rmse: 0.748205\n",
      "[912]\ttraining's rmse: 0.748171\n",
      "[913]\ttraining's rmse: 0.748134\n",
      "[914]\ttraining's rmse: 0.748086\n",
      "[915]\ttraining's rmse: 0.748038\n",
      "[916]\ttraining's rmse: 0.747995\n",
      "[917]\ttraining's rmse: 0.747961\n",
      "[918]\ttraining's rmse: 0.747917\n",
      "[919]\ttraining's rmse: 0.747873\n",
      "[920]\ttraining's rmse: 0.747842\n",
      "[921]\ttraining's rmse: 0.7478\n",
      "[922]\ttraining's rmse: 0.747746\n",
      "[923]\ttraining's rmse: 0.74771\n",
      "[924]\ttraining's rmse: 0.747665\n",
      "[925]\ttraining's rmse: 0.747617\n",
      "[926]\ttraining's rmse: 0.747573\n",
      "[927]\ttraining's rmse: 0.747518\n",
      "[928]\ttraining's rmse: 0.747469\n",
      "[929]\ttraining's rmse: 0.747428\n",
      "[930]\ttraining's rmse: 0.747395\n",
      "[931]\ttraining's rmse: 0.747349\n",
      "[932]\ttraining's rmse: 0.747296\n",
      "[933]\ttraining's rmse: 0.747264\n",
      "[934]\ttraining's rmse: 0.747228\n",
      "[935]\ttraining's rmse: 0.747172\n",
      "[936]\ttraining's rmse: 0.747138\n",
      "[937]\ttraining's rmse: 0.747069\n",
      "[938]\ttraining's rmse: 0.747029\n",
      "[939]\ttraining's rmse: 0.74699\n",
      "[940]\ttraining's rmse: 0.746952\n",
      "[941]\ttraining's rmse: 0.746916\n",
      "[942]\ttraining's rmse: 0.746882\n",
      "[943]\ttraining's rmse: 0.746848\n",
      "[944]\ttraining's rmse: 0.746802\n",
      "[945]\ttraining's rmse: 0.746757\n",
      "[946]\ttraining's rmse: 0.746715\n",
      "[947]\ttraining's rmse: 0.746652\n",
      "[948]\ttraining's rmse: 0.746595\n",
      "[949]\ttraining's rmse: 0.746563\n",
      "[950]\ttraining's rmse: 0.746517\n",
      "[951]\ttraining's rmse: 0.74638\n",
      "[952]\ttraining's rmse: 0.746331\n",
      "[953]\ttraining's rmse: 0.746275\n",
      "[954]\ttraining's rmse: 0.74624\n",
      "[955]\ttraining's rmse: 0.746195\n",
      "[956]\ttraining's rmse: 0.746141\n",
      "[957]\ttraining's rmse: 0.746082\n",
      "[958]\ttraining's rmse: 0.746044\n",
      "[959]\ttraining's rmse: 0.746011\n",
      "[960]\ttraining's rmse: 0.745967\n",
      "[961]\ttraining's rmse: 0.745922\n",
      "[962]\ttraining's rmse: 0.745869\n",
      "[963]\ttraining's rmse: 0.745817\n",
      "[964]\ttraining's rmse: 0.745769\n",
      "[965]\ttraining's rmse: 0.74573\n",
      "[966]\ttraining's rmse: 0.745677\n",
      "[967]\ttraining's rmse: 0.745632\n",
      "[968]\ttraining's rmse: 0.74558\n",
      "[969]\ttraining's rmse: 0.745523\n",
      "[970]\ttraining's rmse: 0.745487\n",
      "[971]\ttraining's rmse: 0.745429\n",
      "[972]\ttraining's rmse: 0.745386\n",
      "[973]\ttraining's rmse: 0.74536\n",
      "[974]\ttraining's rmse: 0.745326\n",
      "[975]\ttraining's rmse: 0.745268\n",
      "[976]\ttraining's rmse: 0.745227\n",
      "[977]\ttraining's rmse: 0.745183\n",
      "[978]\ttraining's rmse: 0.7451\n",
      "[979]\ttraining's rmse: 0.745067\n",
      "[980]\ttraining's rmse: 0.745028\n",
      "[981]\ttraining's rmse: 0.744986\n",
      "[982]\ttraining's rmse: 0.744952\n",
      "[983]\ttraining's rmse: 0.744662\n",
      "[984]\ttraining's rmse: 0.744624\n",
      "[985]\ttraining's rmse: 0.744583\n",
      "[986]\ttraining's rmse: 0.744554\n",
      "[987]\ttraining's rmse: 0.744523\n",
      "[988]\ttraining's rmse: 0.744487\n",
      "[989]\ttraining's rmse: 0.744451\n",
      "[990]\ttraining's rmse: 0.744388\n",
      "[991]\ttraining's rmse: 0.744342\n",
      "[992]\ttraining's rmse: 0.744308\n",
      "[993]\ttraining's rmse: 0.744269\n",
      "[994]\ttraining's rmse: 0.744228\n",
      "[995]\ttraining's rmse: 0.7442\n",
      "[996]\ttraining's rmse: 0.744169\n",
      "[997]\ttraining's rmse: 0.744136\n",
      "[998]\ttraining's rmse: 0.7441\n",
      "[999]\ttraining's rmse: 0.74405\n",
      "[1000]\ttraining's rmse: 0.744023\n"
     ]
    }
   ],
   "source": [
    "trainData = matrix[matrix['date_block_num'] < 34]\n",
    "label_train = trainData['item_cnt_month']\n",
    "X_train = trainData.drop('item_cnt_month', axis=1)\n",
    "\n",
    "# validData = matrix[matrix['date_block_num'] == 33]\n",
    "# label_valid = validData['item_cnt_month']\n",
    "# X_valid = validData.drop('item_cnt_month', axis=1)\n",
    "\n",
    "train_data = lgb.Dataset(data=X_train, label=label_train)\n",
    "# valid_data = lgb.Dataset(data=X_valid, label=label_valid)\n",
    "params = {\n",
    "    'objective': 'regression',  # 回归\n",
    "    'metric': 'rmse',   # 回归问题选择rmse\n",
    "    'n_estimators': 1000,\n",
    "    'num_leaves': 200,   # 每个弱学习器拥有的叶子的数量\n",
    "    'learning_rate': 0.01,\n",
    "    'bagging_fraction': 0.9,    # 每次训练“弱学习器”用的数据比例（应该也是随机的），用于加快训练速度和减小过拟合\n",
    "    'feature_fraction': 0.3,   # 每次迭代过程中，随机选择30%的特征建树（弱学习器）\n",
    "    'bagging_seed': 0,\n",
    "    'early_stop_rounds': 50\n",
    "}\n",
    "lgb_model = lgb.train(params, train_data, valid_sets=[train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test数据\n",
    "testData = matrix[matrix['date_block_num'] == 34]\n",
    "X_test = testData.drop('item_cnt_month', axis=1)\n",
    "\n",
    "# 预测&生成文件\n",
    "y_test = lgb_model.predict(X_test).clip(0, 20)\n",
    "submission = pd.DataFrame({ 'ID': range(0, 214200), 'item_cnt_month': y_test})\n",
    "\n",
    "test0 = test[test.item_id.isin(six_zero_item_id)]\n",
    "ids = list(test0.ID.values)\n",
    "submission.loc[submission.ID.isin(ids), 'item_cnt_month'] = 0.0\n",
    "submission.to_csv('./submit/sub1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
